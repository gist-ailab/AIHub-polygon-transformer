/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2024-10-29 10:22:11 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-29 10:22:11 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-29 10:22:11 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-29 10:22:11 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-29 10:22:11 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-29 10:22:11 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-29 10:22:11 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-29 10:22:11 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-29 10:22:17 - utils.py[line:258] - INFO: distributed init (rank 2): env://
2024-10-29 10:22:17 - utils.py[line:261] - INFO: Start init
2024-10-29 10:22:17 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2024-10-29 10:22:17 - utils.py[line:258] - INFO: distributed init (rank 7): env://
2024-10-29 10:22:17 - utils.py[line:261] - INFO: Start init
2024-10-29 10:22:17 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 7
2024-10-29 10:22:17 - utils.py[line:258] - INFO: distributed init (rank 5): env://
2024-10-29 10:22:17 - utils.py[line:261] - INFO: Start init
2024-10-29 10:22:17 - utils.py[line:258] - INFO: distributed init (rank 3): env://
2024-10-29 10:22:17 - utils.py[line:261] - INFO: Start init
2024-10-29 10:22:17 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 5
2024-10-29 10:22:17 - utils.py[line:258] - INFO: distributed init (rank 6): env://
2024-10-29 10:22:17 - utils.py[line:261] - INFO: Start init
2024-10-29 10:22:17 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 3
2024-10-29 10:22:17 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 6
2024-10-29 10:22:17 - utils.py[line:258] - INFO: distributed init (rank 4): env://
2024-10-29 10:22:17 - utils.py[line:261] - INFO: Start init
2024-10-29 10:22:17 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2024-10-29 10:22:17 - utils.py[line:261] - INFO: Start init
2024-10-29 10:22:17 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 4
2024-10-29 10:22:17 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2024-10-29 10:22:17 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2024-10-29 10:22:17 - utils.py[line:261] - INFO: Start init
2024-10-29 10:22:17 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2024-10-29 10:22:17 - distributed_c10d.py[line:354] - INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-29 10:22:17 - distributed_c10d.py[line:354] - INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-29 10:22:17 - distributed_c10d.py[line:354] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-29 10:22:17 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 0
2024-10-29 10:22:17 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 2
single-machine distributed training is initialized.
single-machine distributed training is initialized.
2024-10-29 10:22:17 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 1
single-machine distributed training is initialized.
2024-10-29 10:22:17 - distributed_c10d.py[line:354] - INFO: Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-29 10:22:17 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 5
single-machine distributed training is initialized.
2024-10-29 10:22:17 - distributed_c10d.py[line:354] - INFO: Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-29 10:22:17 - distributed_c10d.py[line:354] - INFO: Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-29 10:22:17 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 3
single-machine distributed training is initialized.
2024-10-29 10:22:17 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 7
single-machine distributed training is initialized.
2024-10-29 10:22:17 - distributed_c10d.py[line:354] - INFO: Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-29 10:22:17 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 6
single-machine distributed training is initialized.
2024-10-29 10:22:17 - distributed_c10d.py[line:354] - INFO: Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-29 10:22:17 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 4
single-machine distributed training is initialized.
2024-10-29 10:22:22 - train.py[line:77] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../polyformer_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 20, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 20, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'score', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='polyformer_b', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='polyformer_b', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid=20, best_checkpoint_metric='score', bf16=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, cls_weight=0.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../../datasets/pretrain/train_aihub_indoor.tsv,../../datasets/pretrain/val_aihub_indoor.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, det_weight=1.0, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_acc=True, eval_args='{"beam":5,"min_len":2,"max_len_a":0,"max_len_b":2}', eval_print_samples=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=False, freeze_encoder_embedding=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=20, max_image_size=512, max_source_positions=1024, max_src_length=80, max_target_positions=1024, max_tgt_length=420, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=8, num_bins=64, num_shards=1, num_workers=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', out_index=3, pad=1, patch_image_size=512, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, restore_file='checkpoint_last.pt', sample_patch_num=196, save_dir='./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512', save_interval=1, save_interval_updates=1000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', scst=False, scst_args='{}', seed=1, selected_cols='0,3,1,2', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, sync_bn=False, task='refcoco_pretrain', tensorboard_logdir=None, threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[8], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../polyformer_module', uses_ema=False, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=1000, vis_encoder_type='swin-base', wandb_project=None, warmup_ratio=0.06, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'refcoco_pretrain', 'data': '../../datasets/pretrain/train_aihub_indoor.tsv,../../datasets/pretrain/val_aihub_indoor.tsv', 'selected_cols': '0,3,1,2', 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 80, 'max_tgt_length': 420, 'code_dict_size': 8192, 'patch_image_size': 512, 'num_bins': 64, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'eval_acc': True, 'eval_args': '{"beam":5,"min_len":2,"max_len_a":0,"max_len_b":2}', 'uses_ema': False, 'eval_print_samples': False, 'max_image_size': 512, 'scst': False, 'scst_args': '{}'}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'det_weight': 1.0, 'cls_weight': 0.0, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.06, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2024-10-29 10:22:22 - base_task.py[line:187] - INFO: source dictionary: 4100 types
2024-10-29 10:22:22 - base_task.py[line:188] - INFO: target dictionary: 4100 types
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
The model and loaded state dict do not match exactly

unexpected key in source state_dict: norm.weight, norm.bias, head.weight, head.bias, layers.0.blocks.1.attn_mask, layers.1.blocks.1.attn_mask, layers.2.blocks.1.attn_mask, layers.2.blocks.3.attn_mask, layers.2.blocks.5.attn_mask, layers.2.blocks.7.attn_mask, layers.2.blocks.9.attn_mask, layers.2.blocks.11.attn_mask, layers.2.blocks.13.attn_mask, layers.2.blocks.15.attn_mask, layers.2.blocks.17.attn_mask

missing keys in source state_dict: norm3.weight, norm3.bias

Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
2024-10-29 10:22:35 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-29 10:22:35 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-10-29 10:22:35 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-29 10:22:35 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-10-29 10:22:35 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-29 10:22:35 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-10-29 10:22:35 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-29 10:22:35 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-10-29 10:22:35 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-29 10:22:35 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-10-29 10:22:35 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-29 10:22:35 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-10-29 10:22:35 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-29 10:22:35 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-10-29 10:22:35 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-29 10:22:35 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 7 row count 1037 total row count 8297
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 3 row count 1037 total row count 8297
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 1 row count 1037 total row count 8297
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 5 row count 1037 total row count 8297
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 6 row count 1037 total row count 8297
The model and loaded state dict do not match exactly

unexpected key in source state_dict: norm.weight, norm.bias, head.weight, head.bias, layers.0.blocks.1.attn_mask, layers.1.blocks.1.attn_mask, layers.2.blocks.1.attn_mask, layers.2.blocks.3.attn_mask, layers.2.blocks.5.attn_mask, layers.2.blocks.7.attn_mask, layers.2.blocks.9.attn_mask, layers.2.blocks.11.attn_mask, layers.2.blocks.13.attn_mask, layers.2.blocks.15.attn_mask, layers.2.blocks.17.attn_mask

missing keys in source state_dict: norm3.weight, norm3.bias

2024-10-29 10:22:36 - train.py[line:101] - INFO: PolyFormerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(4100, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): SwinTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=128, out_features=128, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): Identity()
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=128, out_features=512, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=512, out_features=128, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=128, out_features=128, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.009)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=128, out_features=512, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=512, out_features=128, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=512, out_features=256, bias=False)
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.017)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.026)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=1024, out_features=512, bias=False)
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.035)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.043)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (2): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.052)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (3): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.061)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (4): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.070)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (5): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.078)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (6): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.087)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (7): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.096)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (8): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.104)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (9): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.113)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (10): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.122)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (11): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.130)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (12): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.139)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (13): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.148)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (14): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.157)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (15): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.165)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (16): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.174)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (17): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.183)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=2048, out_features=1024, bias=False)
            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.191)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.200)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
    (bert): XLMRobertaForMaskedLM(
      (roberta): XLMRobertaModel(
        (embeddings): XLMRobertaEmbeddings(
          (word_embeddings): Embedding(250002, 768, padding_idx=1)
          (position_embeddings): Embedding(514, 768, padding_idx=1)
          (token_type_embeddings): Embedding(1, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): XLMRobertaEncoder(
          (layer): ModuleList(
            (0): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (lm_head): XLMRobertaLMHead(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (decoder): Linear(in_features=768, out_features=250002, bias=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(4100, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (reg_head): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=768, out_features=768, bias=True)
        (1): Linear(in_features=768, out_features=768, bias=True)
        (2): Linear(in_features=768, out_features=2, bias=True)
      )
    )
    (cls_head): Linear(in_features=768, out_features=3, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2024-10-29 10:22:36 - train.py[line:102] - INFO: task: RefcocoPretrainTask
2024-10-29 10:22:36 - train.py[line:103] - INFO: model: PolyFormerModel
2024-10-29 10:22:36 - train.py[line:104] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2024-10-29 10:22:36 - train.py[line:108] - INFO: num. shared model params: 478,547,732 (num. trained: 478,547,732)
2024-10-29 10:22:36 - train.py[line:115] - INFO: num. expert model params: 0 (num. trained: 0)
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 0 row count 1038 total row count 8297
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 2 row count 1037 total row count 8297
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 4 row count 1037 total row count 8297
2024-10-29 10:22:37 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2024-10-29 10:22:38 - distributed_c10d.py[line:354] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2024-10-29 10:22:38 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2024-10-29 10:22:38 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.layers.0.downsample.reduction.bias <- encoder.embed_images.layers.1.downsample.reduction.bias
2024-10-29 10:22:38 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.layers.0.downsample.reduction.bias <- encoder.embed_images.layers.2.downsample.reduction.bias
2024-10-29 10:22:38 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.layers.0.downsample.reduction.bias <- decoder.cls_head.bias
2024-10-29 10:22:38 - trainer.py[line:124] - INFO: detected shared parameter: encoder.bert.roberta.embeddings.word_embeddings.weight <- encoder.bert.lm_head.decoder.weight
2024-10-29 10:22:38 - trainer.py[line:124] - INFO: detected shared parameter: encoder.bert.lm_head.bias <- encoder.bert.lm_head.decoder.bias
2024-10-29 10:22:40 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 8 workers***********************
2024-10-29 10:22:40 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-29 10:22:40 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-29 10:22:40 - utils.py[line:765] - INFO: rank   2: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-29 10:22:40 - utils.py[line:765] - INFO: rank   3: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-29 10:22:40 - utils.py[line:765] - INFO: rank   4: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-29 10:22:40 - utils.py[line:765] - INFO: rank   5: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-29 10:22:40 - utils.py[line:765] - INFO: rank   6: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-29 10:22:40 - utils.py[line:765] - INFO: rank   7: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-29 10:22:40 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 8 workers***********************
2024-10-29 10:22:40 - train.py[line:145] - INFO: training on 8 devices (GPUs/TPUs)
2024-10-29 10:22:40 - train.py[line:151] - INFO: max tokens per device = None and max sentences per device = 20
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping




2024-10-29 10:22:40 - trainer.py[line:458] - INFO: Preparing to load checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 10:22:40 - trainer.py[line:624] - INFO: No existing checkpoint found ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 10:22:40 - trainer.py[line:639] - INFO: loading train data for epoch 1
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
slice_id 0 seek offset 0
slice_id 5 seek offset 41230
slice_id 6 seek offset 49476
slice_id 2 seek offset 16492
slice_id 3 seek offset 24738
slice_id 4 seek offset 32984
Total steps 1040, warmup steps 62, warmup_factor 0.016129032258064516
slice_id 7 seek offset 57721
Total steps 1040, warmup steps 62, warmup_factor 0.016129032258064516
2024-10-29 10:22:43 - trainer.py[line:703] - INFO: begin training epoch 1
2024-10-29 10:22:43 - train.py[line:297] - INFO: Start iterating over samples
slice_id 1 seek offset 8246
Total steps 1040, warmup steps 62, warmup_factor 0.016129032258064516
Total steps 1040, warmup steps 62, warmup_factor 0.016129032258064516
Total steps 1040, warmup steps 62, warmup_factor 0.016129032258064516
Total steps 1040, warmup steps 62, warmup_factor 0.016129032258064516
Total steps 1040, warmup steps 62, warmup_factor 0.016129032258064516
Total steps 1040, warmup steps 62, warmup_factor 0.016129032258064516
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
loss_reg: 0.372802734375 loss_cls: 0.0
loss_reg: 0.382080078125 loss_cls: 0.0
loss_reg: 0.377685546875 loss_cls: 0.0
loss_reg: 0.354248046875 loss_cls: 0.0
loss_reg: 0.3857421875 loss_cls: 0.0
loss_reg: 0.3759765625 loss_cls: 0.0
loss_reg: 0.361083984375 loss_cls: 0.0
loss_reg: 0.382080078125 loss_cls: 0.0
loss_reg: 0.365966796875 loss_cls: 0.0
loss_reg: 0.366943359375 loss_cls: 0.0
loss_reg: 0.3583984375 loss_cls: 0.0
loss_reg: 0.4033203125 loss_cls: 0.0
loss_reg: 0.38720703125 loss_cls: 0.0
loss_reg: 0.331787109375 loss_cls: 0.0
loss_reg: 0.384521484375 loss_cls: 0.0
loss_reg: 0.329833984375 loss_cls: 0.0
loss_reg: 0.353271484375 loss_cls: 0.0
loss_reg: 0.392333984375 loss_cls: 0.0
loss_reg: 0.349853515625 loss_cls: 0.0
loss_reg: 0.366943359375 loss_cls: 0.0
loss_reg: 0.35546875 loss_cls: 0.0
loss_reg: 0.383544921875 loss_cls: 0.0
loss_reg: 0.38623046875 loss_cls: 0.0
loss_reg: 0.364501953125 loss_cls: 0.0
loss_reg: 0.330078125 loss_cls: 0.0
loss_reg: 0.38330078125 loss_cls: 0.0
loss_reg: 0.3271484375 loss_cls: 0.0
loss_reg: 0.35205078125 loss_cls: 0.0
loss_reg: 0.389892578125 loss_cls: 0.0
loss_reg: 0.355712890625 loss_cls: 0.0
loss_reg: 0.4150390625 loss_cls: 0.0
loss_reg: 0.360595703125 loss_cls: 0.0
loss_reg: 0.366455078125 loss_cls: 0.0
loss_reg: 0.3720703125 loss_cls: 0.0
loss_reg: 0.333984375 loss_cls: 0.0
loss_reg: 0.388916015625 loss_cls: 0.0
loss_reg: 0.369384765625 loss_cls: 0.0
loss_reg: 0.3544921875 loss_cls: 0.0
loss_reg: 0.367431640625 loss_cls: 0.0
loss_reg: 0.362548828125 loss_cls: 0.0
loss_reg: 0.40625 loss_cls: 0.0
loss_reg: 0.358154296875 loss_cls: 0.0
loss_reg: 0.36328125 loss_cls: 0.0
loss_reg: 0.3671875 loss_cls: 0.0
loss_reg: 0.373779296875 loss_cls: 0.0
loss_reg: 0.3935546875 loss_cls: 0.0
loss_reg: 0.365966796875 loss_cls: 0.0
loss_reg: 0.39208984375 loss_cls: 0.0
loss_reg: 0.39599609375 loss_cls: 0.0
loss_reg: 0.389892578125 loss_cls: 0.0
loss_reg: 0.39404296875 loss_cls: 0.0
loss_reg: 0.39013671875 loss_cls: 0.0
loss_reg: 0.371826171875 loss_cls: 0.0
loss_reg: 0.3681640625 loss_cls: 0.0
loss_reg: 0.4140625 loss_cls: 0.0
loss_reg: 0.359619140625 loss_cls: 0.0
loss_reg: 0.379638671875 loss_cls: 0.0
loss_reg: 0.388671875 loss_cls: 0.0
loss_reg: 0.37451171875 loss_cls: 0.0
loss_reg: 0.3994140625 loss_cls: 0.0
loss_reg: 0.37255859375 loss_cls: 0.0
loss_reg: 0.345947265625 loss_cls: 0.0
loss_reg: 0.344482421875 loss_cls: 0.0
loss_reg: 0.3505859375 loss_cls: 0.0
2024-10-29 10:24:51 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 52 loss=0.018, loss_v1=0, loss_v2=0, nll_loss=3.355, ntokens=5118.4, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=10.23, wps=420.1, ups=0.08, wpb=5118.4, bsz=1280, num_updates=10, lr=8.06452e-06, gnorm=0.029, clip=0, loss_scale=128, train_wall=46, gb_free=9.5, wall=131
2024-10-29 10:26:54 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 52 loss=0.016, loss_v1=0, loss_v2=0, nll_loss=3.542, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=11.65, wps=416.4, ups=0.08, wpb=5118, bsz=1280, num_updates=20, lr=1.6129e-05, gnorm=0.013, clip=0, loss_scale=128, train_wall=56, gb_free=9.6, wall=254
2024-10-29 10:28:56 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 52 loss=0.013, loss_v1=0, loss_v2=0, nll_loss=3.839, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.31, wps=421.8, ups=0.08, wpb=5118.3, bsz=1280, num_updates=30, lr=2.41935e-05, gnorm=0.018, clip=0, loss_scale=128, train_wall=67, gb_free=9.5, wall=375
2024-10-29 10:30:57 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 52 loss=0.011, loss_v1=0, loss_v2=0, nll_loss=3.764, ntokens=5117.8, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.58, wps=421, ups=0.08, wpb=5117.8, bsz=1280, num_updates=40, lr=3.22581e-05, gnorm=0.011, clip=0, loss_scale=128, train_wall=72, gb_free=9.5, wall=497
2024-10-29 10:32:58 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.926, ntokens=5072.1, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=15.2, wps=420.8, ups=0.08, wpb=5072.1, bsz=1268.6, num_updates=50, lr=4.03226e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=74, gb_free=9.6, wall=617
slice_id 3 seek offset 3112slice_id 1 seek offset 1038slice_id 7 seek offset 7260
slice_id 2 seek offset 2075
slice_id 5 seek offset 5186slice_id 6 seek offset 6223

2024-10-29 10:33:16 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 4 seek offset 4149

slice_id 0 seek offset 0

slice_id 2 seek offset 2075
slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 5 seek offset 5186
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038
2024-10-29 10:34:38 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.01 | loss_v1 0 | loss_v2 0 | nll_loss 3.929 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 15.24 | score 0.1219 | wps 416.4 | wpb 638 | bsz 159.6 | num_updates 52
2024-10-29 10:34:38 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 52 updates
2024-10-29 10:34:38 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
slice_id 6 seek offset 49476
slice_id 3 seek offset 24738
slice_id 1 seek offset 8246
slice_id 7 seek offset 57721
slice_id 5 seek offset 41230
slice_id 2 seek offset 16492
slice_id 4 seek offset 32984
2024-10-29 10:34:50 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_best.pt
2024-10-29 10:35:51 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_best.pt (epoch 1 @ 52 updates, score 0.1219) (writing took 73.43500129133463 seconds)
2024-10-29 10:35:52 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 52 updates
2024-10-29 10:35:52 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 10:36:06 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 10:36:06 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 1 @ 52 updates, score 0) (writing took 14.352839108556509 seconds)
2024-10-29 10:36:06 - train.py[line:336] - INFO: end of epoch 1 (average epoch stats below)
2024-10-29 10:36:06 - progress_bar.py[line:282] - INFO: epoch 001 | loss 0.013 | loss_v1 0 | loss_v2 0 | nll_loss 3.692 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 12.93 | wps 329.7 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 52 | lr 4.19355e-05 | gnorm 0.015 | clip 0 | loss_scale 128 | train_wall 329 | gb_free 9.5 | wall 806
2024-10-29 10:36:06 - trainer.py[line:639] - INFO: loading train data for epoch 2
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 10:36:09 - trainer.py[line:703] - INFO: begin training epoch 2
2024-10-29 10:36:09 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 10:37:47 - progress_bar.py[line:274] - INFO: epoch 002:      8 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.921, ntokens=4880.9, nsentences=1220.6, sample_size=1220.6, sample_size_v1=0, sample_size_v2=0, ppl=15.15, wps=168.6, ups=0.03, wpb=4880.9, bsz=1220.6, num_updates=60, lr=4.83871e-05, gnorm=0.002, clip=0, loss_scale=128, train_wall=41, gb_free=9.5, wall=907
2024-10-29 10:39:48 - progress_bar.py[line:274] - INFO: epoch 002:     18 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.868, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.6, wps=423.8, ups=0.08, wpb=5117.9, bsz=1280, num_updates=70, lr=4.9591e-05, gnorm=0.002, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=1028
2024-10-29 10:41:49 - progress_bar.py[line:274] - INFO: epoch 002:     28 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.855, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.47, wps=422.9, ups=0.08, wpb=5118.2, bsz=1280, num_updates=80, lr=4.90798e-05, gnorm=0.002, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=1149
2024-10-29 10:43:50 - progress_bar.py[line:274] - INFO: epoch 002:     38 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.878, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.7, wps=422.4, ups=0.08, wpb=5118.2, bsz=1280, num_updates=90, lr=4.85685e-05, gnorm=0.002, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=1270
2024-10-29 10:45:51 - progress_bar.py[line:274] - INFO: epoch 002:     48 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.968, ntokens=5117.6, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=15.64, wps=423.4, ups=0.08, wpb=5117.6, bsz=1280, num_updates=100, lr=4.80573e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=1391
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 4 seek offset 41492024-10-29 10:46:33 - train.py[line:449] - INFO: begin validation on "valid" subset

slice_id 6 seek offset 6223
slice_id 0 seek offset 0
slice_id 3 seek offset 3112
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038
slice_id 0 seek offset 0
slice_id 5 seek offset 5186
slice_id 2 seek offset 2075
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038
slice_id 6 seek offset 6223
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
2024-10-29 10:47:54 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 0.01 | loss_v1 0 | loss_v2 0 | nll_loss 3.779 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.73 | score 0.1382 | wps 418.7 | wpb 638 | bsz 159.6 | num_updates 104 | best_score 0.1382
2024-10-29 10:47:54 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 104 updates
2024-10-29 10:47:54 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
slice_id 1 seek offset 8246
slice_id 6 seek offset 49476
slice_id 4 seek offset 32984
slice_id 5 seek offset 41230
slice_id 2 seek offset 16492
slice_id 7 seek offset 57721
slice_id 3 seek offset 24738
2024-10-29 10:48:05 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_best.pt
2024-10-29 10:48:17 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_best.pt (epoch 2 @ 104 updates, score 0.1382) (writing took 22.565485313534737 seconds)
2024-10-29 10:48:18 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 104 updates
2024-10-29 10:48:18 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 10:48:29 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 10:48:29 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 2 @ 104 updates, score 0) (writing took 11.66088480874896 seconds)
2024-10-29 10:48:30 - train.py[line:336] - INFO: end of epoch 2 (average epoch stats below)
2024-10-29 10:48:30 - progress_bar.py[line:282] - INFO: epoch 002 | loss 0.01 | loss_v1 0 | loss_v2 0 | nll_loss 3.895 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.88 | wps 354.7 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 104 | lr 4.78528e-05 | gnorm 0.003 | clip 0 | loss_scale 128 | train_wall 182 | gb_free 9.5 | wall 1550
2024-10-29 10:48:30 - trainer.py[line:639] - INFO: loading train data for epoch 3
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 10:48:32 - trainer.py[line:703] - INFO: begin training epoch 3
2024-10-29 10:48:32 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 10:49:47 - progress_bar.py[line:274] - INFO: epoch 003:      6 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.804, ntokens=4880.7, nsentences=1220.6, sample_size=1220.6, sample_size_v1=0, sample_size_v2=0, ppl=13.97, wps=207.1, ups=0.04, wpb=4880.7, bsz=1220.6, num_updates=110, lr=4.7546e-05, gnorm=0.002, clip=0, loss_scale=128, train_wall=34, gb_free=9.5, wall=1626
2024-10-29 10:51:48 - progress_bar.py[line:274] - INFO: epoch 003:     16 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.731, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.28, wps=421.3, ups=0.08, wpb=5117.9, bsz=1280, num_updates=120, lr=4.70348e-05, gnorm=0.002, clip=0, loss_scale=128, train_wall=35, gb_free=9.4, wall=1748
2024-10-29 10:53:50 - progress_bar.py[line:274] - INFO: epoch 003:     26 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.733, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.3, wps=421.2, ups=0.08, wpb=5118.3, bsz=1280, num_updates=130, lr=4.65235e-05, gnorm=0.002, clip=0, loss_scale=128, train_wall=35, gb_free=9.4, wall=1869
2024-10-29 10:55:51 - progress_bar.py[line:274] - INFO: epoch 003:     36 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.745, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.41, wps=422.3, ups=0.08, wpb=5118.3, bsz=1280, num_updates=140, lr=4.60123e-05, gnorm=0.002, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=1991
2024-10-29 10:57:53 - progress_bar.py[line:274] - INFO: epoch 003:     46 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.813, ntokens=5117.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.05, wps=420.9, ups=0.08, wpb=5117.5, bsz=1280, num_updates=150, lr=4.5501e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=2112
slice_id 2 seek offset 2075
slice_id 6 seek offset 6223
slice_id 3 seek offset 3112
slice_id 7 seek offset 7260
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038
2024-10-29 10:58:59 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 4 seek offset 4149slice_id 0 seek offset 0

slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 3 seek offset 3112
slice_id 6 seek offset 6223
slice_id 4 seek offset 4149
2024-10-29 11:00:19 - progress_bar.py[line:282] - INFO: epoch 003 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.927 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 15.21 | score 0.1343 | wps 424.1 | wpb 638 | bsz 159.6 | num_updates 156 | best_score 0.1382
2024-10-29 11:00:19 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 3 @ 156 updates
2024-10-29 11:00:19 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
slice_id 5 seek offset 41230
slice_id 4 seek offset 32984
slice_id 7 seek offset 57721
slice_id 6 seek offset 49476
slice_id 2 seek offset 16492
slice_id 3 seek offset 24738
slice_id 1 seek offset 8246
2024-10-29 11:00:30 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 11:00:30 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 3 @ 156 updates, score 0.1343) (writing took 10.917450718581676 seconds)
2024-10-29 11:00:30 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 3 @ 156 updates
2024-10-29 11:00:30 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 11:00:40 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 11:00:41 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 3 @ 156 updates, score 0) (writing took 10.416539262980223 seconds)
2024-10-29 11:00:41 - train.py[line:336] - INFO: end of epoch 3 (average epoch stats below)
2024-10-29 11:00:41 - progress_bar.py[line:282] - INFO: epoch 003 | loss 0.01 | loss_v1 0 | loss_v2 0 | nll_loss 3.767 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.62 | wps 360.6 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 156 | lr 4.51943e-05 | gnorm 0.003 | clip 0 | loss_scale 128 | train_wall 182 | gb_free 9.5 | wall 2281
2024-10-29 11:00:41 - trainer.py[line:639] - INFO: loading train data for epoch 4
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 11:00:44 - trainer.py[line:703] - INFO: begin training epoch 4
2024-10-29 11:00:44 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 11:01:35 - progress_bar.py[line:274] - INFO: epoch 004:      4 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.889, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=14.81, wps=221.1, ups=0.04, wpb=4926.1, bsz=1232, num_updates=160, lr=4.49898e-05, gnorm=0.007, clip=0, loss_scale=128, train_wall=34, gb_free=9.4, wall=2335
2024-10-29 11:03:37 - progress_bar.py[line:274] - INFO: epoch 004:     14 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.016, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=16.17, wps=421.8, ups=0.08, wpb=5118, bsz=1280, num_updates=170, lr=4.44785e-05, gnorm=0.006, clip=0, loss_scale=128, train_wall=35, gb_free=9.6, wall=2456
2024-10-29 11:05:38 - progress_bar.py[line:274] - INFO: epoch 004:     24 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.054, ntokens=5118.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=16.61, wps=423.6, ups=0.08, wpb=5118.5, bsz=1280, num_updates=180, lr=4.39673e-05, gnorm=0.006, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=2577
2024-10-29 11:07:38 - progress_bar.py[line:274] - INFO: epoch 004:     34 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.051, ntokens=5072.3, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=16.58, wps=421.4, ups=0.08, wpb=5072.3, bsz=1268.6, num_updates=190, lr=4.3456e-05, gnorm=0.007, clip=0, loss_scale=128, train_wall=35, gb_free=9.6, wall=2698
2024-10-29 11:09:38 - progress_bar.py[line:274] - INFO: epoch 004:     44 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.043, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=16.48, wps=425.5, ups=0.08, wpb=5118, bsz=1280, num_updates=200, lr=4.29448e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=2818
slice_id 2 seek offset 2075
slice_id 6 seek offset 6223
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
slice_id 7 seek offset 7260
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
2024-10-29 11:11:09 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 6 seek offset 6223
slice_id 4 seek offset 4149
slice_id 3 seek offset 3112
2024-10-29 11:12:30 - progress_bar.py[line:282] - INFO: epoch 004 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.114 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 17.32 | score 0.1401 | wps 418.2 | wpb 638 | bsz 159.6 | num_updates 208 | best_score 0.1401
2024-10-29 11:12:30 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 4 @ 208 updates
2024-10-29 11:12:30 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
slice_id 4 seek offset 32984
slice_id 2 seek offset 16492
slice_id 7 seek offset 57721
slice_id 5 seek offset 41230
slice_id 1 seek offset 8246
slice_id 6 seek offset 49476
slice_id 3 seek offset 24738
2024-10-29 11:12:41 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_best.pt
2024-10-29 11:12:54 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_best.pt (epoch 4 @ 208 updates, score 0.1401) (writing took 23.522577080875635 seconds)
2024-10-29 11:12:54 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 4 @ 208 updates
2024-10-29 11:12:54 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 11:13:05 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 11:13:05 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 4 @ 208 updates, score 0) (writing took 10.93926428630948 seconds)
2024-10-29 11:13:06 - train.py[line:336] - INFO: end of epoch 4 (average epoch stats below)
2024-10-29 11:13:06 - progress_bar.py[line:282] - INFO: epoch 004 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.037 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 16.42 | wps 354.3 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 208 | lr 4.25358e-05 | gnorm 0.005 | clip 0 | loss_scale 128 | train_wall 181 | gb_free 9.5 | wall 3026
2024-10-29 11:13:06 - trainer.py[line:639] - INFO: loading train data for epoch 5
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 11:13:08 - trainer.py[line:703] - INFO: begin training epoch 5
2024-10-29 11:13:08 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 11:13:36 - progress_bar.py[line:274] - INFO: epoch 005:      2 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.082, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=16.94, wps=207.5, ups=0.04, wpb=4926.1, bsz=1232, num_updates=210, lr=4.24335e-05, gnorm=0.003, clip=0, loss_scale=128, train_wall=34, gb_free=9.5, wall=3055
2024-10-29 11:15:36 - progress_bar.py[line:274] - INFO: epoch 005:     12 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.1, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.15, wps=424.1, ups=0.08, wpb=5118, bsz=1280, num_updates=220, lr=4.19223e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=3176
2024-10-29 11:17:36 - progress_bar.py[line:274] - INFO: epoch 005:     22 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.108, ntokens=5072.3, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=17.25, wps=423.5, ups=0.08, wpb=5072.3, bsz=1268.6, num_updates=230, lr=4.1411e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=3296
2024-10-29 11:19:37 - progress_bar.py[line:274] - INFO: epoch 005:     32 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.109, ntokens=5118.4, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.26, wps=422.8, ups=0.08, wpb=5118.4, bsz=1280, num_updates=240, lr=4.08998e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=3417
2024-10-29 11:21:38 - progress_bar.py[line:274] - INFO: epoch 005:     42 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.101, ntokens=5118.1, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.17, wps=422.1, ups=0.08, wpb=5118.1, bsz=1280, num_updates=250, lr=4.03885e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.6, wall=3538
2024-10-29 11:23:34 - progress_bar.py[line:274] - INFO: epoch 005:     52 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.113, ntokens=4925.6, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=17.3, wps=425.9, ups=0.09, wpb=4925.6, bsz=1232, num_updates=260, lr=3.98773e-05, gnorm=0.003, clip=0, loss_scale=128, train_wall=34, gb_free=9.5, wall=3654
slice_id 2 seek offset 2075
slice_id 4 seek offset 4149slice_id 6 seek offset 6223slice_id 3 seek offset 3112


slice_id 7 seek offset 7260
2024-10-29 11:23:34 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 1 seek offset 1038slice_id 5 seek offset 5186

slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 6 seek offset 6223
slice_id 4 seek offset 4149
slice_id 3 seek offset 3112
2024-10-29 11:24:55 - progress_bar.py[line:282] - INFO: epoch 005 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.123 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 17.42 | score 0.1481 | wps 417.4 | wpb 638 | bsz 159.6 | num_updates 260 | best_score 0.1481
2024-10-29 11:24:55 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 5 @ 260 updates
2024-10-29 11:24:55 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
slice_id 2 seek offset 16492
slice_id 4 seek offset 32984
slice_id 3 seek offset 24738
slice_id 1 seek offset 8246
slice_id 6 seek offset 49476
slice_id 5 seek offset 41230
slice_id 7 seek offset 57721
2024-10-29 11:25:05 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_best.pt
2024-10-29 11:25:18 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_best.pt (epoch 5 @ 260 updates, score 0.1481) (writing took 22.537378922104836 seconds)
2024-10-29 11:25:18 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 5 @ 260 updates
2024-10-29 11:25:18 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 11:25:29 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 11:25:29 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 5 @ 260 updates, score 0) (writing took 10.904672775417566 seconds)
2024-10-29 11:25:30 - train.py[line:336] - INFO: end of epoch 5 (average epoch stats below)
2024-10-29 11:25:30 - progress_bar.py[line:282] - INFO: epoch 005 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.106 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 17.22 | wps 354.5 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 260 | lr 3.98773e-05 | gnorm 0.004 | clip 0 | loss_scale 128 | train_wall 181 | gb_free 9.5 | wall 3770
2024-10-29 11:25:30 - trainer.py[line:639] - INFO: loading train data for epoch 6
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 11:25:32 - trainer.py[line:703] - INFO: begin training epoch 6
2024-10-29 11:25:32 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 11:27:35 - progress_bar.py[line:274] - INFO: epoch 006:     10 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.109, ntokens=5118.4, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.25, wps=212, ups=0.04, wpb=5118.4, bsz=1280, num_updates=270, lr=3.93661e-05, gnorm=0.003, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=3895
2024-10-29 11:29:37 - progress_bar.py[line:274] - INFO: epoch 006:     20 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.109, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.26, wps=422.8, ups=0.08, wpb=5118, bsz=1280, num_updates=280, lr=3.88548e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.6, wall=4016
2024-10-29 11:31:37 - progress_bar.py[line:274] - INFO: epoch 006:     30 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.134, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.56, wps=423.4, ups=0.08, wpb=5118.3, bsz=1280, num_updates=290, lr=3.83436e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=4137
2024-10-29 11:33:37 - progress_bar.py[line:274] - INFO: epoch 006:     40 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.121, ntokens=5072.2, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=17.4, wps=423, ups=0.08, wpb=5072.2, bsz=1268.6, num_updates=300, lr=3.78323e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=34, gb_free=9.6, wall=4257
2024-10-29 11:35:38 - progress_bar.py[line:274] - INFO: epoch 006:     50 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.098, ntokens=5117.7, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.13, wps=423.6, ups=0.08, wpb=5117.7, bsz=1280, num_updates=310, lr=3.73211e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.6, wall=4378
slice_id 4 seek offset 4149
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038slice_id 5 seek offset 5186

2024-10-29 11:35:57 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 6 seek offset 6223
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 3 seek offset 3112
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 6 seek offset 6223
slice_id 7 seek offset 7260
slice_id 5 seek offset 5186
slice_id 4 seek offset 4149
2024-10-29 11:37:18 - progress_bar.py[line:282] - INFO: epoch 006 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.101 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 17.16 | score 0.1405 | wps 416.5 | wpb 638 | bsz 159.6 | num_updates 312 | best_score 0.1481
2024-10-29 11:37:18 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 6 @ 312 updates
2024-10-29 11:37:18 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
slice_id 7 seek offset 57721
slice_id 2 seek offset 16492
slice_id 6 seek offset 49476
slice_id 4 seek offset 32984
slice_id 1 seek offset 8246
slice_id 3 seek offset 24738
slice_id 5 seek offset 41230
2024-10-29 11:37:28 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 11:37:28 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 6 @ 312 updates, score 0.1405) (writing took 10.459422066807747 seconds)
2024-10-29 11:37:29 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 6 @ 312 updates
2024-10-29 11:37:29 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 11:37:39 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 11:37:39 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 6 @ 312 updates, score 0) (writing took 10.364523116499186 seconds)
2024-10-29 11:37:40 - train.py[line:336] - INFO: end of epoch 6 (average epoch stats below)
2024-10-29 11:37:40 - progress_bar.py[line:282] - INFO: epoch 006 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.113 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 17.31 | wps 361.5 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 312 | lr 3.72188e-05 | gnorm 0.004 | clip 0 | loss_scale 128 | train_wall 181 | gb_free 9.5 | wall 4499
2024-10-29 11:37:40 - trainer.py[line:639] - INFO: loading train data for epoch 7
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 11:37:42 - trainer.py[line:703] - INFO: begin training epoch 7
2024-10-29 11:37:42 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 11:39:22 - progress_bar.py[line:274] - INFO: epoch 007:      8 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.102, ntokens=4926.4, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=17.17, wps=219.9, ups=0.04, wpb=4926.4, bsz=1232, num_updates=320, lr=3.68098e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=34, gb_free=9.5, wall=4602
2024-10-29 11:41:23 - progress_bar.py[line:274] - INFO: epoch 007:     18 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.113, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.3, wps=422.7, ups=0.08, wpb=5118, bsz=1280, num_updates=330, lr=3.62986e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=4723
2024-10-29 11:43:24 - progress_bar.py[line:274] - INFO: epoch 007:     28 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.091, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.04, wps=422.6, ups=0.08, wpb=5118.2, bsz=1280, num_updates=340, lr=3.57873e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=4844
2024-10-29 11:45:24 - progress_bar.py[line:274] - INFO: epoch 007:     38 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.06, ntokens=5072.6, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=16.68, wps=425.1, ups=0.08, wpb=5072.6, bsz=1268.6, num_updates=350, lr=3.52761e-05, gnorm=0.007, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=4963
2024-10-29 11:47:25 - progress_bar.py[line:274] - INFO: epoch 007:     48 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.051, ntokens=5117.6, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=16.58, wps=420.4, ups=0.08, wpb=5117.6, bsz=1280, num_updates=360, lr=3.47648e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=36, gb_free=9.5, wall=5085
slice_id 2 seek offset 2075
slice_id 4 seek offset 4149
slice_id 6 seek offset 6223
2024-10-29 11:48:08 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 7 seek offset 7260
slice_id 0 seek offset 0
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 3 seek offset 3112
slice_id 6 seek offset 6223
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
2024-10-29 11:49:29 - progress_bar.py[line:282] - INFO: epoch 007 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.094 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 17.08 | score 0.1422 | wps 417.5 | wpb 638 | bsz 159.6 | num_updates 364 | best_score 0.1481
2024-10-29 11:49:29 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 7 @ 364 updates
2024-10-29 11:49:29 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
slice_id 4 seek offset 32984
slice_id 1 seek offset 8246
slice_id 5 seek offset 41230
slice_id 7 seek offset 57721
slice_id 2 seek offset 16492
slice_id 3 seek offset 24738
slice_id 6 seek offset 49476
2024-10-29 11:49:40 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 11:49:40 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 7 @ 364 updates, score 0.1422) (writing took 10.586838934570551 seconds)
2024-10-29 11:49:40 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 7 @ 364 updates
2024-10-29 11:49:40 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 11:49:52 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 11:49:52 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 7 @ 364 updates, score 0) (writing took 11.703573510050774 seconds)
2024-10-29 11:49:52 - train.py[line:336] - INFO: end of epoch 7 (average epoch stats below)
2024-10-29 11:49:52 - progress_bar.py[line:282] - INFO: epoch 007 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.081 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 16.93 | wps 360.2 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 364 | lr 3.45603e-05 | gnorm 0.005 | clip 0 | loss_scale 128 | train_wall 181 | gb_free 9.5 | wall 5232
2024-10-29 11:49:52 - trainer.py[line:639] - INFO: loading train data for epoch 8
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 11:49:54 - trainer.py[line:703] - INFO: begin training epoch 8
2024-10-29 11:49:54 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 11:51:10 - progress_bar.py[line:274] - INFO: epoch 008:      6 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.078, ntokens=4926.2, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=16.89, wps=219.8, ups=0.04, wpb=4926.2, bsz=1232, num_updates=370, lr=3.42536e-05, gnorm=0.003, clip=0, loss_scale=128, train_wall=34, gb_free=9.5, wall=5309
2024-10-29 11:53:11 - progress_bar.py[line:274] - INFO: epoch 008:     16 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.115, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.32, wps=422.4, ups=0.08, wpb=5118, bsz=1280, num_updates=380, lr=3.37423e-05, gnorm=0.003, clip=0, loss_scale=128, train_wall=35, gb_free=9.4, wall=5430
2024-10-29 11:55:11 - progress_bar.py[line:274] - INFO: epoch 008:     26 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.129, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.49, wps=426, ups=0.08, wpb=5118.3, bsz=1280, num_updates=390, lr=3.32311e-05, gnorm=0.003, clip=0, loss_scale=128, train_wall=35, gb_free=9.4, wall=5551
2024-10-29 11:57:11 - progress_bar.py[line:274] - INFO: epoch 008:     36 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.119, ntokens=5072.7, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=17.38, wps=422.5, ups=0.08, wpb=5072.7, bsz=1268.6, num_updates=400, lr=3.27198e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=5671
2024-10-29 11:59:12 - progress_bar.py[line:274] - INFO: epoch 008:     46 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.129, ntokens=5117.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.49, wps=424.3, ups=0.08, wpb=5117.5, bsz=1280, num_updates=410, lr=3.22086e-05, gnorm=0.003, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=5791
slice_id 2 seek offset 20752024-10-29 12:00:18 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 6 seek offset 6223
slice_id 7 seek offset 7260
slice_id 3 seek offset 3112
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038
slice_id 4 seek offset 4149
slice_id 0 seek offset 0

slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 3 seek offset 3112
slice_id 7 seek offset 7260
slice_id 6 seek offset 6223
slice_id 4 seek offset 4149
2024-10-29 12:01:40 - progress_bar.py[line:282] - INFO: epoch 008 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.148 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 17.73 | score 0.1439 | wps 416.9 | wpb 638 | bsz 159.6 | num_updates 416 | best_score 0.1481
2024-10-29 12:01:40 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 8 @ 416 updates
2024-10-29 12:01:40 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
slice_id 2 seek offset 16492
slice_id 7 seek offset 57721
slice_id 6 seek offset 49476
slice_id 3 seek offset 24738
slice_id 4 seek offset 32984
slice_id 5 seek offset 41230
slice_id 1 seek offset 8246
2024-10-29 12:01:50 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:01:50 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 8 @ 416 updates, score 0.1439) (writing took 10.569867342710495 seconds)
2024-10-29 12:01:51 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 8 @ 416 updates
2024-10-29 12:01:51 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:02:01 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:02:01 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 8 @ 416 updates, score 0) (writing took 10.657380230724812 seconds)
2024-10-29 12:02:02 - train.py[line:336] - INFO: end of epoch 8 (average epoch stats below)
2024-10-29 12:02:02 - progress_bar.py[line:282] - INFO: epoch 008 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.119 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 17.37 | wps 361.3 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 416 | lr 3.19018e-05 | gnorm 0.003 | clip 0 | loss_scale 128 | train_wall 181 | gb_free 9.5 | wall 5962
2024-10-29 12:02:02 - trainer.py[line:639] - INFO: loading train data for epoch 9
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 12:02:04 - trainer.py[line:703] - INFO: begin training epoch 9
2024-10-29 12:02:04 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 12:02:56 - progress_bar.py[line:274] - INFO: epoch 009:      4 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.123, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=17.43, wps=219.4, ups=0.04, wpb=4926.1, bsz=1232, num_updates=420, lr=3.16973e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=34, gb_free=9.4, wall=6016
2024-10-29 12:04:58 - progress_bar.py[line:274] - INFO: epoch 009:     14 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.141, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.64, wps=420.3, ups=0.08, wpb=5118, bsz=1280, num_updates=430, lr=3.11861e-05, gnorm=0.003, clip=0, loss_scale=128, train_wall=35, gb_free=9.6, wall=6138
2024-10-29 12:06:58 - progress_bar.py[line:274] - INFO: epoch 009:     24 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.131, ntokens=5072.9, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=17.52, wps=422.5, ups=0.08, wpb=5072.9, bsz=1268.6, num_updates=440, lr=3.06748e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=6258
2024-10-29 12:08:59 - progress_bar.py[line:274] - INFO: epoch 009:     34 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.119, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.37, wps=422.6, ups=0.08, wpb=5117.9, bsz=1280, num_updates=450, lr=3.01636e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=35, gb_free=9.6, wall=6379
2024-10-29 12:11:00 - progress_bar.py[line:274] - INFO: epoch 009:     44 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.116, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.33, wps=422.1, ups=0.08, wpb=5118, bsz=1280, num_updates=460, lr=2.96524e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=6500
slice_id 6 seek offset 6223slice_id 7 seek offset 7260slice_id 2 seek offset 2075
slice_id 4 seek offset 4149
slice_id 1 seek offset 1038slice_id 3 seek offset 3112

slice_id 5 seek offset 5186
2024-10-29 12:12:31 - train.py[line:449] - INFO: begin validation on "valid" subset


slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 1 seek offset 1038
slice_id 2 seek offset 2075
slice_id 5 seek offset 5186
slice_id 6 seek offset 6223
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
slice_id 7 seek offset 7260
2024-10-29 12:13:51 - progress_bar.py[line:282] - INFO: epoch 009 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.135 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 17.57 | score 0.1407 | wps 418.9 | wpb 638 | bsz 159.6 | num_updates 468 | best_score 0.1481
2024-10-29 12:13:51 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 9 @ 468 updates
2024-10-29 12:13:51 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
slice_id 7 seek offset 57721
slice_id 4 seek offset 32984
slice_id 2 seek offset 16492
slice_id 1 seek offset 8246
slice_id 6 seek offset 49476
slice_id 3 seek offset 24738
slice_id 5 seek offset 41230
2024-10-29 12:14:01 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:14:02 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 9 @ 468 updates, score 0.1407) (writing took 10.335136499255896 seconds)
2024-10-29 12:14:02 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 9 @ 468 updates
2024-10-29 12:14:02 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:14:12 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:14:13 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 9 @ 468 updates, score 0) (writing took 10.507096946239471 seconds)
2024-10-29 12:14:13 - train.py[line:336] - INFO: end of epoch 9 (average epoch stats below)
2024-10-29 12:14:13 - progress_bar.py[line:282] - INFO: epoch 009 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.125 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 17.45 | wps 360.7 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 468 | lr 2.92434e-05 | gnorm 0.004 | clip 0 | loss_scale 128 | train_wall 181 | gb_free 9.5 | wall 6693
2024-10-29 12:14:13 - trainer.py[line:639] - INFO: loading train data for epoch 10
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 12:14:16 - trainer.py[line:703] - INFO: begin training epoch 10
2024-10-29 12:14:16 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 12:14:43 - progress_bar.py[line:274] - INFO: epoch 010:      2 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.116, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=17.34, wps=221.4, ups=0.04, wpb=4926.1, bsz=1232, num_updates=470, lr=2.91411e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=34, gb_free=9.5, wall=6723
2024-10-29 12:16:44 - progress_bar.py[line:274] - INFO: epoch 010:     12 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.124, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.44, wps=422, ups=0.08, wpb=5118, bsz=1280, num_updates=480, lr=2.86299e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=6844
2024-10-29 12:18:45 - progress_bar.py[line:274] - INFO: epoch 010:     22 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.144, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.68, wps=423.5, ups=0.08, wpb=5117.9, bsz=1280, num_updates=490, lr=2.81186e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=6965
2024-10-29 12:20:46 - progress_bar.py[line:274] - INFO: epoch 010:     32 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.157, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.84, wps=423.8, ups=0.08, wpb=5118.3, bsz=1280, num_updates=500, lr=2.76074e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=7085
2024-10-29 12:22:46 - progress_bar.py[line:274] - INFO: epoch 010:     42 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.15, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.75, wps=423.9, ups=0.08, wpb=5118.2, bsz=1280, num_updates=510, lr=2.70961e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=7206
2024-10-29 12:24:41 - progress_bar.py[line:274] - INFO: epoch 010:     52 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.148, ntokens=4880, nsentences=1220.6, sample_size=1220.6, sample_size_v1=0, sample_size_v2=0, ppl=17.73, wps=427.3, ups=0.09, wpb=4880, bsz=1220.6, num_updates=520, lr=2.65849e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=33, gb_free=9.5, wall=7320
slice_id 4 seek offset 4149slice_id 6 seek offset 6223slice_id 7 seek offset 7260
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186

slice_id 3 seek offset 3112

2024-10-29 12:24:41 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 5 seek offset 5186
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 6 seek offset 6223
slice_id 1 seek offset 1038
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
2024-10-29 12:26:02 - progress_bar.py[line:282] - INFO: epoch 010 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.178 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.1 | score 0.1398 | wps 416 | wpb 638 | bsz 159.6 | num_updates 520 | best_score 0.1481
2024-10-29 12:26:02 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 10 @ 520 updates
2024-10-29 12:26:02 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966

slice_id 2 seek offset 16492
slice_id 3 seek offset 24738
slice_id 7 seek offset 57721
slice_id 6 seek offset 49476
slice_id 5 seek offset 41230
slice_id 1 seek offset 8246
slice_id 4 seek offset 32984
2024-10-29 12:26:13 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:26:13 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 10 @ 520 updates, score 0.1398) (writing took 10.777356341481209 seconds)
2024-10-29 12:26:13 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 10 @ 520 updates
2024-10-29 12:26:13 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:26:24 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:26:24 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 10 @ 520 updates, score 0) (writing took 10.66140914708376 seconds)
2024-10-29 12:26:25 - train.py[line:336] - INFO: end of epoch 10 (average epoch stats below)
2024-10-29 12:26:25 - progress_bar.py[line:282] - INFO: epoch 010 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.143 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 17.67 | wps 360.7 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 520 | lr 2.65849e-05 | gnorm 0.005 | clip 0 | loss_scale 256 | train_wall 181 | gb_free 9.5 | wall 7424
2024-10-29 12:26:25 - trainer.py[line:639] - INFO: loading train data for epoch 11
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 12:26:27 - trainer.py[line:703] - INFO: begin training epoch 11
2024-10-29 12:26:27 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 12:28:32 - progress_bar.py[line:274] - INFO: epoch 011:     10 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.178, ntokens=5118.4, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.1, wps=221.6, ups=0.04, wpb=5118.4, bsz=1280, num_updates=530, lr=2.60736e-05, gnorm=0.006, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=7551
2024-10-29 12:30:33 - progress_bar.py[line:274] - INFO: epoch 011:     20 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.202, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.41, wps=422.6, ups=0.08, wpb=5118, bsz=1280, num_updates=540, lr=2.55624e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=7672
2024-10-29 12:32:33 - progress_bar.py[line:274] - INFO: epoch 011:     30 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.228, ntokens=5072.7, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=18.74, wps=422.4, ups=0.08, wpb=5072.7, bsz=1268.6, num_updates=550, lr=2.50511e-05, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=7792
2024-10-29 12:34:34 - progress_bar.py[line:274] - INFO: epoch 011:     40 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.234, ntokens=5117.8, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.82, wps=422.7, ups=0.08, wpb=5117.8, bsz=1280, num_updates=560, lr=2.45399e-05, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=7914
2024-10-29 12:36:35 - progress_bar.py[line:274] - INFO: epoch 011:     50 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.256, ntokens=5117.7, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=19.11, wps=422.2, ups=0.08, wpb=5117.7, bsz=1280, num_updates=570, lr=2.40286e-05, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=8035
slice_id 3 seek offset 3112slice_id 2 seek offset 2075slice_id 4 seek offset 4149
slice_id 7 seek offset 7260
2024-10-29 12:36:54 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 6 seek offset 6223
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038

slice_id 0 seek offset 0

slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
2024-10-29 12:38:15 - progress_bar.py[line:282] - INFO: epoch 011 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.241 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.91 | score 0.1286 | wps 415.6 | wpb 638 | bsz 159.6 | num_updates 572 | best_score 0.1481
2024-10-29 12:38:15 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 11 @ 572 updates
2024-10-29 12:38:15 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
slice_id 5 seek offset 41230
slice_id 6 seek offset 49476
slice_id 1 seek offset 8246
slice_id 4 seek offset 32984
slice_id 7 seek offset 57721
slice_id 2 seek offset 16492
slice_id 3 seek offset 24738
2024-10-29 12:38:25 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:38:25 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 11 @ 572 updates, score 0.1286) (writing took 10.384999088943005 seconds)
2024-10-29 12:38:26 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 11 @ 572 updates
2024-10-29 12:38:26 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:38:36 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:38:36 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 11 @ 572 updates, score 0) (writing took 9.907666008919477 seconds)
2024-10-29 12:38:36 - train.py[line:336] - INFO: end of epoch 11 (average epoch stats below)
2024-10-29 12:38:36 - progress_bar.py[line:282] - INFO: epoch 011 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.22 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.64 | wps 360.4 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 572 | lr 2.39264e-05 | gnorm 0.005 | clip 0 | loss_scale 256 | train_wall 181 | gb_free 9.5 | wall 8156
2024-10-29 12:38:36 - trainer.py[line:639] - INFO: loading train data for epoch 12
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 12:38:39 - trainer.py[line:703] - INFO: begin training epoch 12
2024-10-29 12:38:39 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 12:40:19 - progress_bar.py[line:274] - INFO: epoch 012:      8 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.226, ntokens=4926.4, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=18.72, wps=220, ups=0.04, wpb=4926.4, bsz=1232, num_updates=580, lr=2.35174e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=34, gb_free=9.5, wall=8259
2024-10-29 12:42:18 - progress_bar.py[line:274] - INFO: epoch 012:     18 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.203, ntokens=5072.4, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=18.41, wps=424.7, ups=0.08, wpb=5072.4, bsz=1268.6, num_updates=590, lr=2.30061e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=8378
2024-10-29 12:44:20 - progress_bar.py[line:274] - INFO: epoch 012:     28 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.212, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.54, wps=421.1, ups=0.08, wpb=5118.2, bsz=1280, num_updates=600, lr=2.24949e-05, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=8500
2024-10-29 12:46:21 - progress_bar.py[line:274] - INFO: epoch 012:     38 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.218, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.61, wps=422, ups=0.08, wpb=5118.2, bsz=1280, num_updates=610, lr=2.19836e-05, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=8621
2024-10-29 12:48:23 - progress_bar.py[line:274] - INFO: epoch 012:     48 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.227, ntokens=5117.6, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.73, wps=421.4, ups=0.08, wpb=5117.6, bsz=1280, num_updates=620, lr=2.14724e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=8742
slice_id 4 seek offset 4149slice_id 7 seek offset 7260slice_id 2 seek offset 2075
2024-10-29 12:49:05 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 6 seek offset 6223
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186


slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 5 seek offset 5186
slice_id 6 seek offset 6223
slice_id 1 seek offset 1038
slice_id 2 seek offset 2075
slice_id 3 seek offset 3112
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
2024-10-29 12:50:27 - progress_bar.py[line:282] - INFO: epoch 012 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.247 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.99 | score 0.1304 | wps 414.5 | wpb 638 | bsz 159.6 | num_updates 624 | best_score 0.1481
2024-10-29 12:50:27 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 12 @ 624 updates
2024-10-29 12:50:27 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
slice_id 2 seek offset 16492
slice_id 4 seek offset 32984
slice_id 7 seek offset 57721
slice_id 6 seek offset 49476
slice_id 1 seek offset 8246
slice_id 5 seek offset 41230
slice_id 3 seek offset 24738
2024-10-29 12:50:38 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:50:38 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 12 @ 624 updates, score 0.1304) (writing took 10.941190596669912 seconds)
2024-10-29 12:50:38 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 12 @ 624 updates
2024-10-29 12:50:38 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:50:48 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 12:50:49 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 12 @ 624 updates, score 0) (writing took 10.488725379109383 seconds)
2024-10-29 12:50:49 - train.py[line:336] - INFO: end of epoch 12 (average epoch stats below)
2024-10-29 12:50:49 - progress_bar.py[line:282] - INFO: epoch 012 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.218 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.6 | wps 359.8 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 624 | lr 2.12679e-05 | gnorm 0.005 | clip 0 | loss_scale 256 | train_wall 181 | gb_free 9.5 | wall 8889
2024-10-29 12:50:49 - trainer.py[line:639] - INFO: loading train data for epoch 13
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 12:50:52 - trainer.py[line:703] - INFO: begin training epoch 13
2024-10-29 12:50:52 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 12:52:08 - progress_bar.py[line:274] - INFO: epoch 013:      6 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.236, ntokens=4926.2, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=18.84, wps=219.1, ups=0.04, wpb=4926.2, bsz=1232, num_updates=630, lr=2.09611e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=34, gb_free=9.5, wall=8967
2024-10-29 12:54:09 - progress_bar.py[line:274] - INFO: epoch 013:     16 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.206, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.46, wps=423, ups=0.08, wpb=5118, bsz=1280, num_updates=640, lr=2.04499e-05, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.4, wall=9088
2024-10-29 12:56:09 - progress_bar.py[line:274] - INFO: epoch 013:     26 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.187, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.22, wps=423.7, ups=0.08, wpb=5118.3, bsz=1280, num_updates=650, lr=1.99387e-05, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.4, wall=9209
2024-10-29 12:58:10 - progress_bar.py[line:274] - INFO: epoch 013:     36 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.185, ntokens=5072.7, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=18.18, wps=422.2, ups=0.08, wpb=5072.7, bsz=1268.6, num_updates=660, lr=1.94274e-05, gnorm=0.006, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=9329
2024-10-29 13:00:11 - progress_bar.py[line:274] - INFO: epoch 013:     46 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.197, ntokens=5117.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.33, wps=420.3, ups=0.08, wpb=5117.5, bsz=1280, num_updates=670, lr=1.89162e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=9451
slice_id 2 seek offset 2075
slice_id 4 seek offset 4149slice_id 6 seek offset 6223
slice_id 7 seek offset 7260
2024-10-29 13:01:18 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 5 seek offset 5186slice_id 0 seek offset 0slice_id 1 seek offset 1038
slice_id 3 seek offset 3112



slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 7 seek offset 7260
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
slice_id 5 seek offset 5186
slice_id 6 seek offset 6223
2024-10-29 13:02:39 - progress_bar.py[line:282] - INFO: epoch 013 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.219 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.62 | score 0.1291 | wps 415.6 | wpb 638 | bsz 159.6 | num_updates 676 | best_score 0.1481
2024-10-29 13:02:39 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 13 @ 676 updates
2024-10-29 13:02:39 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping



local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
slice_id 3 seek offset 24738
slice_id 6 seek offset 49476
slice_id 5 seek offset 41230
slice_id 7 seek offset 57721
slice_id 1 seek offset 8246
slice_id 4 seek offset 32984
slice_id 2 seek offset 16492
2024-10-29 13:02:50 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:02:50 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 13 @ 676 updates, score 0.1291) (writing took 10.685918893665075 seconds)
2024-10-29 13:02:50 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 13 @ 676 updates
2024-10-29 13:02:50 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:03:01 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:03:01 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 13 @ 676 updates, score 0) (writing took 10.72309786081314 seconds)
2024-10-29 13:03:02 - train.py[line:336] - INFO: end of epoch 13 (average epoch stats below)
2024-10-29 13:03:02 - progress_bar.py[line:282] - INFO: epoch 013 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.198 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.36 | wps 360.2 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 676 | lr 1.86094e-05 | gnorm 0.005 | clip 0 | loss_scale 256 | train_wall 181 | gb_free 9.5 | wall 9621
2024-10-29 13:03:02 - trainer.py[line:639] - INFO: loading train data for epoch 14
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 13:03:04 - trainer.py[line:703] - INFO: begin training epoch 14
2024-10-29 13:03:04 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 13:03:56 - progress_bar.py[line:274] - INFO: epoch 014:      4 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.196, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=18.33, wps=219.5, ups=0.04, wpb=4926.1, bsz=1232, num_updates=680, lr=1.84049e-05, gnorm=0.004, clip=0, loss_scale=256, train_wall=34, gb_free=9.4, wall=9675
2024-10-29 13:05:56 - progress_bar.py[line:274] - INFO: epoch 014:     14 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.209, ntokens=5072.4, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=18.49, wps=421.6, ups=0.08, wpb=5072.4, bsz=1268.6, num_updates=690, lr=1.78937e-05, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=9796
2024-10-29 13:07:57 - progress_bar.py[line:274] - INFO: epoch 014:     24 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.19, ntokens=5118.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.25, wps=424.2, ups=0.08, wpb=5118.5, bsz=1280, num_updates=700, lr=1.73824e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=9916
2024-10-29 13:09:57 - progress_bar.py[line:274] - INFO: epoch 014:     34 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.205, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.45, wps=425.3, ups=0.08, wpb=5117.9, bsz=1280, num_updates=710, lr=1.68712e-05, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=10037
2024-10-29 13:11:58 - progress_bar.py[line:274] - INFO: epoch 014:     44 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.196, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.32, wps=423.7, ups=0.08, wpb=5118, bsz=1280, num_updates=720, lr=1.63599e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=10158
slice_id 2 seek offset 2075
slice_id 6 seek offset 6223
slice_id 4 seek offset 4149
slice_id 7 seek offset 7260
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038
2024-10-29 13:13:29 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 3 seek offset 3112
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 7 seek offset 7260
slice_id 6 seek offset 6223
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
2024-10-29 13:14:50 - progress_bar.py[line:282] - INFO: epoch 014 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.202 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.4 | score 0.1293 | wps 414.6 | wpb 638 | bsz 159.6 | num_updates 728 | best_score 0.1481
2024-10-29 13:14:50 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 14 @ 728 updates
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping2024-10-29 13:14:50 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
slice_id 2 seek offset 16492
slice_id 5 seek offset 41230
slice_id 1 seek offset 8246
slice_id 6 seek offset 49476
slice_id 4 seek offset 32984
slice_id 7 seek offset 57721
slice_id 3 seek offset 24738
2024-10-29 13:15:00 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:15:00 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 14 @ 728 updates, score 0.1293) (writing took 10.193228218704462 seconds)
2024-10-29 13:15:01 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 14 @ 728 updates
2024-10-29 13:15:01 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:15:11 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:15:12 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 14 @ 728 updates, score 0) (writing took 10.607906814664602 seconds)
2024-10-29 13:15:12 - train.py[line:336] - INFO: end of epoch 14 (average epoch stats below)
2024-10-29 13:15:12 - progress_bar.py[line:282] - INFO: epoch 014 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 4.199 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.37 | wps 361 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 728 | lr 1.59509e-05 | gnorm 0.005 | clip 0 | loss_scale 256 | train_wall 181 | gb_free 9.5 | wall 10352
2024-10-29 13:15:12 - trainer.py[line:639] - INFO: loading train data for epoch 15
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 13:15:15 - trainer.py[line:703] - INFO: begin training epoch 15
2024-10-29 13:15:15 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 13:15:42 - progress_bar.py[line:274] - INFO: epoch 015:      2 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.195, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=18.31, wps=219.6, ups=0.04, wpb=4926.1, bsz=1232, num_updates=730, lr=1.58487e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=34, gb_free=9.5, wall=10382
2024-10-29 13:17:43 - progress_bar.py[line:274] - INFO: epoch 015:     12 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.188, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.23, wps=423.3, ups=0.08, wpb=5118, bsz=1280, num_updates=740, lr=1.53374e-05, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=10503
2024-10-29 13:19:44 - progress_bar.py[line:274] - INFO: epoch 015:     22 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.183, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.16, wps=423.1, ups=0.08, wpb=5117.9, bsz=1280, num_updates=750, lr=1.48262e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=10624
2024-10-29 13:21:45 - progress_bar.py[line:274] - INFO: epoch 015:     32 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.176, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.08, wps=424.4, ups=0.08, wpb=5118.3, bsz=1280, num_updates=760, lr=1.43149e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=10744
2024-10-29 13:23:45 - progress_bar.py[line:274] - INFO: epoch 015:     42 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.193, ntokens=5072.6, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=18.29, wps=422.8, ups=0.08, wpb=5072.6, bsz=1268.6, num_updates=770, lr=1.38037e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=10864
2024-10-29 13:25:40 - progress_bar.py[line:274] - INFO: epoch 015:     52 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.182, ntokens=4925.6, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=18.15, wps=426.7, ups=0.09, wpb=4925.6, bsz=1232, num_updates=780, lr=1.32924e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=34, gb_free=9.5, wall=10980
slice_id 4 seek offset 4149
slice_id 3 seek offset 3112
slice_id 6 seek offset 6223
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038
2024-10-29 13:25:40 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 7 seek offset 7260
slice_id 3 seek offset 3112
slice_id 6 seek offset 6223
slice_id 4 seek offset 4149
2024-10-29 13:27:01 - progress_bar.py[line:282] - INFO: epoch 015 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.2 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.38 | score 0.1298 | wps 416.4 | wpb 638 | bsz 159.6 | num_updates 780 | best_score 0.1481
2024-10-29 13:27:01 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 15 @ 780 updates
2024-10-29 13:27:01 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
slice_id 4 seek offset 32984
slice_id 5 seek offset 41230
slice_id 6 seek offset 49476
slice_id 3 seek offset 24738
slice_id 2 seek offset 16492
slice_id 1 seek offset 8246
slice_id 7 seek offset 57721
2024-10-29 13:27:11 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:27:11 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 15 @ 780 updates, score 0.1298) (writing took 9.880040004849434 seconds)
2024-10-29 13:27:12 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 15 @ 780 updates
2024-10-29 13:27:12 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:27:22 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:27:22 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 15 @ 780 updates, score 0) (writing took 10.569128341972828 seconds)
2024-10-29 13:27:23 - train.py[line:336] - INFO: end of epoch 15 (average epoch stats below)
2024-10-29 13:27:23 - progress_bar.py[line:282] - INFO: epoch 015 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 4.186 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.2 | wps 361 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 780 | lr 1.32924e-05 | gnorm 0.006 | clip 0 | loss_scale 256 | train_wall 181 | gb_free 9.5 | wall 11083
2024-10-29 13:27:23 - trainer.py[line:639] - INFO: loading train data for epoch 16
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 13:27:26 - trainer.py[line:703] - INFO: begin training epoch 16
2024-10-29 13:27:26 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 13:29:29 - progress_bar.py[line:274] - INFO: epoch 016:     10 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.197, ntokens=5072.8, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=18.34, wps=221.8, ups=0.04, wpb=5072.8, bsz=1268.6, num_updates=790, lr=1.27812e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=11208
2024-10-29 13:31:30 - progress_bar.py[line:274] - INFO: epoch 016:     20 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.182, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.15, wps=422.2, ups=0.08, wpb=5118, bsz=1280, num_updates=800, lr=1.22699e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=11330
2024-10-29 13:33:30 - progress_bar.py[line:274] - INFO: epoch 016:     30 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.193, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.3, wps=426.3, ups=0.08, wpb=5118.3, bsz=1280, num_updates=810, lr=1.17587e-05, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=11450
2024-10-29 13:35:31 - progress_bar.py[line:274] - INFO: epoch 016:     40 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.195, ntokens=5117.8, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.32, wps=422.5, ups=0.08, wpb=5117.8, bsz=1280, num_updates=820, lr=1.12474e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=11571
2024-10-29 13:37:32 - progress_bar.py[line:274] - INFO: epoch 016:     50 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.216, ntokens=5117.7, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.59, wps=425, ups=0.08, wpb=5117.7, bsz=1280, num_updates=830, lr=1.07362e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=11691
slice_id 7 seek offset 7260
2024-10-29 13:37:50 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 4 seek offset 4149
slice_id 1 seek offset 1038
slice_id 6 seek offset 6223slice_id 3 seek offset 3112
slice_id 5 seek offset 5186
slice_id 2 seek offset 2075
slice_id 0 seek offset 0

slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 6 seek offset 6223
slice_id 5 seek offset 5186
slice_id 7 seek offset 7260
slice_id 1 seek offset 1038
slice_id 4 seek offset 4149
slice_id 3 seek offset 3112
2024-10-29 13:39:12 - progress_bar.py[line:282] - INFO: epoch 016 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.178 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.1 | score 0.1021 | wps 415.3 | wpb 638 | bsz 159.6 | num_updates 832 | best_score 0.1481
2024-10-29 13:39:12 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 16 @ 832 updates
2024-10-29 13:39:12 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping



local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
slice_id 1 seek offset 8246
slice_id 3 seek offset 24738
slice_id 2 seek offset 16492
slice_id 5 seek offset 41230
slice_id 4 seek offset 32984
slice_id 6 seek offset 49476
slice_id 7 seek offset 57721
2024-10-29 13:39:22 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:39:22 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 16 @ 832 updates, score 0.1021) (writing took 10.546369727700949 seconds)
2024-10-29 13:39:23 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 16 @ 832 updates
2024-10-29 13:39:23 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:39:33 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:39:33 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 16 @ 832 updates, score 0) (writing took 10.348072558641434 seconds)
2024-10-29 13:39:34 - train.py[line:336] - INFO: end of epoch 16 (average epoch stats below)
2024-10-29 13:39:34 - progress_bar.py[line:282] - INFO: epoch 016 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 4.196 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.33 | wps 361 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 832 | lr 1.06339e-05 | gnorm 0.005 | clip 0 | loss_scale 256 | train_wall 181 | gb_free 9.5 | wall 11813
2024-10-29 13:39:34 - trainer.py[line:639] - INFO: loading train data for epoch 17
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 13:39:36 - trainer.py[line:703] - INFO: begin training epoch 17
2024-10-29 13:39:36 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 13:41:16 - progress_bar.py[line:274] - INFO: epoch 017:      8 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.202, ntokens=4926.4, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=18.41, wps=219.5, ups=0.04, wpb=4926.4, bsz=1232, num_updates=840, lr=1.02249e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=34, gb_free=9.5, wall=11916
2024-10-29 13:43:16 - progress_bar.py[line:274] - INFO: epoch 017:     18 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.227, ntokens=5072.4, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=18.73, wps=422, ups=0.08, wpb=5072.4, bsz=1268.6, num_updates=850, lr=9.7137e-06, gnorm=0.008, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=12036
2024-10-29 13:45:18 - progress_bar.py[line:274] - INFO: epoch 017:     28 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.234, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.82, wps=422.2, ups=0.08, wpb=5118.2, bsz=1280, num_updates=860, lr=9.20245e-06, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=12157
2024-10-29 13:47:18 - progress_bar.py[line:274] - INFO: epoch 017:     38 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.235, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.83, wps=424.9, ups=0.08, wpb=5118.2, bsz=1280, num_updates=870, lr=8.69121e-06, gnorm=0.006, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=12278
2024-10-29 13:49:19 - progress_bar.py[line:274] - INFO: epoch 017:     48 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.234, ntokens=5117.6, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.82, wps=423.9, ups=0.08, wpb=5117.6, bsz=1280, num_updates=880, lr=8.17996e-06, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=12398
slice_id 4 seek offset 4149slice_id 3 seek offset 3112
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 6 seek offset 6223
2024-10-29 13:50:01 - train.py[line:449] - INFO: begin validation on "valid" subset

slice_id 5 seek offset 5186
slice_id 1 seek offset 1038
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038
slice_id 7 seek offset 7260
slice_id 2 seek offset 2075
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
2024-10-29 13:51:23 - progress_bar.py[line:282] - INFO: epoch 017 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.198 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.35 | score 0.1035 | wps 416.5 | wpb 638 | bsz 159.6 | num_updates 884 | best_score 0.1481
2024-10-29 13:51:23 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 17 @ 884 updates
2024-10-29 13:51:23 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
slice_id 6 seek offset 49476
slice_id 7 seek offset 57721
slice_id 4 seek offset 32984
slice_id 2 seek offset 16492
slice_id 1 seek offset 8246
slice_id 3 seek offset 24738
slice_id 5 seek offset 41230
2024-10-29 13:51:33 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:51:33 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 17 @ 884 updates, score 0.1035) (writing took 10.827037673443556 seconds)
2024-10-29 13:51:34 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 17 @ 884 updates
2024-10-29 13:51:34 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:51:44 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 13:51:45 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 17 @ 884 updates, score 0) (writing took 10.706647288054228 seconds)
2024-10-29 13:51:45 - train.py[line:336] - INFO: end of epoch 17 (average epoch stats below)
2024-10-29 13:51:45 - progress_bar.py[line:282] - INFO: epoch 017 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 4.228 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.74 | wps 360.6 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 884 | lr 7.97546e-06 | gnorm 0.007 | clip 0 | loss_scale 256 | train_wall 181 | gb_free 9.5 | wall 12545
2024-10-29 13:51:45 - trainer.py[line:639] - INFO: loading train data for epoch 18
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 13:51:48 - trainer.py[line:703] - INFO: begin training epoch 18
2024-10-29 13:51:48 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 13:53:03 - progress_bar.py[line:274] - INFO: epoch 018:      6 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.228, ntokens=4926.2, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=18.74, wps=219.7, ups=0.04, wpb=4926.2, bsz=1232, num_updates=890, lr=7.66871e-06, gnorm=0.008, clip=0, loss_scale=256, train_wall=34, gb_free=9.5, wall=12623
2024-10-29 13:55:04 - progress_bar.py[line:274] - INFO: epoch 018:     16 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.218, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.61, wps=423.1, ups=0.08, wpb=5118, bsz=1280, num_updates=900, lr=7.15746e-06, gnorm=0.006, clip=0, loss_scale=256, train_wall=35, gb_free=9.4, wall=12744
2024-10-29 13:57:05 - progress_bar.py[line:274] - INFO: epoch 018:     26 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.203, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.41, wps=421.8, ups=0.08, wpb=5118.3, bsz=1280, num_updates=910, lr=6.64622e-06, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.4, wall=12865
2024-10-29 13:59:05 - progress_bar.py[line:274] - INFO: epoch 018:     36 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.198, ntokens=5072.7, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=18.35, wps=423.6, ups=0.08, wpb=5072.7, bsz=1268.6, num_updates=920, lr=6.13497e-06, gnorm=0.006, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=12985
2024-10-29 14:01:06 - progress_bar.py[line:274] - INFO: epoch 018:     46 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.193, ntokens=5117.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.3, wps=424.6, ups=0.08, wpb=5117.5, bsz=1280, num_updates=930, lr=5.62372e-06, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=13105
slice_id 7 seek offset 7260
slice_id 2 seek offset 2075
slice_id 5 seek offset 5186
slice_id 4 seek offset 4149
slice_id 1 seek offset 1038
slice_id 3 seek offset 3112
2024-10-29 14:02:12 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 3 seek offset 3112
slice_id 7 seek offset 7260
slice_id 6 seek offset 6223
slice_id 4 seek offset 4149
2024-10-29 14:03:33 - progress_bar.py[line:282] - INFO: epoch 018 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.196 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.33 | score 0.1233 | wps 415.5 | wpb 638 | bsz 159.6 | num_updates 936 | best_score 0.1481
2024-10-29 14:03:33 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 18 @ 936 updates
2024-10-29 14:03:33 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966



file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
slice_id 5 seek offset 41230
slice_id 3 seek offset 24738
slice_id 1 seek offset 8246
slice_id 6 seek offset 49476
slice_id 7 seek offset 57721
slice_id 4 seek offset 32984
slice_id 2 seek offset 16492
2024-10-29 14:03:44 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 14:03:44 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 18 @ 936 updates, score 0.1233) (writing took 10.875095449388027 seconds)
2024-10-29 14:03:45 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 18 @ 936 updates
2024-10-29 14:03:45 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 14:03:55 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 14:03:56 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 18 @ 936 updates, score 0) (writing took 11.151505324989557 seconds)
2024-10-29 14:03:56 - train.py[line:336] - INFO: end of epoch 18 (average epoch stats below)
2024-10-29 14:03:56 - progress_bar.py[line:282] - INFO: epoch 018 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 4.206 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.45 | wps 360.7 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 936 | lr 5.31697e-06 | gnorm 0.006 | clip 0 | loss_scale 256 | train_wall 181 | gb_free 9.5 | wall 13276
2024-10-29 14:03:56 - trainer.py[line:639] - INFO: loading train data for epoch 19
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 14:03:59 - trainer.py[line:703] - INFO: begin training epoch 19
2024-10-29 14:03:59 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 14:04:50 - progress_bar.py[line:274] - INFO: epoch 019:      4 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.196, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=18.33, wps=219.3, ups=0.04, wpb=4926.1, bsz=1232, num_updates=940, lr=5.11247e-06, gnorm=0.005, clip=0, loss_scale=256, train_wall=34, gb_free=9.4, wall=13330
2024-10-29 14:06:50 - progress_bar.py[line:274] - INFO: epoch 019:     14 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.193, ntokens=5072.4, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=18.29, wps=422.5, ups=0.08, wpb=5072.4, bsz=1268.6, num_updates=950, lr=4.60123e-06, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=13450
2024-10-29 14:08:52 - progress_bar.py[line:274] - INFO: epoch 019:     24 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.185, ntokens=5118.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.19, wps=420.8, ups=0.08, wpb=5118.5, bsz=1280, num_updates=960, lr=4.08998e-06, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=13572
2024-10-29 14:10:53 - progress_bar.py[line:274] - INFO: epoch 019:     34 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.189, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.24, wps=421.7, ups=0.08, wpb=5117.9, bsz=1280, num_updates=970, lr=3.57873e-06, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=13693
2024-10-29 14:12:55 - progress_bar.py[line:274] - INFO: epoch 019:     44 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.175, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.06, wps=421.7, ups=0.08, wpb=5118, bsz=1280, num_updates=980, lr=3.06748e-06, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=13814
slice_id 2 seek offset 2075
slice_id 4 seek offset 41492024-10-29 14:14:26 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 5 seek offset 5186slice_id 7 seek offset 7260
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038
slice_id 6 seek offset 6223
slice_id 0 seek offset 0


slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 1 seek offset 1038
slice_id 3 seek offset 3112
slice_id 2 seek offset 2075
slice_id 5 seek offset 5186
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
2024-10-29 14:15:48 - progress_bar.py[line:282] - INFO: epoch 019 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.172 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.03 | score 0.1232 | wps 415.7 | wpb 638 | bsz 159.6 | num_updates 988 | best_score 0.1481
2024-10-29 14:15:48 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 19 @ 988 updates
2024-10-29 14:15:48 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
slice_id 4 seek offset 32984
slice_id 6 seek offset 49476
slice_id 5 seek offset 41230
slice_id 2 seek offset 16492
slice_id 1 seek offset 8246
slice_id 3 seek offset 24738
slice_id 7 seek offset 57721
2024-10-29 14:15:58 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 14:15:59 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 19 @ 988 updates, score 0.1232) (writing took 10.875864047557116 seconds)
2024-10-29 14:15:59 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 19 @ 988 updates
2024-10-29 14:15:59 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 14:16:09 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 14:16:10 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 19 @ 988 updates, score 0) (writing took 10.652387388050556 seconds)
2024-10-29 14:16:10 - train.py[line:336] - INFO: end of epoch 19 (average epoch stats below)
2024-10-29 14:16:10 - progress_bar.py[line:282] - INFO: epoch 019 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 4.184 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.18 | wps 359.5 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 988 | lr 2.65849e-06 | gnorm 0.004 | clip 0 | loss_scale 256 | train_wall 181 | gb_free 9.5 | wall 14010
2024-10-29 14:16:10 - trainer.py[line:639] - INFO: loading train data for epoch 20
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 14:16:13 - trainer.py[line:703] - INFO: begin training epoch 20
2024-10-29 14:16:13 - train.py[line:297] - INFO: Start iterating over samples
2024-10-29 14:16:40 - progress_bar.py[line:274] - INFO: epoch 020:      2 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.177, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=18.09, wps=218.8, ups=0.04, wpb=4926.1, bsz=1232, num_updates=990, lr=2.55624e-06, gnorm=0.004, clip=0, loss_scale=256, train_wall=34, gb_free=9.5, wall=14039
2024-10-29 14:18:41 - progress_bar.py[line:274] - INFO: epoch 020:     12 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.189, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.24, wps=421.6, ups=0.08, wpb=5118, bsz=1280, num_updates=1000, lr=2.04499e-06, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=14161
slice_id 6 seek offset 6223
slice_id 7 seek offset 7260slice_id 2 seek offset 2075
slice_id 1 seek offset 1038

2024-10-29 14:18:41 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 5 seek offset 5186
slice_id 4 seek offset 4149
slice_id 3 seek offset 3112
slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 7 seek offset 7260
slice_id 5 seek offset 5186
slice_id 6 seek offset 6223
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
2024-10-29 14:20:03 - progress_bar.py[line:282] - INFO: epoch 020 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.18 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.12 | score 0.1252 | wps 412.2 | wpb 638 | bsz 159.6 | num_updates 1000 | best_score 0.1481
2024-10-29 14:20:03 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 20 @ 1000 updates
2024-10-29 14:20:03 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_20_1000.pt
2024-10-29 14:20:15 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_20_1000.pt
2024-10-29 14:20:21 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_20_1000.pt (epoch 20 @ 1000 updates, score 0.1252) (writing took 17.919153213500977 seconds)
2024-10-29 14:22:10 - progress_bar.py[line:274] - INFO: epoch 020:     22 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.174, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.05, wps=245.4, ups=0.05, wpb=5117.9, bsz=1280, num_updates=1010, lr=1.53374e-06, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=14369
2024-10-29 14:24:10 - progress_bar.py[line:274] - INFO: epoch 020:     32 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.179, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.12, wps=423.8, ups=0.08, wpb=5118.3, bsz=1280, num_updates=1020, lr=1.02249e-06, gnorm=0.004, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=14490
2024-10-29 14:26:11 - progress_bar.py[line:274] - INFO: epoch 020:     42 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.182, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=18.16, wps=424.4, ups=0.08, wpb=5118.2, bsz=1280, num_updates=1030, lr=5.11247e-07, gnorm=0.004, clip=0, loss_scale=512, train_wall=35, gb_free=9.5, wall=14611
2024-10-29 14:28:04 - progress_bar.py[line:274] - INFO: epoch 020:     52 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=4.182, ntokens=4880, nsentences=1220.6, sample_size=1220.6, sample_size_v1=0, sample_size_v2=0, ppl=18.15, wps=430.4, ups=0.09, wpb=4880, bsz=1220.6, num_updates=1040, lr=0, gnorm=0.004, clip=0, loss_scale=512, train_wall=34, gb_free=9.5, wall=14724
slice_id 6 seek offset 6223
slice_id 7 seek offset 7260
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038
slice_id 4 seek offset 4149
slice_id 5 seek offset 5186
slice_id 2 seek offset 2075
2024-10-29 14:28:04 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 7 seek offset 7260
slice_id 6 seek offset 6223
slice_id 4 seek offset 4149
slice_id 3 seek offset 3112
2024-10-29 14:29:26 - progress_bar.py[line:282] - INFO: epoch 020 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.172 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.03 | score 0.1191 | wps 415.2 | wpb 638 | bsz 159.6 | num_updates 1040 | best_score 0.1481
2024-10-29 14:29:26 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 20 @ 1040 updates
2024-10-29 14:29:26 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 1 seek offset 8246
slice_id 6 seek offset 49476
slice_id 5 seek offset 41230
slice_id 7 seek offset 57721
slice_id 2 seek offset 16492
slice_id 4 seek offset 32984
slice_id 3 seek offset 24738
2024-10-29 14:29:36 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 14:29:36 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 20 @ 1040 updates, score 0.1191) (writing took 10.288783710449934 seconds)
2024-10-29 14:29:37 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 20 @ 1040 updates
2024-10-29 14:29:37 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 14:29:46 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt
2024-10-29 14:29:47 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/20_5e-5_512/checkpoint_last.pt (epoch 20 @ 1040 updates, score 0) (writing took 10.004201736301184 seconds)
2024-10-29 14:29:47 - train.py[line:336] - INFO: end of epoch 20 (average epoch stats below)
2024-10-29 14:29:47 - progress_bar.py[line:282] - INFO: epoch 020 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 4.182 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 18.15 | wps 322.7 | ups 0.06 | wpb 5072.4 | bsz 1268.6 | num_updates 1040 | lr 0 | gnorm 0.004 | clip 0 | loss_scale 512 | train_wall 181 | gb_free 9.5 | wall 14827
2024-10-29 14:29:47 - trainer.py[line:639] - INFO: loading train data for epoch 21
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-29 14:29:50 - train.py[line:206] - INFO: done training in 14826.7 seconds
