/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2024-10-30 04:33:05 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-30 04:33:05 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-30 04:33:05 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-30 04:33:05 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-30 04:33:05 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-30 04:33:05 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-30 04:33:05 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-30 04:33:05 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-10-30 04:33:11 - utils.py[line:258] - INFO: distributed init (rank 6): env://
2024-10-30 04:33:11 - utils.py[line:261] - INFO: Start init
2024-10-30 04:33:11 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 6
2024-10-30 04:33:11 - utils.py[line:258] - INFO: distributed init (rank 4): env://
2024-10-30 04:33:11 - utils.py[line:261] - INFO: Start init
2024-10-30 04:33:11 - utils.py[line:258] - INFO: distributed init (rank 2): env://
2024-10-30 04:33:11 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 4
2024-10-30 04:33:11 - utils.py[line:261] - INFO: Start init
2024-10-30 04:33:11 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2024-10-30 04:33:11 - utils.py[line:258] - INFO: distributed init (rank 5): env://
2024-10-30 04:33:11 - utils.py[line:261] - INFO: Start init
2024-10-30 04:33:11 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 5
2024-10-30 04:33:11 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2024-10-30 04:33:11 - utils.py[line:261] - INFO: Start init
2024-10-30 04:33:11 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2024-10-30 04:33:11 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2024-10-30 04:33:11 - utils.py[line:261] - INFO: Start init
2024-10-30 04:33:11 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2024-10-30 04:33:11 - utils.py[line:258] - INFO: distributed init (rank 7): env://
2024-10-30 04:33:11 - utils.py[line:261] - INFO: Start init
2024-10-30 04:33:11 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 7
2024-10-30 04:33:11 - utils.py[line:258] - INFO: distributed init (rank 3): env://
2024-10-30 04:33:11 - utils.py[line:261] - INFO: Start init
2024-10-30 04:33:11 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 3
2024-10-30 04:33:11 - distributed_c10d.py[line:354] - INFO: Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-30 04:33:11 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 3
single-machine distributed training is initialized.
2024-10-30 04:33:11 - distributed_c10d.py[line:354] - INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-30 04:33:11 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 2
single-machine distributed training is initialized.
2024-10-30 04:33:11 - distributed_c10d.py[line:354] - INFO: Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-30 04:33:11 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 7
single-machine distributed training is initialized.
2024-10-30 04:33:11 - distributed_c10d.py[line:354] - INFO: Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-30 04:33:11 - distributed_c10d.py[line:354] - INFO: Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-30 04:33:11 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 5
single-machine distributed training is initialized.
2024-10-30 04:33:11 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 6
single-machine distributed training is initialized.2024-10-30 04:33:11 - distributed_c10d.py[line:354] - INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.

2024-10-30 04:33:11 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 1
single-machine distributed training is initialized.
2024-10-30 04:33:11 - distributed_c10d.py[line:354] - INFO: Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-30 04:33:11 - distributed_c10d.py[line:354] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-10-30 04:33:11 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 0
2024-10-30 04:33:11 - utils.py[line:274] - INFO: initialized host cheetah-676973745f61696c6162-gzn5in-5cbcd8c768-b46pk as rank 4
single-machine distributed training is initialized.
single-machine distributed training is initialized.
2024-10-30 04:33:16 - train.py[line:77] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../polyformer_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 20, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 200, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'score', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='polyformer_b', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='polyformer_b', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid=20, best_checkpoint_metric='score', bf16=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, cls_weight=0.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../../datasets/pretrain/train_aihub_indoor.tsv,../../datasets/pretrain/val_aihub_indoor.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, det_weight=1.0, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_acc=True, eval_args='{"beam":5,"min_len":2,"max_len_a":0,"max_len_b":2}', eval_print_samples=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=False, freeze_encoder_embedding=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=200, max_image_size=512, max_source_positions=1024, max_src_length=80, max_target_positions=1024, max_tgt_length=420, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=8, num_bins=64, num_shards=1, num_workers=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', out_index=3, pad=1, patch_image_size=512, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, restore_file='checkpoint_last.pt', sample_patch_num=196, save_dir='./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512', save_interval=1, save_interval_updates=1000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', scst=False, scst_args='{}', seed=1, selected_cols='0,3,1,2', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, sync_bn=False, task='refcoco_pretrain', tensorboard_logdir=None, threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[8], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../polyformer_module', uses_ema=False, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=1000, vis_encoder_type='swin-base', wandb_project=None, warmup_ratio=0.06, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'refcoco_pretrain', 'data': '../../datasets/pretrain/train_aihub_indoor.tsv,../../datasets/pretrain/val_aihub_indoor.tsv', 'selected_cols': '0,3,1,2', 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 80, 'max_tgt_length': 420, 'code_dict_size': 8192, 'patch_image_size': 512, 'num_bins': 64, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'eval_acc': True, 'eval_args': '{"beam":5,"min_len":2,"max_len_a":0,"max_len_b":2}', 'uses_ema': False, 'eval_print_samples': False, 'max_image_size': 512, 'scst': False, 'scst_args': '{}'}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'det_weight': 1.0, 'cls_weight': 0.0, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.06, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2024-10-30 04:33:16 - base_task.py[line:187] - INFO: source dictionary: 4100 types
2024-10-30 04:33:16 - base_task.py[line:188] - INFO: target dictionary: 4100 types
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
The model and loaded state dict do not match exactly

unexpected key in source state_dict: norm.weight, norm.bias, head.weight, head.bias, layers.0.blocks.1.attn_mask, layers.1.blocks.1.attn_mask, layers.2.blocks.1.attn_mask, layers.2.blocks.3.attn_mask, layers.2.blocks.5.attn_mask, layers.2.blocks.7.attn_mask, layers.2.blocks.9.attn_mask, layers.2.blocks.11.attn_mask, layers.2.blocks.13.attn_mask, layers.2.blocks.15.attn_mask, layers.2.blocks.17.attn_mask

missing keys in source state_dict: norm3.weight, norm3.bias

Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
Loaded Swin Pretrained Weights ../../pretrained_weights/swin_base_patch4_window12_384_22k.pth
2024-10-30 04:33:29 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-30 04:33:29 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-30 04:33:29 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-30 04:33:29 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-10-30 04:33:29 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-10-30 04:33:29 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-10-30 04:33:29 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-30 04:33:29 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-10-30 04:33:29 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-30 04:33:29 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-10-30 04:33:29 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-30 04:33:29 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-10-30 04:33:29 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-30 04:33:29 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-10-30 04:33:29 - base_task.py[line:85] - WARNING: Some weights of the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin were not used when initializing XLMRobertaForMaskedLM: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']
- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2024-10-30 04:33:29 - base_task.py[line:96] - WARNING: Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at ../../pretrained_weights/bert-base-uncased-pytorch_model.bin and are newly initialized: ['bert.roberta.embeddings.position_ids', 'bert.roberta.embeddings.word_embeddings.weight', 'bert.roberta.embeddings.position_embeddings.weight', 'bert.roberta.embeddings.token_type_embeddings.weight', 'bert.roberta.embeddings.LayerNorm.weight', 'bert.roberta.embeddings.LayerNorm.bias', 'bert.roberta.encoder.layer.0.attention.self.query.weight', 'bert.roberta.encoder.layer.0.attention.self.query.bias', 'bert.roberta.encoder.layer.0.attention.self.key.weight', 'bert.roberta.encoder.layer.0.attention.self.key.bias', 'bert.roberta.encoder.layer.0.attention.self.value.weight', 'bert.roberta.encoder.layer.0.attention.self.value.bias', 'bert.roberta.encoder.layer.0.attention.output.dense.weight', 'bert.roberta.encoder.layer.0.attention.output.dense.bias', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.0.intermediate.dense.weight', 'bert.roberta.encoder.layer.0.intermediate.dense.bias', 'bert.roberta.encoder.layer.0.output.dense.weight', 'bert.roberta.encoder.layer.0.output.dense.bias', 'bert.roberta.encoder.layer.0.output.LayerNorm.weight', 'bert.roberta.encoder.layer.0.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.attention.self.query.weight', 'bert.roberta.encoder.layer.1.attention.self.query.bias', 'bert.roberta.encoder.layer.1.attention.self.key.weight', 'bert.roberta.encoder.layer.1.attention.self.key.bias', 'bert.roberta.encoder.layer.1.attention.self.value.weight', 'bert.roberta.encoder.layer.1.attention.self.value.bias', 'bert.roberta.encoder.layer.1.attention.output.dense.weight', 'bert.roberta.encoder.layer.1.attention.output.dense.bias', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.1.intermediate.dense.weight', 'bert.roberta.encoder.layer.1.intermediate.dense.bias', 'bert.roberta.encoder.layer.1.output.dense.weight', 'bert.roberta.encoder.layer.1.output.dense.bias', 'bert.roberta.encoder.layer.1.output.LayerNorm.weight', 'bert.roberta.encoder.layer.1.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.attention.self.query.weight', 'bert.roberta.encoder.layer.2.attention.self.query.bias', 'bert.roberta.encoder.layer.2.attention.self.key.weight', 'bert.roberta.encoder.layer.2.attention.self.key.bias', 'bert.roberta.encoder.layer.2.attention.self.value.weight', 'bert.roberta.encoder.layer.2.attention.self.value.bias', 'bert.roberta.encoder.layer.2.attention.output.dense.weight', 'bert.roberta.encoder.layer.2.attention.output.dense.bias', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.2.intermediate.dense.weight', 'bert.roberta.encoder.layer.2.intermediate.dense.bias', 'bert.roberta.encoder.layer.2.output.dense.weight', 'bert.roberta.encoder.layer.2.output.dense.bias', 'bert.roberta.encoder.layer.2.output.LayerNorm.weight', 'bert.roberta.encoder.layer.2.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.attention.self.query.weight', 'bert.roberta.encoder.layer.3.attention.self.query.bias', 'bert.roberta.encoder.layer.3.attention.self.key.weight', 'bert.roberta.encoder.layer.3.attention.self.key.bias', 'bert.roberta.encoder.layer.3.attention.self.value.weight', 'bert.roberta.encoder.layer.3.attention.self.value.bias', 'bert.roberta.encoder.layer.3.attention.output.dense.weight', 'bert.roberta.encoder.layer.3.attention.output.dense.bias', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.3.intermediate.dense.weight', 'bert.roberta.encoder.layer.3.intermediate.dense.bias', 'bert.roberta.encoder.layer.3.output.dense.weight', 'bert.roberta.encoder.layer.3.output.dense.bias', 'bert.roberta.encoder.layer.3.output.LayerNorm.weight', 'bert.roberta.encoder.layer.3.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.attention.self.query.weight', 'bert.roberta.encoder.layer.4.attention.self.query.bias', 'bert.roberta.encoder.layer.4.attention.self.key.weight', 'bert.roberta.encoder.layer.4.attention.self.key.bias', 'bert.roberta.encoder.layer.4.attention.self.value.weight', 'bert.roberta.encoder.layer.4.attention.self.value.bias', 'bert.roberta.encoder.layer.4.attention.output.dense.weight', 'bert.roberta.encoder.layer.4.attention.output.dense.bias', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.4.intermediate.dense.weight', 'bert.roberta.encoder.layer.4.intermediate.dense.bias', 'bert.roberta.encoder.layer.4.output.dense.weight', 'bert.roberta.encoder.layer.4.output.dense.bias', 'bert.roberta.encoder.layer.4.output.LayerNorm.weight', 'bert.roberta.encoder.layer.4.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.attention.self.query.weight', 'bert.roberta.encoder.layer.5.attention.self.query.bias', 'bert.roberta.encoder.layer.5.attention.self.key.weight', 'bert.roberta.encoder.layer.5.attention.self.key.bias', 'bert.roberta.encoder.layer.5.attention.self.value.weight', 'bert.roberta.encoder.layer.5.attention.self.value.bias', 'bert.roberta.encoder.layer.5.attention.output.dense.weight', 'bert.roberta.encoder.layer.5.attention.output.dense.bias', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.5.intermediate.dense.weight', 'bert.roberta.encoder.layer.5.intermediate.dense.bias', 'bert.roberta.encoder.layer.5.output.dense.weight', 'bert.roberta.encoder.layer.5.output.dense.bias', 'bert.roberta.encoder.layer.5.output.LayerNorm.weight', 'bert.roberta.encoder.layer.5.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.attention.self.query.weight', 'bert.roberta.encoder.layer.6.attention.self.query.bias', 'bert.roberta.encoder.layer.6.attention.self.key.weight', 'bert.roberta.encoder.layer.6.attention.self.key.bias', 'bert.roberta.encoder.layer.6.attention.self.value.weight', 'bert.roberta.encoder.layer.6.attention.self.value.bias', 'bert.roberta.encoder.layer.6.attention.output.dense.weight', 'bert.roberta.encoder.layer.6.attention.output.dense.bias', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.6.intermediate.dense.weight', 'bert.roberta.encoder.layer.6.intermediate.dense.bias', 'bert.roberta.encoder.layer.6.output.dense.weight', 'bert.roberta.encoder.layer.6.output.dense.bias', 'bert.roberta.encoder.layer.6.output.LayerNorm.weight', 'bert.roberta.encoder.layer.6.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.attention.self.query.weight', 'bert.roberta.encoder.layer.7.attention.self.query.bias', 'bert.roberta.encoder.layer.7.attention.self.key.weight', 'bert.roberta.encoder.layer.7.attention.self.key.bias', 'bert.roberta.encoder.layer.7.attention.self.value.weight', 'bert.roberta.encoder.layer.7.attention.self.value.bias', 'bert.roberta.encoder.layer.7.attention.output.dense.weight', 'bert.roberta.encoder.layer.7.attention.output.dense.bias', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.7.intermediate.dense.weight', 'bert.roberta.encoder.layer.7.intermediate.dense.bias', 'bert.roberta.encoder.layer.7.output.dense.weight', 'bert.roberta.encoder.layer.7.output.dense.bias', 'bert.roberta.encoder.layer.7.output.LayerNorm.weight', 'bert.roberta.encoder.layer.7.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.attention.self.query.weight', 'bert.roberta.encoder.layer.8.attention.self.query.bias', 'bert.roberta.encoder.layer.8.attention.self.key.weight', 'bert.roberta.encoder.layer.8.attention.self.key.bias', 'bert.roberta.encoder.layer.8.attention.self.value.weight', 'bert.roberta.encoder.layer.8.attention.self.value.bias', 'bert.roberta.encoder.layer.8.attention.output.dense.weight', 'bert.roberta.encoder.layer.8.attention.output.dense.bias', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.8.intermediate.dense.weight', 'bert.roberta.encoder.layer.8.intermediate.dense.bias', 'bert.roberta.encoder.layer.8.output.dense.weight', 'bert.roberta.encoder.layer.8.output.dense.bias', 'bert.roberta.encoder.layer.8.output.LayerNorm.weight', 'bert.roberta.encoder.layer.8.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.attention.self.query.weight', 'bert.roberta.encoder.layer.9.attention.self.query.bias', 'bert.roberta.encoder.layer.9.attention.self.key.weight', 'bert.roberta.encoder.layer.9.attention.self.key.bias', 'bert.roberta.encoder.layer.9.attention.self.value.weight', 'bert.roberta.encoder.layer.9.attention.self.value.bias', 'bert.roberta.encoder.layer.9.attention.output.dense.weight', 'bert.roberta.encoder.layer.9.attention.output.dense.bias', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.9.intermediate.dense.weight', 'bert.roberta.encoder.layer.9.intermediate.dense.bias', 'bert.roberta.encoder.layer.9.output.dense.weight', 'bert.roberta.encoder.layer.9.output.dense.bias', 'bert.roberta.encoder.layer.9.output.LayerNorm.weight', 'bert.roberta.encoder.layer.9.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.attention.self.query.weight', 'bert.roberta.encoder.layer.10.attention.self.query.bias', 'bert.roberta.encoder.layer.10.attention.self.key.weight', 'bert.roberta.encoder.layer.10.attention.self.key.bias', 'bert.roberta.encoder.layer.10.attention.self.value.weight', 'bert.roberta.encoder.layer.10.attention.self.value.bias', 'bert.roberta.encoder.layer.10.attention.output.dense.weight', 'bert.roberta.encoder.layer.10.attention.output.dense.bias', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.10.intermediate.dense.weight', 'bert.roberta.encoder.layer.10.intermediate.dense.bias', 'bert.roberta.encoder.layer.10.output.dense.weight', 'bert.roberta.encoder.layer.10.output.dense.bias', 'bert.roberta.encoder.layer.10.output.LayerNorm.weight', 'bert.roberta.encoder.layer.10.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.attention.self.query.weight', 'bert.roberta.encoder.layer.11.attention.self.query.bias', 'bert.roberta.encoder.layer.11.attention.self.key.weight', 'bert.roberta.encoder.layer.11.attention.self.key.bias', 'bert.roberta.encoder.layer.11.attention.self.value.weight', 'bert.roberta.encoder.layer.11.attention.self.value.bias', 'bert.roberta.encoder.layer.11.attention.output.dense.weight', 'bert.roberta.encoder.layer.11.attention.output.dense.bias', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.roberta.encoder.layer.11.intermediate.dense.weight', 'bert.roberta.encoder.layer.11.intermediate.dense.bias', 'bert.roberta.encoder.layer.11.output.dense.weight', 'bert.roberta.encoder.layer.11.output.dense.bias', 'bert.roberta.encoder.layer.11.output.LayerNorm.weight', 'bert.roberta.encoder.layer.11.output.LayerNorm.bias', 'bert.lm_head.bias', 'bert.lm_head.dense.weight', 'bert.lm_head.dense.bias', 'bert.lm_head.layer_norm.weight', 'bert.lm_head.layer_norm.bias', 'bert.lm_head.decoder.weight', 'bert.lm_head.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 2 row count 1037 total row count 8297
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 6 row count 1037 total row count 8297
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 7 row count 1037 total row count 8297
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 4 row count 1037 total row count 8297
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 5 row count 1037 total row count 8297
The model and loaded state dict do not match exactly

unexpected key in source state_dict: norm.weight, norm.bias, head.weight, head.bias, layers.0.blocks.1.attn_mask, layers.1.blocks.1.attn_mask, layers.2.blocks.1.attn_mask, layers.2.blocks.3.attn_mask, layers.2.blocks.5.attn_mask, layers.2.blocks.7.attn_mask, layers.2.blocks.9.attn_mask, layers.2.blocks.11.attn_mask, layers.2.blocks.13.attn_mask, layers.2.blocks.15.attn_mask, layers.2.blocks.17.attn_mask

missing keys in source state_dict: norm3.weight, norm3.bias

2024-10-30 04:33:30 - train.py[line:101] - INFO: PolyFormerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(4100, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): SwinTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=128, out_features=128, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): Identity()
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=128, out_features=512, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=512, out_features=128, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=128, out_features=128, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.009)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=128, out_features=512, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=512, out_features=128, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=512, out_features=256, bias=False)
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.017)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.026)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=1024, out_features=512, bias=False)
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.035)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.043)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (2): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.052)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (3): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.061)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (4): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.070)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (5): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.078)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (6): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.087)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (7): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.096)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (8): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.104)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (9): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.113)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (10): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.122)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (11): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.130)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (12): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.139)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (13): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.148)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (14): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.157)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (15): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.165)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (16): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.174)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (17): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.183)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=2048, out_features=1024, bias=False)
            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.191)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.200)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
    (bert): XLMRobertaForMaskedLM(
      (roberta): XLMRobertaModel(
        (embeddings): XLMRobertaEmbeddings(
          (word_embeddings): Embedding(250002, 768, padding_idx=1)
          (position_embeddings): Embedding(514, 768, padding_idx=1)
          (token_type_embeddings): Embedding(1, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): XLMRobertaEncoder(
          (layer): ModuleList(
            (0): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (lm_head): XLMRobertaLMHead(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (decoder): Linear(in_features=768, out_features=250002, bias=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(4100, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (reg_head): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=768, out_features=768, bias=True)
        (1): Linear(in_features=768, out_features=768, bias=True)
        (2): Linear(in_features=768, out_features=2, bias=True)
      )
    )
    (cls_head): Linear(in_features=768, out_features=3, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2024-10-30 04:33:30 - train.py[line:102] - INFO: task: RefcocoPretrainTask
2024-10-30 04:33:30 - train.py[line:103] - INFO: model: PolyFormerModel
2024-10-30 04:33:30 - train.py[line:104] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2024-10-30 04:33:30 - train.py[line:108] - INFO: num. shared model params: 478,547,732 (num. trained: 478,547,732)
2024-10-30 04:33:30 - train.py[line:115] - INFO: num. expert model params: 0 (num. trained: 0)
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 0 row count 1038 total row count 8297
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 3 row count 1037 total row count 8297
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/val_aihub_indoor.tsv slice_id 1 row count 1037 total row count 8297
2024-10-30 04:33:31 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2024-10-30 04:33:32 - distributed_c10d.py[line:354] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2024-10-30 04:33:32 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2024-10-30 04:33:32 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.layers.0.downsample.reduction.bias <- encoder.embed_images.layers.1.downsample.reduction.bias
2024-10-30 04:33:32 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.layers.0.downsample.reduction.bias <- encoder.embed_images.layers.2.downsample.reduction.bias
2024-10-30 04:33:32 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.layers.0.downsample.reduction.bias <- decoder.cls_head.bias
2024-10-30 04:33:32 - trainer.py[line:124] - INFO: detected shared parameter: encoder.bert.roberta.embeddings.word_embeddings.weight <- encoder.bert.lm_head.decoder.weight
2024-10-30 04:33:32 - trainer.py[line:124] - INFO: detected shared parameter: encoder.bert.lm_head.bias <- encoder.bert.lm_head.decoder.bias
2024-10-30 04:33:34 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 8 workers***********************
2024-10-30 04:33:34 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-30 04:33:34 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-30 04:33:34 - utils.py[line:765] - INFO: rank   2: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-30 04:33:34 - utils.py[line:765] - INFO: rank   3: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-30 04:33:34 - utils.py[line:765] - INFO: rank   4: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-30 04:33:34 - utils.py[line:765] - INFO: rank   5: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-30 04:33:34 - utils.py[line:765] - INFO: rank   6: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-30 04:33:34 - utils.py[line:765] - INFO: rank   7: capabilities =  8.0  ; total memory = 39.394 GB ; name = NVIDIA A100-SXM4-40GB                   
2024-10-30 04:33:34 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 8 workers***********************
2024-10-30 04:33:34 - train.py[line:145] - INFO: training on 8 devices (GPUs/TPUs)
2024-10-30 04:33:34 - train.py[line:151] - INFO: max tokens per device = None and max sentences per device = 20
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
2024-10-30 04:33:34 - trainer.py[line:458] - INFO: Preparing to load checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt


2024-10-30 04:33:34 - trainer.py[line:624] - INFO: No existing checkpoint found ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 04:33:34 - trainer.py[line:639] - INFO: loading train data for epoch 1
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966




file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
slice_id 0 seek offset 0
slice_id 4 seek offset 32984
slice_id 2 seek offset 16492
slice_id 1 seek offset 8246
slice_id 5 seek offset 41230
slice_id 6 seek offset 49476
slice_id 7 seek offset 57721
slice_id 3 seek offset 24738
Total steps 10400, warmup steps 624, warmup_factor 0.0016025641025641025
2024-10-30 04:33:37 - trainer.py[line:703] - INFO: begin training epoch 1
2024-10-30 04:33:37 - train.py[line:297] - INFO: Start iterating over samples
Total steps 10400, warmup steps 624, warmup_factor 0.0016025641025641025
Total steps 10400, warmup steps 624, warmup_factor 0.0016025641025641025
Total steps 10400, warmup steps 624, warmup_factor 0.0016025641025641025
Total steps 10400, warmup steps 624, warmup_factor 0.0016025641025641025
Total steps 10400, warmup steps 624, warmup_factor 0.0016025641025641025
Total steps 10400, warmup steps 624, warmup_factor 0.0016025641025641025
Total steps 10400, warmup steps 624, warmup_factor 0.0016025641025641025
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/jovyan/SSDb/miniconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
loss_reg: 0.382080078125 loss_cls: 0.0
loss_reg: 0.366943359375 loss_cls: 0.0
loss_reg: 0.377685546875 loss_cls: 0.0
loss_reg: 0.372802734375 loss_cls: 0.0
loss_reg: 0.3759765625 loss_cls: 0.0
loss_reg: 0.3857421875 loss_cls: 0.0
loss_reg: 0.354248046875 loss_cls: 0.0
loss_reg: 0.382080078125 loss_cls: 0.0
loss_reg: 0.331787109375 loss_cls: 0.0
loss_reg: 0.392333984375 loss_cls: 0.0
loss_reg: 0.365966796875 loss_cls: 0.0
loss_reg: 0.361083984375 loss_cls: 0.0
loss_reg: 0.38720703125 loss_cls: 0.0
loss_reg: 0.4033203125 loss_cls: 0.0
loss_reg: 0.3583984375 loss_cls: 0.0
loss_reg: 0.329833984375 loss_cls: 0.0
loss_reg: 0.383544921875 loss_cls: 0.0
loss_reg: 0.38330078125 loss_cls: 0.0
loss_reg: 0.353271484375 loss_cls: 0.0
loss_reg: 0.384521484375 loss_cls: 0.0
loss_reg: 0.35546875 loss_cls: 0.0
loss_reg: 0.366943359375 loss_cls: 0.0
loss_reg: 0.349853515625 loss_cls: 0.0
loss_reg: 0.355712890625 loss_cls: 0.0
loss_reg: 0.364501953125 loss_cls: 0.0
loss_reg: 0.333984375 loss_cls: 0.0
loss_reg: 0.330078125 loss_cls: 0.0
loss_reg: 0.38623046875 loss_cls: 0.0
loss_reg: 0.389892578125 loss_cls: 0.0
loss_reg: 0.35205078125 loss_cls: 0.0
loss_reg: 0.3271484375 loss_cls: 0.0
loss_reg: 0.360595703125 loss_cls: 0.0
loss_reg: 0.3544921875 loss_cls: 0.0
loss_reg: 0.36328125 loss_cls: 0.0
loss_reg: 0.366455078125 loss_cls: 0.0
loss_reg: 0.4150390625 loss_cls: 0.0
loss_reg: 0.369384765625 loss_cls: 0.0
loss_reg: 0.388916015625 loss_cls: 0.0
loss_reg: 0.3935546875 loss_cls: 0.0
loss_reg: 0.40625 loss_cls: 0.0
loss_reg: 0.3720703125 loss_cls: 0.0
loss_reg: 0.39404296875 loss_cls: 0.0
loss_reg: 0.362548828125 loss_cls: 0.0
loss_reg: 0.367431640625 loss_cls: 0.0
loss_reg: 0.373779296875 loss_cls: 0.0
loss_reg: 0.3671875 loss_cls: 0.0
loss_reg: 0.3681640625 loss_cls: 0.0
loss_reg: 0.39599609375 loss_cls: 0.0
loss_reg: 0.37451171875 loss_cls: 0.0
loss_reg: 0.358154296875 loss_cls: 0.0
loss_reg: 0.39208984375 loss_cls: 0.0
loss_reg: 0.365966796875 loss_cls: 0.0
loss_reg: 0.371826171875 loss_cls: 0.0
loss_reg: 0.39013671875 loss_cls: 0.0
loss_reg: 0.345947265625 loss_cls: 0.0
loss_reg: 0.359619140625 loss_cls: 0.0
loss_reg: 0.389892578125 loss_cls: 0.0
loss_reg: 0.344482421875 loss_cls: 0.0
loss_reg: 0.379638671875 loss_cls: 0.0
loss_reg: 0.4140625 loss_cls: 0.0
loss_reg: 0.37255859375 loss_cls: 0.0
loss_reg: 0.3994140625 loss_cls: 0.0
loss_reg: 0.3505859375 loss_cls: 0.0
loss_reg: 0.388671875 loss_cls: 0.0
2024-10-30 04:35:47 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 52 loss=0.019, loss_v1=0, loss_v2=0, nll_loss=3.324, ntokens=5118.4, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=10.02, wps=413.7, ups=0.08, wpb=5118.4, bsz=1280, num_updates=10, lr=8.01282e-07, gnorm=0.038, clip=0, loss_scale=128, train_wall=51, gb_free=9.5, wall=133
2024-10-30 04:37:50 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 52 loss=0.018, loss_v1=0, loss_v2=0, nll_loss=3.344, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=10.15, wps=417.3, ups=0.08, wpb=5118, bsz=1280, num_updates=20, lr=1.60256e-06, gnorm=0.03, clip=0, loss_scale=128, train_wall=87, gb_free=9.6, wall=256
2024-10-30 04:39:53 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 52 loss=0.017, loss_v1=0, loss_v2=0, nll_loss=3.377, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=10.39, wps=415.4, ups=0.08, wpb=5118.3, bsz=1280, num_updates=30, lr=2.40385e-06, gnorm=0.018, clip=0, loss_scale=128, train_wall=110, gb_free=9.5, wall=379
2024-10-30 04:41:57 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 52 loss=0.016, loss_v1=0, loss_v2=0, nll_loss=3.438, ntokens=5117.8, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=10.84, wps=414.8, ups=0.08, wpb=5117.8, bsz=1280, num_updates=40, lr=3.20513e-06, gnorm=0.014, clip=0, loss_scale=128, train_wall=123, gb_free=9.5, wall=502
2024-10-30 04:43:58 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 52 loss=0.015, loss_v1=0, loss_v2=0, nll_loss=3.628, ntokens=5072.1, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=12.36, wps=418.6, ups=0.08, wpb=5072.1, bsz=1268.6, num_updates=50, lr=4.00641e-06, gnorm=0.015, clip=0, loss_scale=128, train_wall=121, gb_free=9.6, wall=624
slice_id 3 seek offset 3112slice_id 2 seek offset 2075

slice_id 7 seek offset 7260
2024-10-30 04:44:16 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 1 seek offset 1038
slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 5 seek offset 5186
slice_id 4 seek offset 4149
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 0 seek offset 0
slice_id 3 seek offset 3112
slice_id 6 seek offset 6223
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
2024-10-30 04:45:37 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.014 | loss_v1 0 | loss_v2 0 | nll_loss 3.861 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.53 | score 0 | wps 422.2 | wpb 638 | bsz 159.6 | num_updates 52
2024-10-30 04:45:37 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 52 updates
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
2024-10-30 04:45:37 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping




file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 6 seek offset 49476
slice_id 5 seek offset 41230
slice_id 2 seek offset 16492
slice_id 7 seek offset 57721
slice_id 3 seek offset 24738
slice_id 4 seek offset 32984
slice_id 1 seek offset 8246
2024-10-30 04:45:47 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
2024-10-30 04:46:00 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt (epoch 1 @ 52 updates, score 0.0) (writing took 22.911187030375004 seconds)
2024-10-30 04:46:00 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 52 updates
2024-10-30 04:46:00 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
2024-10-30 04:46:10 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
2024-10-30 04:46:23 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt (epoch 1 @ 52 updates, score 0) (writing took 23.81037562340498 seconds)
2024-10-30 04:46:23 - train.py[line:336] - INFO: end of epoch 1 (average epoch stats below)
2024-10-30 04:46:23 - progress_bar.py[line:282] - INFO: epoch 001 | loss 0.017 | loss_v1 0 | loss_v2 0 | nll_loss 3.433 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 10.8 | wps 346.1 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 52 | lr 4.16667e-06 | gnorm 0.022 | clip 0 | loss_scale 128 | train_wall 511 | gb_free 9.5 | wall 769
2024-10-30 04:46:23 - trainer.py[line:639] - INFO: loading train data for epoch 2
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 04:46:26 - trainer.py[line:703] - INFO: begin training epoch 2
2024-10-30 04:46:26 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 04:48:04 - progress_bar.py[line:274] - INFO: epoch 002:      8 / 52 loss=0.014, loss_v1=0, loss_v2=0, nll_loss=3.811, ntokens=4880.9, nsentences=1220.6, sample_size=1220.6, sample_size_v1=0, sample_size_v2=0, ppl=14.03, wps=198.6, ups=0.04, wpb=4880.9, bsz=1220.6, num_updates=60, lr=4.80769e-06, gnorm=0.018, clip=0, loss_scale=128, train_wall=46, gb_free=9.5, wall=869
2024-10-30 04:50:04 - progress_bar.py[line:274] - INFO: epoch 002:     18 / 52 loss=0.013, loss_v1=0, loss_v2=0, nll_loss=3.829, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.21, wps=425.3, ups=0.08, wpb=5117.9, bsz=1280, num_updates=70, lr=5.60897e-06, gnorm=0.02, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=990
2024-10-30 04:52:04 - progress_bar.py[line:274] - INFO: epoch 002:     28 / 52 loss=0.012, loss_v1=0, loss_v2=0, nll_loss=3.897, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.9, wps=425.9, ups=0.08, wpb=5118.2, bsz=1280, num_updates=80, lr=6.41026e-06, gnorm=0.018, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=1110
2024-10-30 04:54:03 - progress_bar.py[line:274] - INFO: epoch 002:     38 / 52 loss=0.011, loss_v1=0, loss_v2=0, nll_loss=4.001, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=16.01, wps=429.1, ups=0.08, wpb=5118.2, bsz=1280, num_updates=90, lr=7.21154e-06, gnorm=0.014, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=1229
2024-10-30 04:56:03 - progress_bar.py[line:274] - INFO: epoch 002:     48 / 52 loss=0.011, loss_v1=0, loss_v2=0, nll_loss=4.108, ntokens=5117.6, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=17.25, wps=426.2, ups=0.08, wpb=5117.6, bsz=1280, num_updates=100, lr=8.01282e-06, gnorm=0.011, clip=0, loss_scale=128, train_wall=36, gb_free=9.5, wall=1349
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 5 seek offset 5186
slice_id 6 seek offset 6223
slice_id 3 seek offset 3112
2024-10-30 04:56:46 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 4 seek offset 4149
slice_id 1 seek offset 1038
slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038
slice_id 3 seek offset 3112
slice_id 6 seek offset 6223
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
2024-10-30 04:58:06 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 0.01 | loss_v1 0 | loss_v2 0 | nll_loss 4.17 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 18 | score 0.0943 | wps 420.5 | wpb 638 | bsz 159.6 | num_updates 104 | best_score 0.0943
2024-10-30 04:58:06 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 104 updates
2024-10-30 04:58:06 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 2 seek offset 16492
slice_id 1 seek offset 8246
slice_id 7 seek offset 57721
slice_id 5 seek offset 41230
slice_id 6 seek offset 49476
slice_id 4 seek offset 32984
slice_id 3 seek offset 24738
2024-10-30 04:58:16 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
2024-10-30 04:58:29 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt (epoch 2 @ 104 updates, score 0.0943) (writing took 22.63314376398921 seconds)
2024-10-30 04:58:29 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 104 updates
2024-10-30 04:58:29 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 04:58:40 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 04:58:40 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 2 @ 104 updates, score 0) (writing took 10.84479670599103 seconds)
2024-10-30 04:58:40 - train.py[line:336] - INFO: end of epoch 2 (average epoch stats below)
2024-10-30 04:58:40 - progress_bar.py[line:282] - INFO: epoch 002 | loss 0.012 | loss_v1 0 | loss_v2 0 | nll_loss 3.949 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 15.45 | wps 358.1 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 104 | lr 8.33333e-06 | gnorm 0.016 | clip 0 | loss_scale 128 | train_wall 184 | gb_free 9.5 | wall 1506
2024-10-30 04:58:40 - trainer.py[line:639] - INFO: loading train data for epoch 3
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 04:58:42 - trainer.py[line:703] - INFO: begin training epoch 3
2024-10-30 04:58:42 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 04:59:56 - progress_bar.py[line:274] - INFO: epoch 003:      6 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=4.128, ntokens=4880.7, nsentences=1220.6, sample_size=1220.6, sample_size_v1=0, sample_size_v2=0, ppl=17.48, wps=210, ups=0.04, wpb=4880.7, bsz=1220.6, num_updates=110, lr=8.8141e-06, gnorm=0.009, clip=0, loss_scale=128, train_wall=36, gb_free=9.5, wall=1582
2024-10-30 05:01:57 - progress_bar.py[line:274] - INFO: epoch 003:     16 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=4.075, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=16.85, wps=424, ups=0.08, wpb=5117.9, bsz=1280, num_updates=120, lr=9.61538e-06, gnorm=0.007, clip=0, loss_scale=128, train_wall=35, gb_free=9.4, wall=1702
2024-10-30 05:03:57 - progress_bar.py[line:274] - INFO: epoch 003:     26 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=4.036, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=16.41, wps=424.5, ups=0.08, wpb=5118.3, bsz=1280, num_updates=130, lr=1.04167e-05, gnorm=0.006, clip=0, loss_scale=128, train_wall=35, gb_free=9.4, wall=1823
2024-10-30 05:05:57 - progress_bar.py[line:274] - INFO: epoch 003:     36 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=4.019, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=16.21, wps=426.3, ups=0.08, wpb=5118.3, bsz=1280, num_updates=140, lr=1.12179e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=1943
2024-10-30 05:07:58 - progress_bar.py[line:274] - INFO: epoch 003:     46 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.967, ntokens=5117.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=15.64, wps=424.8, ups=0.08, wpb=5117.5, bsz=1280, num_updates=150, lr=1.20192e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=36, gb_free=9.4, wall=2063
slice_id 2 seek offset 2075
slice_id 6 seek offset 6223
slice_id 7 seek offset 7260
slice_id 5 seek offset 5186
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
slice_id 1 seek offset 10382024-10-30 05:09:04 - train.py[line:449] - INFO: begin validation on "valid" subset

slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 5 seek offset 5186
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 6 seek offset 6223
slice_id 1 seek offset 1038
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
2024-10-30 05:10:25 - progress_bar.py[line:282] - INFO: epoch 003 | valid on 'valid' subset | loss 0.01 | loss_v1 0 | loss_v2 0 | nll_loss 3.936 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 15.3 | score 0.1261 | wps 419.6 | wpb 638 | bsz 159.6 | num_updates 156 | best_score 0.1261
2024-10-30 05:10:25 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 3 @ 156 updates
2024-10-30 05:10:25 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
slice_id 5 seek offset 41230
slice_id 4 seek offset 32984
slice_id 7 seek offset 57721
slice_id 6 seek offset 49476
slice_id 2 seek offset 16492
slice_id 3 seek offset 24738
slice_id 1 seek offset 8246
2024-10-30 05:10:35 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
2024-10-30 05:10:48 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt (epoch 3 @ 156 updates, score 0.1261) (writing took 23.302667930722237 seconds)
2024-10-30 05:10:48 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 3 @ 156 updates
2024-10-30 05:10:48 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 05:10:59 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 05:10:59 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 3 @ 156 updates, score 0) (writing took 10.777565956115723 seconds)
2024-10-30 05:10:59 - train.py[line:336] - INFO: end of epoch 3 (average epoch stats below)
2024-10-30 05:10:59 - progress_bar.py[line:282] - INFO: epoch 003 | loss 0.01 | loss_v1 0 | loss_v2 0 | nll_loss 4.027 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 16.3 | wps 356.8 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 156 | lr 1.25e-05 | gnorm 0.005 | clip 0 | loss_scale 128 | train_wall 183 | gb_free 9.5 | wall 2245
2024-10-30 05:10:59 - trainer.py[line:639] - INFO: loading train data for epoch 4
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 05:11:02 - trainer.py[line:703] - INFO: begin training epoch 4
2024-10-30 05:11:02 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 05:11:53 - progress_bar.py[line:274] - INFO: epoch 004:      4 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.932, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=15.26, wps=209.5, ups=0.04, wpb=4926.1, bsz=1232, num_updates=160, lr=1.28205e-05, gnorm=0.003, clip=0, loss_scale=128, train_wall=34, gb_free=9.4, wall=2299
2024-10-30 05:13:53 - progress_bar.py[line:274] - INFO: epoch 004:     14 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.903, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.96, wps=425.6, ups=0.08, wpb=5118, bsz=1280, num_updates=170, lr=1.36218e-05, gnorm=0.003, clip=0, loss_scale=128, train_wall=35, gb_free=9.6, wall=2419
2024-10-30 05:15:53 - progress_bar.py[line:274] - INFO: epoch 004:     24 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.867, ntokens=5118.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.59, wps=427, ups=0.08, wpb=5118.5, bsz=1280, num_updates=180, lr=1.44231e-05, gnorm=0.003, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=2539
2024-10-30 05:17:52 - progress_bar.py[line:274] - INFO: epoch 004:     34 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.867, ntokens=5072.3, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=14.59, wps=425, ups=0.08, wpb=5072.3, bsz=1268.6, num_updates=190, lr=1.52244e-05, gnorm=0.003, clip=0, loss_scale=128, train_wall=36, gb_free=9.6, wall=2658
2024-10-30 05:19:52 - progress_bar.py[line:274] - INFO: epoch 004:     44 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.807, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14, wps=428.5, ups=0.08, wpb=5118, bsz=1280, num_updates=200, lr=1.60256e-05, gnorm=0.002, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=2778
slice_id 4 seek offset 4149
slice_id 2 seek offset 2075
slice_id 6 seek offset 6223
slice_id 3 seek offset 3112slice_id 7 seek offset 7260

2024-10-30 05:21:22 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038
slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 3 seek offset 3112
slice_id 5 seek offset 5186
slice_id 7 seek offset 7260
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 4 seek offset 4149
2024-10-30 05:22:43 - progress_bar.py[line:282] - INFO: epoch 004 | valid on 'valid' subset | loss 0.01 | loss_v1 0 | loss_v2 0 | nll_loss 3.814 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.07 | score 0.1237 | wps 418.8 | wpb 638 | bsz 159.6 | num_updates 208 | best_score 0.1261
2024-10-30 05:22:43 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 4 @ 208 updates
2024-10-30 05:22:43 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 2 seek offset 16492
slice_id 5 seek offset 41230
slice_id 4 seek offset 32984
slice_id 1 seek offset 8246
slice_id 6 seek offset 49476
slice_id 3 seek offset 24738
slice_id 7 seek offset 57721
2024-10-30 05:22:54 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 05:22:54 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 4 @ 208 updates, score 0.1237) (writing took 10.880041122436523 seconds)
2024-10-30 05:22:54 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 4 @ 208 updates
2024-10-30 05:22:54 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 05:23:05 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 05:23:05 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 4 @ 208 updates, score 0) (writing took 10.90651623159647 seconds)
2024-10-30 05:23:05 - train.py[line:336] - INFO: end of epoch 4 (average epoch stats below)
2024-10-30 05:23:05 - progress_bar.py[line:282] - INFO: epoch 004 | loss 0.01 | loss_v1 0 | loss_v2 0 | nll_loss 3.857 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.49 | wps 363.4 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 208 | lr 1.66667e-05 | gnorm 0.003 | clip 0 | loss_scale 128 | train_wall 182 | gb_free 9.5 | wall 2971
2024-10-30 05:23:05 - trainer.py[line:639] - INFO: loading train data for epoch 5
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 05:23:07 - trainer.py[line:703] - INFO: begin training epoch 5
2024-10-30 05:23:07 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 05:23:34 - progress_bar.py[line:274] - INFO: epoch 005:      2 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.805, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=13.98, wps=221.4, ups=0.04, wpb=4926.1, bsz=1232, num_updates=210, lr=1.68269e-05, gnorm=0.002, clip=0, loss_scale=128, train_wall=34, gb_free=9.5, wall=3000
2024-10-30 05:25:34 - progress_bar.py[line:274] - INFO: epoch 005:     12 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.76, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.54, wps=426.1, ups=0.08, wpb=5118, bsz=1280, num_updates=220, lr=1.76282e-05, gnorm=0.003, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=3120
2024-10-30 05:27:33 - progress_bar.py[line:274] - INFO: epoch 005:     22 / 52 loss=0.01, loss_v1=0, loss_v2=0, nll_loss=3.77, ntokens=5072.3, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=13.64, wps=428.7, ups=0.08, wpb=5072.3, bsz=1268.6, num_updates=230, lr=1.84295e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=3238
2024-10-30 05:29:32 - progress_bar.py[line:274] - INFO: epoch 005:     32 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.851, ntokens=5118.4, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.43, wps=428.2, ups=0.08, wpb=5118.4, bsz=1280, num_updates=240, lr=1.92308e-05, gnorm=0.006, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=3358
2024-10-30 05:31:32 - progress_bar.py[line:274] - INFO: epoch 005:     42 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.942, ntokens=5118.1, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=15.37, wps=428.1, ups=0.08, wpb=5118.1, bsz=1280, num_updates=250, lr=2.00321e-05, gnorm=0.01, clip=0, loss_scale=128, train_wall=35, gb_free=9.6, wall=3478
2024-10-30 05:33:27 - progress_bar.py[line:274] - INFO: epoch 005:     52 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.026, ntokens=4925.6, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=16.29, wps=429.2, ups=0.09, wpb=4925.6, bsz=1232, num_updates=260, lr=2.08333e-05, gnorm=0.01, clip=0, loss_scale=128, train_wall=36, gb_free=9.5, wall=3592
slice_id 4 seek offset 4149
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 3 seek offset 3112slice_id 1 seek offset 1038

slice_id 6 seek offset 6223
2024-10-30 05:33:27 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 5 seek offset 5186
slice_id 0 seek offset 0
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038
slice_id 2 seek offset 2075
slice_id 4 seek offset 4149
slice_id 7 seek offset 7260
slice_id 6 seek offset 6223
slice_id 3 seek offset 3112
2024-10-30 05:34:47 - progress_bar.py[line:282] - INFO: epoch 005 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.091 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 17.05 | score 0.139 | wps 420.7 | wpb 638 | bsz 159.6 | num_updates 260 | best_score 0.139
2024-10-30 05:34:47 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 5 @ 260 updates
2024-10-30 05:34:47 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 7 seek offset 57721
slice_id 3 seek offset 24738
slice_id 1 seek offset 8246
slice_id 4 seek offset 32984
slice_id 5 seek offset 41230
slice_id 6 seek offset 49476
slice_id 2 seek offset 16492
2024-10-30 05:34:57 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
2024-10-30 05:35:09 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt (epoch 5 @ 260 updates, score 0.139) (writing took 22.333856604993343 seconds)
2024-10-30 05:35:10 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 5 @ 260 updates
2024-10-30 05:35:10 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 05:35:21 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 05:35:21 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 5 @ 260 updates, score 0) (writing took 11.064955595880747 seconds)
2024-10-30 05:35:21 - train.py[line:336] - INFO: end of epoch 5 (average epoch stats below)
2024-10-30 05:35:21 - progress_bar.py[line:282] - INFO: epoch 005 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.866 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.58 | wps 358.5 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 260 | lr 2.08333e-05 | gnorm 0.006 | clip 0 | loss_scale 128 | train_wall 183 | gb_free 9.5 | wall 3706
2024-10-30 05:35:21 - trainer.py[line:639] - INFO: loading train data for epoch 6
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 05:35:23 - trainer.py[line:703] - INFO: begin training epoch 6
2024-10-30 05:35:23 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 05:37:26 - progress_bar.py[line:274] - INFO: epoch 006:     10 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.063, ntokens=5118.4, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=16.71, wps=213.9, ups=0.04, wpb=5118.4, bsz=1280, num_updates=270, lr=2.16346e-05, gnorm=0.007, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=3832
2024-10-30 05:39:26 - progress_bar.py[line:274] - INFO: epoch 006:     20 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.062, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=16.7, wps=424.7, ups=0.08, wpb=5118, bsz=1280, num_updates=280, lr=2.24359e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=35, gb_free=9.6, wall=3952
2024-10-30 05:41:27 - progress_bar.py[line:274] - INFO: epoch 006:     30 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.039, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=16.44, wps=425.8, ups=0.08, wpb=5118.3, bsz=1280, num_updates=290, lr=2.32372e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=4072
2024-10-30 05:43:26 - progress_bar.py[line:274] - INFO: epoch 006:     40 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.99, ntokens=5072.2, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=15.88, wps=425.1, ups=0.08, wpb=5072.2, bsz=1268.6, num_updates=300, lr=2.40385e-05, gnorm=0.006, clip=0, loss_scale=128, train_wall=35, gb_free=9.6, wall=4192
2024-10-30 05:45:27 - progress_bar.py[line:274] - INFO: epoch 006:     50 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.978, ntokens=5117.7, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=15.75, wps=424.1, ups=0.08, wpb=5117.7, bsz=1280, num_updates=310, lr=2.48397e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.6, wall=4312
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260slice_id 6 seek offset 6223
2024-10-30 05:45:45 - train.py[line:449] - INFO: begin validation on "valid" subset

slice_id 0 seek offset 0
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038slice_id 5 seek offset 5186
slice_id 4 seek offset 4149

slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 5 seek offset 5186
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 1 seek offset 1038
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
2024-10-30 05:47:06 - progress_bar.py[line:282] - INFO: epoch 006 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.004 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 16.05 | score 0.1476 | wps 419.2 | wpb 638 | bsz 159.6 | num_updates 312 | best_score 0.1476
2024-10-30 05:47:06 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 6 @ 312 updates
2024-10-30 05:47:06 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
slice_id 4 seek offset 32984
slice_id 1 seek offset 8246
slice_id 5 seek offset 41230
slice_id 7 seek offset 57721
slice_id 6 seek offset 49476
slice_id 3 seek offset 24738
slice_id 2 seek offset 16492
2024-10-30 05:47:16 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
2024-10-30 05:47:30 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt (epoch 6 @ 312 updates, score 0.1476) (writing took 23.90675787255168 seconds)
2024-10-30 05:47:30 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 6 @ 312 updates
2024-10-30 05:47:30 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 05:47:41 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 05:47:41 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 6 @ 312 updates, score 0) (writing took 11.518191393464804 seconds)
2024-10-30 05:47:41 - train.py[line:336] - INFO: end of epoch 6 (average epoch stats below)
2024-10-30 05:47:41 - progress_bar.py[line:282] - INFO: epoch 006 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 4.025 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 16.27 | wps 356.3 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 312 | lr 2.5e-05 | gnorm 0.005 | clip 0 | loss_scale 128 | train_wall 181 | gb_free 9.5 | wall 4447
2024-10-30 05:47:41 - trainer.py[line:639] - INFO: loading train data for epoch 7
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 05:47:44 - trainer.py[line:703] - INFO: begin training epoch 7
2024-10-30 05:47:44 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 05:49:22 - progress_bar.py[line:274] - INFO: epoch 007:      8 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.004, ntokens=4926.4, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=16.05, wps=209, ups=0.04, wpb=4926.4, bsz=1232, num_updates=320, lr=2.5641e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=34, gb_free=9.5, wall=4548
2024-10-30 05:51:22 - progress_bar.py[line:274] - INFO: epoch 007:     18 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=4.033, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=16.37, wps=428.1, ups=0.08, wpb=5118, bsz=1280, num_updates=330, lr=2.64423e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=4668
2024-10-30 05:53:21 - progress_bar.py[line:274] - INFO: epoch 007:     28 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.981, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=15.79, wps=428.9, ups=0.08, wpb=5118.2, bsz=1280, num_updates=340, lr=2.72436e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=4787
2024-10-30 05:55:20 - progress_bar.py[line:274] - INFO: epoch 007:     38 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.901, ntokens=5072.6, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=14.94, wps=426.8, ups=0.08, wpb=5072.6, bsz=1268.6, num_updates=350, lr=2.80449e-05, gnorm=0.006, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=4906
2024-10-30 05:57:20 - progress_bar.py[line:274] - INFO: epoch 007:     48 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.873, ntokens=5117.6, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.65, wps=426, ups=0.08, wpb=5117.6, bsz=1280, num_updates=360, lr=2.88462e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=5026
slice_id 2 seek offset 2075
slice_id 3 seek offset 3112
slice_id 7 seek offset 7260
slice_id 1 seek offset 1038
slice_id 6 seek offset 6223
slice_id 4 seek offset 4149slice_id 5 seek offset 51862024-10-30 05:58:02 - train.py[line:449] - INFO: begin validation on "valid" subset


slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 7 seek offset 7260
slice_id 6 seek offset 6223
slice_id 2 seek offset 2075
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 4 seek offset 4149
2024-10-30 05:59:23 - progress_bar.py[line:282] - INFO: epoch 007 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.899 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.92 | score 0.1488 | wps 418.8 | wpb 638 | bsz 159.6 | num_updates 364 | best_score 0.1488
2024-10-30 05:59:23 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 7 @ 364 updates
2024-10-30 05:59:23 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping




file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
slice_id 2 seek offset 16492
slice_id 6 seek offset 49476
slice_id 3 seek offset 24738
slice_id 4 seek offset 32984
slice_id 1 seek offset 8246
slice_id 5 seek offset 41230
slice_id 7 seek offset 57721
2024-10-30 05:59:33 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
2024-10-30 05:59:47 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt (epoch 7 @ 364 updates, score 0.1488) (writing took 23.45985161885619 seconds)
2024-10-30 05:59:47 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 7 @ 364 updates
2024-10-30 05:59:47 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 05:59:58 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 05:59:58 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 7 @ 364 updates, score 0) (writing took 11.383469112217426 seconds)
2024-10-30 05:59:58 - train.py[line:336] - INFO: end of epoch 7 (average epoch stats below)
2024-10-30 05:59:58 - progress_bar.py[line:282] - INFO: epoch 007 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.953 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 15.49 | wps 357.9 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 364 | lr 2.91667e-05 | gnorm 0.005 | clip 0 | loss_scale 128 | train_wall 181 | gb_free 9.5 | wall 5184
2024-10-30 05:59:58 - trainer.py[line:639] - INFO: loading train data for epoch 8
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 06:00:00 - trainer.py[line:703] - INFO: begin training epoch 8
2024-10-30 06:00:00 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 06:01:16 - progress_bar.py[line:274] - INFO: epoch 008:      6 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.888, ntokens=4926.2, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=14.8, wps=209.2, ups=0.04, wpb=4926.2, bsz=1232, num_updates=370, lr=2.96474e-05, gnorm=0.006, clip=0, loss_scale=128, train_wall=34, gb_free=9.5, wall=5261
2024-10-30 06:03:16 - progress_bar.py[line:274] - INFO: epoch 008:     16 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.93, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=15.24, wps=425.4, ups=0.08, wpb=5118, bsz=1280, num_updates=380, lr=3.04487e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=36, gb_free=9.4, wall=5382
2024-10-30 06:05:16 - progress_bar.py[line:274] - INFO: epoch 008:     26 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.937, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=15.32, wps=425.1, ups=0.08, wpb=5118.3, bsz=1280, num_updates=390, lr=3.125e-05, gnorm=0.006, clip=0, loss_scale=128, train_wall=35, gb_free=9.4, wall=5502
2024-10-30 06:07:15 - progress_bar.py[line:274] - INFO: epoch 008:     36 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.93, ntokens=5072.7, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=15.24, wps=426.5, ups=0.08, wpb=5072.7, bsz=1268.6, num_updates=400, lr=3.20513e-05, gnorm=0.006, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=5621
2024-10-30 06:09:15 - progress_bar.py[line:274] - INFO: epoch 008:     46 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.884, ntokens=5117.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.77, wps=428.5, ups=0.08, wpb=5117.5, bsz=1280, num_updates=410, lr=3.28526e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.4, wall=5740
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
slice_id 3 seek offset 3112
slice_id 6 seek offset 6223
2024-10-30 06:10:21 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 5 seek offset 5186slice_id 0 seek offset 0

slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 6 seek offset 6223
slice_id 3 seek offset 3112
slice_id 7 seek offset 7260
slice_id 5 seek offset 5186
slice_id 4 seek offset 4149
2024-10-30 06:11:42 - progress_bar.py[line:282] - INFO: epoch 008 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.846 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.38 | score 0.1435 | wps 419.3 | wpb 638 | bsz 159.6 | num_updates 416 | best_score 0.1488
2024-10-30 06:11:42 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 8 @ 416 updates
2024-10-30 06:11:42 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
slice_id 4 seek offset 32984
slice_id 1 seek offset 8246
slice_id 7 seek offset 57721
slice_id 5 seek offset 41230
slice_id 3 seek offset 24738
slice_id 6 seek offset 49476
slice_id 2 seek offset 16492
2024-10-30 06:11:52 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 06:11:53 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 8 @ 416 updates, score 0.1435) (writing took 10.454238820821047 seconds)
2024-10-30 06:11:53 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 8 @ 416 updates
2024-10-30 06:11:53 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 06:12:03 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 06:12:03 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 8 @ 416 updates, score 0) (writing took 10.596245214343071 seconds)
2024-10-30 06:12:03 - train.py[line:336] - INFO: end of epoch 8 (average epoch stats below)
2024-10-30 06:12:03 - progress_bar.py[line:282] - INFO: epoch 008 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.909 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 15.03 | wps 363.7 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 416 | lr 3.33333e-05 | gnorm 0.005 | clip 0 | loss_scale 128 | train_wall 182 | gb_free 9.5 | wall 5909
2024-10-30 06:12:03 - trainer.py[line:639] - INFO: loading train data for epoch 9
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 06:12:06 - trainer.py[line:703] - INFO: begin training epoch 9
2024-10-30 06:12:06 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 06:12:56 - progress_bar.py[line:274] - INFO: epoch 009:      4 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.844, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=14.36, wps=222.1, ups=0.05, wpb=4926.1, bsz=1232, num_updates=420, lr=3.36538e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=34, gb_free=9.4, wall=5962
2024-10-30 06:14:56 - progress_bar.py[line:274] - INFO: epoch 009:     14 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.861, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.53, wps=428.3, ups=0.08, wpb=5118, bsz=1280, num_updates=430, lr=3.44551e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.6, wall=6082
2024-10-30 06:16:54 - progress_bar.py[line:274] - INFO: epoch 009:     24 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.873, ntokens=5072.9, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=14.66, wps=428.7, ups=0.08, wpb=5072.9, bsz=1268.6, num_updates=440, lr=3.52564e-05, gnorm=0.007, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=6200
2024-10-30 06:18:54 - progress_bar.py[line:274] - INFO: epoch 009:     34 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.836, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.28, wps=426.1, ups=0.08, wpb=5117.9, bsz=1280, num_updates=450, lr=3.60577e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=35, gb_free=9.6, wall=6320
2024-10-30 06:20:54 - progress_bar.py[line:274] - INFO: epoch 009:     44 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.803, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.95, wps=427.6, ups=0.08, wpb=5118, bsz=1280, num_updates=460, lr=3.6859e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=36, gb_free=9.5, wall=6440
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 6 seek offset 6223
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
slice_id 5 seek offset 51862024-10-30 06:22:25 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 3 seek offset 3112
slice_id 0 seek offset 0

slice_id 0 seek offset 0
slice_id 5 seek offset 5186
slice_id 6 seek offset 6223
slice_id 7 seek offset 7260
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
2024-10-30 06:23:46 - progress_bar.py[line:282] - INFO: epoch 009 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.79 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.83 | score 0.1433 | wps 418.8 | wpb 638 | bsz 159.6 | num_updates 468 | best_score 0.1488
2024-10-30 06:23:46 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 9 @ 468 updates
2024-10-30 06:23:46 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 7 seek offset 57721
slice_id 6 seek offset 49476
slice_id 4 seek offset 32984
slice_id 5 seek offset 41230
slice_id 2 seek offset 16492
slice_id 1 seek offset 8246
slice_id 3 seek offset 24738
2024-10-30 06:23:56 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 06:23:57 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 9 @ 468 updates, score 0.1433) (writing took 10.706975050270557 seconds)
2024-10-30 06:23:57 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 9 @ 468 updates
2024-10-30 06:23:57 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 06:24:07 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 06:24:08 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 9 @ 468 updates, score 0) (writing took 10.965908043086529 seconds)
2024-10-30 06:24:08 - train.py[line:336] - INFO: end of epoch 9 (average epoch stats below)
2024-10-30 06:24:08 - progress_bar.py[line:282] - INFO: epoch 009 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.837 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.29 | wps 364.1 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 468 | lr 3.75e-05 | gnorm 0.005 | clip 0 | loss_scale 128 | train_wall 183 | gb_free 9.5 | wall 6633
2024-10-30 06:24:08 - trainer.py[line:639] - INFO: loading train data for epoch 10
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 06:24:10 - trainer.py[line:703] - INFO: begin training epoch 10
2024-10-30 06:24:10 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 06:24:37 - progress_bar.py[line:274] - INFO: epoch 010:      2 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.804, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=13.97, wps=220.9, ups=0.04, wpb=4926.1, bsz=1232, num_updates=470, lr=3.76603e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=36, gb_free=9.5, wall=6663
2024-10-30 06:26:38 - progress_bar.py[line:274] - INFO: epoch 010:     12 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.838, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.3, wps=422.9, ups=0.08, wpb=5118, bsz=1280, num_updates=480, lr=3.84615e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=6784
2024-10-30 06:28:38 - progress_bar.py[line:274] - INFO: epoch 010:     22 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.867, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.59, wps=426.4, ups=0.08, wpb=5117.9, bsz=1280, num_updates=490, lr=3.92628e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=6904
2024-10-30 06:30:38 - progress_bar.py[line:274] - INFO: epoch 010:     32 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.882, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.75, wps=425.6, ups=0.08, wpb=5118.3, bsz=1280, num_updates=500, lr=4.00641e-05, gnorm=0.005, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=7024
2024-10-30 06:32:38 - progress_bar.py[line:274] - INFO: epoch 010:     42 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.903, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.96, wps=428.4, ups=0.08, wpb=5118.2, bsz=1280, num_updates=510, lr=4.08654e-05, gnorm=0.004, clip=0, loss_scale=128, train_wall=35, gb_free=9.5, wall=7144
2024-10-30 06:34:31 - progress_bar.py[line:274] - INFO: epoch 010:     52 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.894, ntokens=4880, nsentences=1220.6, sample_size=1220.6, sample_size_v1=0, sample_size_v2=0, ppl=14.87, wps=431, ups=0.09, wpb=4880, bsz=1220.6, num_updates=520, lr=4.16667e-05, gnorm=0.004, clip=0, loss_scale=256, train_wall=34, gb_free=9.5, wall=7257
slice_id 6 seek offset 6223
slice_id 3 seek offset 3112
slice_id 7 seek offset 7260
slice_id 5 seek offset 5186
slice_id 4 seek offset 4149
slice_id 1 seek offset 10382024-10-30 06:34:31 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 2 seek offset 2075
slice_id 0 seek offset 0

slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 3 seek offset 3112
slice_id 7 seek offset 7260
slice_id 2 seek offset 2075
slice_id 4 seek offset 4149
2024-10-30 06:35:52 - progress_bar.py[line:282] - INFO: epoch 010 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.86 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.52 | score 0.1446 | wps 417.3 | wpb 638 | bsz 159.6 | num_updates 520 | best_score 0.1488
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
2024-10-30 06:35:52 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 10 @ 520 updates
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
2024-10-30 06:35:52 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
slice_id 1 seek offset 8246
slice_id 2 seek offset 16492
slice_id 6 seek offset 49476
slice_id 3 seek offset 24738
slice_id 5 seek offset 41230
slice_id 4 seek offset 32984
slice_id 7 seek offset 57721
2024-10-30 06:36:03 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 06:36:03 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 10 @ 520 updates, score 0.1446) (writing took 10.586769979447126 seconds)
2024-10-30 06:36:03 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 10 @ 520 updates
2024-10-30 06:36:03 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 06:36:13 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 06:36:13 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 10 @ 520 updates, score 0) (writing took 10.534193877130747 seconds)
2024-10-30 06:36:13 - train.py[line:336] - INFO: end of epoch 10 (average epoch stats below)
2024-10-30 06:36:13 - progress_bar.py[line:282] - INFO: epoch 010 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.873 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.65 | wps 363.4 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 520 | lr 4.16667e-05 | gnorm 0.005 | clip 0 | loss_scale 256 | train_wall 181 | gb_free 9.5 | wall 7359
2024-10-30 06:36:13 - trainer.py[line:639] - INFO: loading train data for epoch 11
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 06:36:16 - trainer.py[line:703] - INFO: begin training epoch 11
2024-10-30 06:36:16 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 06:38:19 - progress_bar.py[line:274] - INFO: epoch 011:     10 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.889, ntokens=5118.4, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.82, wps=224.8, ups=0.04, wpb=5118.4, bsz=1280, num_updates=530, lr=4.24679e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=36, gb_free=9.5, wall=7485
2024-10-30 06:40:19 - progress_bar.py[line:274] - INFO: epoch 011:     20 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.879, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.72, wps=425.6, ups=0.08, wpb=5118, bsz=1280, num_updates=540, lr=4.32692e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=7605
2024-10-30 06:42:18 - progress_bar.py[line:274] - INFO: epoch 011:     30 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.877, ntokens=5072.7, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=14.69, wps=426.4, ups=0.08, wpb=5072.7, bsz=1268.6, num_updates=550, lr=4.40705e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=7724
2024-10-30 06:44:17 - progress_bar.py[line:274] - INFO: epoch 011:     40 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.859, ntokens=5117.8, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.51, wps=429.5, ups=0.08, wpb=5117.8, bsz=1280, num_updates=560, lr=4.48718e-05, gnorm=0.006, clip=0, loss_scale=256, train_wall=36, gb_free=9.6, wall=7843
2024-10-30 06:46:19 - progress_bar.py[line:274] - INFO: epoch 011:     50 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.875, ntokens=5117.7, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.67, wps=421.3, ups=0.08, wpb=5117.7, bsz=1280, num_updates=570, lr=4.56731e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=38, gb_free=9.6, wall=7964
slice_id 2 seek offset 2075slice_id 3 seek offset 3112slice_id 4 seek offset 4149
slice_id 6 seek offset 6223
slice_id 5 seek offset 51862024-10-30 06:46:37 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 7 seek offset 7260

slice_id 1 seek offset 1038


slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 5 seek offset 5186
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 3 seek offset 3112
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
2024-10-30 06:47:58 - progress_bar.py[line:282] - INFO: epoch 011 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.837 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.29 | score 0.1391 | wps 417.9 | wpb 638 | bsz 159.6 | num_updates 572 | best_score 0.1488
2024-10-30 06:47:58 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 11 @ 572 updates
2024-10-30 06:47:58 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
slice_id 6 seek offset 49476
slice_id 7 seek offset 57721
slice_id 3 seek offset 24738
slice_id 4 seek offset 32984
slice_id 1 seek offset 8246
slice_id 2 seek offset 16492
slice_id 5 seek offset 41230
2024-10-30 06:48:09 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 06:48:09 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 11 @ 572 updates, score 0.1391) (writing took 10.68667071312666 seconds)
2024-10-30 06:48:09 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 11 @ 572 updates
2024-10-30 06:48:09 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 06:48:19 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 06:48:20 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 11 @ 572 updates, score 0) (writing took 10.515417449176311 seconds)
2024-10-30 06:48:20 - train.py[line:336] - INFO: end of epoch 11 (average epoch stats below)
2024-10-30 06:48:20 - progress_bar.py[line:282] - INFO: epoch 011 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.875 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.67 | wps 363.2 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 572 | lr 4.58333e-05 | gnorm 0.006 | clip 0 | loss_scale 256 | train_wall 187 | gb_free 9.5 | wall 8085
2024-10-30 06:48:20 - trainer.py[line:639] - INFO: loading train data for epoch 12
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 06:48:22 - trainer.py[line:703] - INFO: begin training epoch 12
2024-10-30 06:48:22 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 06:50:01 - progress_bar.py[line:274] - INFO: epoch 012:      8 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.85, ntokens=4926.4, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=14.42, wps=221.6, ups=0.04, wpb=4926.4, bsz=1232, num_updates=580, lr=4.64744e-05, gnorm=0.009, clip=0, loss_scale=256, train_wall=37, gb_free=9.5, wall=8187
2024-10-30 06:52:00 - progress_bar.py[line:274] - INFO: epoch 012:     18 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.892, ntokens=5072.4, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=14.85, wps=426.8, ups=0.08, wpb=5072.4, bsz=1268.6, num_updates=590, lr=4.72756e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=8306
2024-10-30 06:54:00 - progress_bar.py[line:274] - INFO: epoch 012:     28 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.942, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=15.37, wps=426.5, ups=0.08, wpb=5118.2, bsz=1280, num_updates=600, lr=4.80769e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=8426
2024-10-30 06:55:59 - progress_bar.py[line:274] - INFO: epoch 012:     38 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.911, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=15.04, wps=428.9, ups=0.08, wpb=5118.2, bsz=1280, num_updates=610, lr=4.88782e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=8545
2024-10-30 06:57:59 - progress_bar.py[line:274] - INFO: epoch 012:     48 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.919, ntokens=5117.6, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=15.13, wps=427.7, ups=0.08, wpb=5117.6, bsz=1280, num_updates=620, lr=4.96795e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=8665
slice_id 7 seek offset 7260
slice_id 2 seek offset 2075
slice_id 5 seek offset 5186
2024-10-30 06:58:42 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
slice_id 6 seek offset 6223
slice_id 1 seek offset 1038
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 5 seek offset 5186
slice_id 6 seek offset 6223
slice_id 7 seek offset 7260
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038
slice_id 4 seek offset 4149
2024-10-30 07:00:02 - progress_bar.py[line:282] - INFO: epoch 012 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.882 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.75 | score 0.1168 | wps 419.7 | wpb 638 | bsz 159.6 | num_updates 624 | best_score 0.1488
2024-10-30 07:00:02 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 12 @ 624 updates
2024-10-30 07:00:02 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 5 seek offset 41230
slice_id 1 seek offset 8246
slice_id 7 seek offset 57721
slice_id 6 seek offset 49476
slice_id 4 seek offset 32984
slice_id 3 seek offset 24738
slice_id 2 seek offset 16492
2024-10-30 07:00:12 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 07:00:13 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 12 @ 624 updates, score 0.1168) (writing took 10.281786687672138 seconds)
2024-10-30 07:00:13 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 12 @ 624 updates
2024-10-30 07:00:13 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 07:00:23 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 07:00:23 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 12 @ 624 updates, score 0) (writing took 10.649575226008892 seconds)
2024-10-30 07:00:23 - train.py[line:336] - INFO: end of epoch 12 (average epoch stats below)
2024-10-30 07:00:23 - progress_bar.py[line:282] - INFO: epoch 012 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.906 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.99 | wps 364.5 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 624 | lr 5e-05 | gnorm 0.007 | clip 0 | loss_scale 256 | train_wall 182 | gb_free 9.5 | wall 8809
2024-10-30 07:00:23 - trainer.py[line:639] - INFO: loading train data for epoch 13
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 07:00:26 - trainer.py[line:703] - INFO: begin training epoch 13
2024-10-30 07:00:26 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 07:01:41 - progress_bar.py[line:274] - INFO: epoch 013:      6 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.894, ntokens=4926.2, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=14.86, wps=221.7, ups=0.05, wpb=4926.2, bsz=1232, num_updates=630, lr=4.99693e-05, gnorm=0.006, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=8887
2024-10-30 07:03:41 - progress_bar.py[line:274] - INFO: epoch 013:     16 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.842, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.34, wps=425.5, ups=0.08, wpb=5118, bsz=1280, num_updates=640, lr=4.99182e-05, gnorm=0.006, clip=0, loss_scale=256, train_wall=35, gb_free=9.4, wall=9007
2024-10-30 07:05:41 - progress_bar.py[line:274] - INFO: epoch 013:     26 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.803, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.96, wps=426.9, ups=0.08, wpb=5118.3, bsz=1280, num_updates=650, lr=4.9867e-05, gnorm=0.009, clip=0, loss_scale=256, train_wall=35, gb_free=9.4, wall=9127
2024-10-30 07:07:40 - progress_bar.py[line:274] - INFO: epoch 013:     36 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.827, ntokens=5072.7, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=14.19, wps=425.8, ups=0.08, wpb=5072.7, bsz=1268.6, num_updates=660, lr=4.98159e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=9246
2024-10-30 07:09:40 - progress_bar.py[line:274] - INFO: epoch 013:     46 / 52 loss=0.009, loss_v1=0, loss_v2=0, nll_loss=3.863, ntokens=5117.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.55, wps=426.8, ups=0.08, wpb=5117.5, bsz=1280, num_updates=670, lr=4.97647e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.4, wall=9366
slice_id 3 seek offset 3112slice_id 6 seek offset 6223
slice_id 7 seek offset 7260slice_id 5 seek offset 5186
slice_id 4 seek offset 4149
slice_id 1 seek offset 1038
slice_id 2 seek offset 2075
2024-10-30 07:10:46 - train.py[line:449] - INFO: begin validation on "valid" subset

slice_id 0 seek offset 0

slice_id 5 seek offset 5186
slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038
slice_id 6 seek offset 6223
slice_id 4 seek offset 4149
2024-10-30 07:12:07 - progress_bar.py[line:282] - INFO: epoch 013 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.868 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.6 | score 0.0983 | wps 417 | wpb 638 | bsz 159.6 | num_updates 676 | best_score 0.1488
2024-10-30 07:12:07 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 13 @ 676 updates
2024-10-30 07:12:07 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
slice_id 5 seek offset 41230
slice_id 2 seek offset 16492
slice_id 7 seek offset 57721
slice_id 3 seek offset 24738
slice_id 1 seek offset 8246
slice_id 6 seek offset 49476
slice_id 4 seek offset 32984
2024-10-30 07:12:19 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 07:12:20 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 13 @ 676 updates, score 0.0983) (writing took 12.142572112381458 seconds)
2024-10-30 07:12:20 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 13 @ 676 updates
2024-10-30 07:12:20 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 07:12:32 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 07:12:32 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 13 @ 676 updates, score 0) (writing took 12.289880149066448 seconds)
2024-10-30 07:12:32 - train.py[line:336] - INFO: end of epoch 13 (average epoch stats below)
2024-10-30 07:12:32 - progress_bar.py[line:282] - INFO: epoch 013 | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.84 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.32 | wps 362 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 676 | lr 4.9734e-05 | gnorm 0.007 | clip 0 | loss_scale 256 | train_wall 181 | gb_free 9.5 | wall 9538
2024-10-30 07:12:32 - trainer.py[line:639] - INFO: loading train data for epoch 14
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 07:12:35 - trainer.py[line:703] - INFO: begin training epoch 14
2024-10-30 07:12:35 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 07:13:26 - progress_bar.py[line:274] - INFO: epoch 014:      4 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.849, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=14.41, wps=218.7, ups=0.04, wpb=4926.1, bsz=1232, num_updates=680, lr=4.97136e-05, gnorm=0.009, clip=0, loss_scale=256, train_wall=34, gb_free=9.4, wall=9591
2024-10-30 07:15:25 - progress_bar.py[line:274] - INFO: epoch 014:     14 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.877, ntokens=5072.4, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=14.69, wps=423.7, ups=0.08, wpb=5072.4, bsz=1268.6, num_updates=690, lr=4.96624e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=9711
2024-10-30 07:17:25 - progress_bar.py[line:274] - INFO: epoch 014:     24 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.857, ntokens=5118.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.49, wps=427.8, ups=0.08, wpb=5118.5, bsz=1280, num_updates=700, lr=4.96113e-05, gnorm=0.006, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=9831
2024-10-30 07:19:25 - progress_bar.py[line:274] - INFO: epoch 014:     34 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.809, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.02, wps=425.2, ups=0.08, wpb=5117.9, bsz=1280, num_updates=710, lr=4.95601e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=9951
2024-10-30 07:21:25 - progress_bar.py[line:274] - INFO: epoch 014:     44 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.8, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.93, wps=426.2, ups=0.08, wpb=5118, bsz=1280, num_updates=720, lr=4.9509e-05, gnorm=0.01, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=10071
slice_id 4 seek offset 4149
slice_id 2 seek offset 2075
slice_id 7 seek offset 72602024-10-30 07:22:55 - train.py[line:449] - INFO: begin validation on "valid" subset

slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 3 seek offset 3112
slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 4 seek offset 4149
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 3 seek offset 3112
2024-10-30 07:24:17 - progress_bar.py[line:282] - INFO: epoch 014 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.728 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.25 | score 0.149 | wps 417.5 | wpb 638 | bsz 159.6 | num_updates 728 | best_score 0.149
2024-10-30 07:24:17 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 14 @ 728 updates
2024-10-30 07:24:17 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966


local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 7 seek offset 57721
slice_id 5 seek offset 41230
slice_id 2 seek offset 16492
slice_id 3 seek offset 24738
slice_id 6 seek offset 49476
slice_id 1 seek offset 8246
slice_id 4 seek offset 32984
2024-10-30 07:24:27 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
2024-10-30 07:24:40 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt (epoch 14 @ 728 updates, score 0.149) (writing took 23.746265165507793 seconds)
2024-10-30 07:24:40 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 14 @ 728 updates
2024-10-30 07:24:40 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 07:24:50 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 07:24:51 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 14 @ 728 updates, score 0) (writing took 10.100078836083412 seconds)
2024-10-30 07:24:51 - train.py[line:336] - INFO: end of epoch 14 (average epoch stats below)
2024-10-30 07:24:51 - progress_bar.py[line:282] - INFO: epoch 014 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.832 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.24 | wps 357.1 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 728 | lr 4.94681e-05 | gnorm 0.008 | clip 0 | loss_scale 256 | train_wall 182 | gb_free 9.5 | wall 10276
2024-10-30 07:24:51 - trainer.py[line:639] - INFO: loading train data for epoch 15
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 07:24:53 - trainer.py[line:703] - INFO: begin training epoch 15
2024-10-30 07:24:53 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 07:25:20 - progress_bar.py[line:274] - INFO: epoch 015:      2 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.794, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=13.87, wps=210.3, ups=0.04, wpb=4926.1, bsz=1232, num_updates=730, lr=4.94579e-05, gnorm=0.009, clip=0, loss_scale=256, train_wall=34, gb_free=9.5, wall=10305
2024-10-30 07:27:20 - progress_bar.py[line:274] - INFO: epoch 015:     12 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.773, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.67, wps=425.3, ups=0.08, wpb=5118, bsz=1280, num_updates=740, lr=4.94067e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=10426
2024-10-30 07:29:20 - progress_bar.py[line:274] - INFO: epoch 015:     22 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.778, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.72, wps=426.6, ups=0.08, wpb=5117.9, bsz=1280, num_updates=750, lr=4.93556e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=10546
2024-10-30 07:31:20 - progress_bar.py[line:274] - INFO: epoch 015:     32 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.752, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.47, wps=427.2, ups=0.08, wpb=5118.3, bsz=1280, num_updates=760, lr=4.93044e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=10665
2024-10-30 07:33:18 - progress_bar.py[line:274] - INFO: epoch 015:     42 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.756, ntokens=5072.6, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=13.51, wps=427.4, ups=0.08, wpb=5072.6, bsz=1268.6, num_updates=770, lr=4.92533e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=10784
2024-10-30 07:35:13 - progress_bar.py[line:274] - INFO: epoch 015:     52 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.783, ntokens=4925.6, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=13.77, wps=430.8, ups=0.09, wpb=4925.6, bsz=1232, num_updates=780, lr=4.92021e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=34, gb_free=9.5, wall=10898
slice_id 4 seek offset 4149
slice_id 2 seek offset 2075
slice_id 6 seek offset 6223slice_id 5 seek offset 5186slice_id 7 seek offset 7260slice_id 1 seek offset 1038
slice_id 3 seek offset 3112
2024-10-30 07:35:13 - train.py[line:449] - INFO: begin validation on "valid" subset



slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 6 seek offset 6223
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
slice_id 7 seek offset 7260
2024-10-30 07:36:33 - progress_bar.py[line:282] - INFO: epoch 015 | valid on 'valid' subset | loss 0.009 | loss_v1 0 | loss_v2 0 | nll_loss 3.721 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.19 | score 0.1493 | wps 420 | wpb 638 | bsz 159.6 | num_updates 780 | best_score 0.1493
2024-10-30 07:36:33 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 15 @ 780 updates
2024-10-30 07:36:33 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 5 seek offset 41230
slice_id 7 seek offset 57721
slice_id 4 seek offset 32984
slice_id 6 seek offset 49476
slice_id 1 seek offset 8246
slice_id 2 seek offset 16492
slice_id 3 seek offset 24738
2024-10-30 07:36:43 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt
2024-10-30 07:36:56 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_best.pt (epoch 15 @ 780 updates, score 0.1493) (writing took 22.90820413827896 seconds)
2024-10-30 07:36:57 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 15 @ 780 updates
2024-10-30 07:36:57 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 07:37:07 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 07:37:07 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 15 @ 780 updates, score 0) (writing took 10.2691073641181 seconds)
2024-10-30 07:37:07 - train.py[line:336] - INFO: end of epoch 15 (average epoch stats below)
2024-10-30 07:37:07 - progress_bar.py[line:282] - INFO: epoch 015 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.768 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.63 | wps 358.3 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 780 | lr 4.92021e-05 | gnorm 0.008 | clip 0 | loss_scale 256 | train_wall 181 | gb_free 9.5 | wall 11012
2024-10-30 07:37:07 - trainer.py[line:639] - INFO: loading train data for epoch 16
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 07:37:09 - trainer.py[line:703] - INFO: begin training epoch 16
2024-10-30 07:37:09 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 07:39:11 - progress_bar.py[line:274] - INFO: epoch 016:     10 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.767, ntokens=5072.8, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=13.61, wps=212.9, ups=0.04, wpb=5072.8, bsz=1268.6, num_updates=790, lr=4.9151e-05, gnorm=0.009, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=11137
2024-10-30 07:41:12 - progress_bar.py[line:274] - INFO: epoch 016:     20 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.79, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.83, wps=423.6, ups=0.08, wpb=5118, bsz=1280, num_updates=800, lr=4.90998e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=11258
2024-10-30 07:43:12 - progress_bar.py[line:274] - INFO: epoch 016:     30 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.794, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.87, wps=425.6, ups=0.08, wpb=5118.3, bsz=1280, num_updates=810, lr=4.90487e-05, gnorm=0.006, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=11378
2024-10-30 07:45:12 - progress_bar.py[line:274] - INFO: epoch 016:     40 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.752, ntokens=5117.8, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.47, wps=426.5, ups=0.08, wpb=5117.8, bsz=1280, num_updates=820, lr=4.89975e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=11498
2024-10-30 07:47:12 - progress_bar.py[line:274] - INFO: epoch 016:     50 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.768, ntokens=5117.7, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.63, wps=425.5, ups=0.08, wpb=5117.7, bsz=1280, num_updates=830, lr=4.89464e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=11618
slice_id 3 seek offset 31122024-10-30 07:47:31 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 6 seek offset 6223slice_id 7 seek offset 7260
slice_id 1 seek offset 1038
slice_id 2 seek offset 2075slice_id 4 seek offset 4149
slice_id 5 seek offset 5186
slice_id 0 seek offset 0



slice_id 0 seek offset 0
slice_id 5 seek offset 5186
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 3 seek offset 3112
slice_id 7 seek offset 7260
slice_id 6 seek offset 6223
slice_id 4 seek offset 4149
2024-10-30 07:48:52 - progress_bar.py[line:282] - INFO: epoch 016 | valid on 'valid' subset | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.679 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 12.81 | score 0.1475 | wps 417.3 | wpb 638 | bsz 159.6 | num_updates 832 | best_score 0.1493
2024-10-30 07:48:52 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 16 @ 832 updates
2024-10-30 07:48:52 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 4 seek offset 32984
slice_id 5 seek offset 41230
slice_id 7 seek offset 57721
slice_id 3 seek offset 24738
slice_id 1 seek offset 8246
slice_id 2 seek offset 16492
slice_id 6 seek offset 49476
2024-10-30 07:49:03 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 07:49:03 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 16 @ 832 updates, score 0.1475) (writing took 11.275813061743975 seconds)
2024-10-30 07:49:03 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 16 @ 832 updates
2024-10-30 07:49:03 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 07:49:13 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 07:49:13 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 16 @ 832 updates, score 0) (writing took 9.905694272369146 seconds)
2024-10-30 07:49:13 - train.py[line:336] - INFO: end of epoch 16 (average epoch stats below)
2024-10-30 07:49:13 - progress_bar.py[line:282] - INFO: epoch 016 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.774 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.68 | wps 363.1 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 832 | lr 4.89362e-05 | gnorm 0.008 | clip 0 | loss_scale 256 | train_wall 182 | gb_free 9.5 | wall 11739
2024-10-30 07:49:13 - trainer.py[line:639] - INFO: loading train data for epoch 17
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 07:49:16 - trainer.py[line:703] - INFO: begin training epoch 17
2024-10-30 07:49:16 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 07:50:55 - progress_bar.py[line:274] - INFO: epoch 017:      8 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.723, ntokens=4926.4, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=13.2, wps=220.9, ups=0.04, wpb=4926.4, bsz=1232, num_updates=840, lr=4.88953e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=34, gb_free=9.5, wall=11841
2024-10-30 07:52:54 - progress_bar.py[line:274] - INFO: epoch 017:     18 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.742, ntokens=5072.4, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=13.38, wps=426.8, ups=0.08, wpb=5072.4, bsz=1268.6, num_updates=850, lr=4.88441e-05, gnorm=0.009, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=11960
2024-10-30 07:54:54 - progress_bar.py[line:274] - INFO: epoch 017:     28 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.755, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.5, wps=427.2, ups=0.08, wpb=5118.2, bsz=1280, num_updates=860, lr=4.8793e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=12080
2024-10-30 07:56:54 - progress_bar.py[line:274] - INFO: epoch 017:     38 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.725, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.23, wps=425.9, ups=0.08, wpb=5118.2, bsz=1280, num_updates=870, lr=4.87418e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=12200
2024-10-30 07:58:55 - progress_bar.py[line:274] - INFO: epoch 017:     48 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.692, ntokens=5117.6, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=12.92, wps=423, ups=0.08, wpb=5117.6, bsz=1280, num_updates=880, lr=4.86907e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=37, gb_free=9.5, wall=12321
slice_id 4 seek offset 41492024-10-30 07:59:39 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 2 seek offset 2075
slice_id 3 seek offset 3112

slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 7 seek offset 7260
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 2 seek offset 2075
slice_id 5 seek offset 5186
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
2024-10-30 08:01:00 - progress_bar.py[line:282] - INFO: epoch 017 | valid on 'valid' subset | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.673 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 12.76 | score 0.1387 | wps 418.4 | wpb 638 | bsz 159.6 | num_updates 884 | best_score 0.1493
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
2024-10-30 08:01:00 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 17 @ 884 updates

2024-10-30 08:01:00 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
slice_id 2 seek offset 16492
slice_id 7 seek offset 57721
slice_id 4 seek offset 32984
slice_id 5 seek offset 41230
slice_id 3 seek offset 24738
slice_id 1 seek offset 8246
slice_id 6 seek offset 49476
2024-10-30 08:01:13 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:01:13 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 17 @ 884 updates, score 0.1387) (writing took 12.844116598367691 seconds)
2024-10-30 08:01:13 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 17 @ 884 updates
2024-10-30 08:01:13 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:01:25 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:01:26 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 17 @ 884 updates, score 0) (writing took 12.606718968600035 seconds)
2024-10-30 08:01:26 - train.py[line:336] - INFO: end of epoch 17 (average epoch stats below)
2024-10-30 08:01:26 - progress_bar.py[line:282] - INFO: epoch 017 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.724 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.21 | wps 360.1 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 884 | lr 4.86702e-05 | gnorm 0.008 | clip 0 | loss_scale 256 | train_wall 191 | gb_free 9.5 | wall 12471
2024-10-30 08:01:26 - trainer.py[line:639] - INFO: loading train data for epoch 18
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 08:01:28 - trainer.py[line:703] - INFO: begin training epoch 18
2024-10-30 08:01:28 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 08:02:44 - progress_bar.py[line:274] - INFO: epoch 018:      6 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.684, ntokens=4926.2, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=12.85, wps=215.8, ups=0.04, wpb=4926.2, bsz=1232, num_updates=890, lr=4.86395e-05, gnorm=0.011, clip=0, loss_scale=256, train_wall=42, gb_free=9.5, wall=12549
2024-10-30 08:04:44 - progress_bar.py[line:274] - INFO: epoch 018:     16 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.663, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=12.67, wps=426.5, ups=0.08, wpb=5118, bsz=1280, num_updates=900, lr=4.85884e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.4, wall=12669
2024-10-30 08:06:46 - progress_bar.py[line:274] - INFO: epoch 018:     26 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.659, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=12.63, wps=419.4, ups=0.08, wpb=5118.3, bsz=1280, num_updates=910, lr=4.85372e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=35, gb_free=9.4, wall=12791
2024-10-30 08:08:46 - progress_bar.py[line:274] - INFO: epoch 018:     36 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.658, ntokens=5072.7, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=12.63, wps=421.6, ups=0.08, wpb=5072.7, bsz=1268.6, num_updates=920, lr=4.84861e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=12912
2024-10-30 08:10:47 - progress_bar.py[line:274] - INFO: epoch 018:     46 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.641, ntokens=5117.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=12.47, wps=421.4, ups=0.08, wpb=5117.5, bsz=1280, num_updates=930, lr=4.84349e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.4, wall=13033
slice_id 4 seek offset 4149
slice_id 7 seek offset 7260
slice_id 3 seek offset 3112
slice_id 6 seek offset 62232024-10-30 08:11:53 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 1 seek offset 1038
slice_id 2 seek offset 2075
slice_id 5 seek offset 5186
slice_id 0 seek offset 0

slice_id 0 seek offset 0
slice_id 5 seek offset 5186
slice_id 2 seek offset 2075
slice_id 6 seek offset 6223
slice_id 1 seek offset 1038
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
slice_id 7 seek offset 7260
2024-10-30 08:13:15 - progress_bar.py[line:282] - INFO: epoch 018 | valid on 'valid' subset | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.692 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 12.93 | score 0.1115 | wps 416.8 | wpb 638 | bsz 159.6 | num_updates 936 | best_score 0.1493
2024-10-30 08:13:15 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 18 @ 936 updates
2024-10-30 08:13:15 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 7 seek offset 57721
slice_id 5 seek offset 41230
slice_id 4 seek offset 32984
slice_id 1 seek offset 8246
slice_id 3 seek offset 24738
slice_id 2 seek offset 16492
slice_id 6 seek offset 49476
2024-10-30 08:13:26 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:13:26 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 18 @ 936 updates, score 0.1115) (writing took 11.31542382761836 seconds)
2024-10-30 08:13:26 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 18 @ 936 updates
2024-10-30 08:13:26 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:13:37 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:13:37 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 18 @ 936 updates, score 0) (writing took 11.334657549858093 seconds)
2024-10-30 08:13:37 - train.py[line:336] - INFO: end of epoch 18 (average epoch stats below)
2024-10-30 08:13:37 - progress_bar.py[line:282] - INFO: epoch 018 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.657 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 12.61 | wps 360.5 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 936 | lr 4.84043e-05 | gnorm 0.008 | clip 0 | loss_scale 256 | train_wall 182 | gb_free 9.5 | wall 13203
2024-10-30 08:13:37 - trainer.py[line:639] - INFO: loading train data for epoch 19
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 08:13:40 - trainer.py[line:703] - INFO: begin training epoch 19
2024-10-30 08:13:40 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 08:14:31 - progress_bar.py[line:274] - INFO: epoch 019:      4 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.643, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=12.5, wps=219.9, ups=0.04, wpb=4926.1, bsz=1232, num_updates=940, lr=4.83838e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=34, gb_free=9.4, wall=13257
2024-10-30 08:16:31 - progress_bar.py[line:274] - INFO: epoch 019:     14 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.698, ntokens=5072.4, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=12.98, wps=422.4, ups=0.08, wpb=5072.4, bsz=1268.6, num_updates=950, lr=4.83327e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=13377
2024-10-30 08:18:32 - progress_bar.py[line:274] - INFO: epoch 019:     24 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.687, ntokens=5118.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=12.88, wps=426, ups=0.08, wpb=5118.5, bsz=1280, num_updates=960, lr=4.82815e-05, gnorm=0.008, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=13497
2024-10-30 08:20:32 - progress_bar.py[line:274] - INFO: epoch 019:     34 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.719, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.17, wps=423.6, ups=0.08, wpb=5117.9, bsz=1280, num_updates=970, lr=4.82304e-05, gnorm=0.006, clip=0, loss_scale=256, train_wall=35, gb_free=9.6, wall=13618
2024-10-30 08:22:33 - progress_bar.py[line:274] - INFO: epoch 019:     44 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.698, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=12.98, wps=424.7, ups=0.08, wpb=5118, bsz=1280, num_updates=980, lr=4.81792e-05, gnorm=0.006, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=13739
slice_id 3 seek offset 3112
slice_id 6 seek offset 6223
slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
2024-10-30 08:24:03 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038slice_id 0 seek offset 0

slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 5 seek offset 5186
slice_id 2 seek offset 2075
slice_id 3 seek offset 3112
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
slice_id 1 seek offset 1038
2024-10-30 08:25:24 - progress_bar.py[line:282] - INFO: epoch 019 | valid on 'valid' subset | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.73 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.27 | score 0.1068 | wps 417.8 | wpb 638 | bsz 159.6 | num_updates 988 | best_score 0.1493
2024-10-30 08:25:24 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 19 @ 988 updates
2024-10-30 08:25:24 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 1 seek offset 8246
slice_id 5 seek offset 41230
slice_id 4 seek offset 32984
slice_id 7 seek offset 57721
slice_id 3 seek offset 24738
slice_id 6 seek offset 49476
slice_id 2 seek offset 16492
2024-10-30 08:25:35 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:25:35 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 19 @ 988 updates, score 0.1068) (writing took 11.247088868170977 seconds)
2024-10-30 08:25:35 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 19 @ 988 updates
2024-10-30 08:25:35 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:25:46 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:25:47 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 19 @ 988 updates, score 0) (writing took 11.183293044567108 seconds)
2024-10-30 08:25:47 - train.py[line:336] - INFO: end of epoch 19 (average epoch stats below)
2024-10-30 08:25:47 - progress_bar.py[line:282] - INFO: epoch 019 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.693 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 12.93 | wps 361.6 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 988 | lr 4.81383e-05 | gnorm 0.007 | clip 0 | loss_scale 256 | train_wall 182 | gb_free 9.5 | wall 13932
2024-10-30 08:25:47 - trainer.py[line:639] - INFO: loading train data for epoch 20
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 08:25:49 - trainer.py[line:703] - INFO: begin training epoch 20
2024-10-30 08:25:49 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 08:26:16 - progress_bar.py[line:274] - INFO: epoch 020:      2 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.688, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=12.89, wps=220.7, ups=0.04, wpb=4926.1, bsz=1232, num_updates=990, lr=4.81281e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=34, gb_free=9.5, wall=13962
2024-10-30 08:28:16 - progress_bar.py[line:274] - INFO: epoch 020:     12 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.684, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=12.85, wps=426.2, ups=0.08, wpb=5118, bsz=1280, num_updates=1000, lr=4.80769e-05, gnorm=0.007, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=14082
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
slice_id 2 seek offset 2075
slice_id 5 seek offset 5186slice_id 1 seek offset 1038

slice_id 6 seek offset 6223
2024-10-30 08:28:16 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 7 seek offset 7260
slice_id 5 seek offset 5186
slice_id 0 seek offset 0
slice_id 3 seek offset 3112
slice_id 6 seek offset 6223
slice_id 2 seek offset 2075
slice_id 4 seek offset 4149
slice_id 7 seek offset 7260
slice_id 1 seek offset 1038
2024-10-30 08:29:37 - progress_bar.py[line:282] - INFO: epoch 020 | valid on 'valid' subset | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.759 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.54 | score 0.1105 | wps 419.5 | wpb 638 | bsz 159.6 | num_updates 1000 | best_score 0.1493
2024-10-30 08:29:37 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 20 @ 1000 updates
2024-10-30 08:29:37 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_20_1000.pt
2024-10-30 08:29:47 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_20_1000.pt
2024-10-30 08:29:54 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_20_1000.pt (epoch 20 @ 1000 updates, score 0.1105) (writing took 17.046993874013424 seconds)
2024-10-30 08:31:42 - progress_bar.py[line:274] - INFO: epoch 020:     22 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.722, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.19, wps=248.1, ups=0.05, wpb=5117.9, bsz=1280, num_updates=1010, lr=4.80258e-05, gnorm=0.006, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=14288
2024-10-30 08:33:42 - progress_bar.py[line:274] - INFO: epoch 020:     32 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.729, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.26, wps=427.5, ups=0.08, wpb=5118.3, bsz=1280, num_updates=1020, lr=4.79746e-05, gnorm=0.005, clip=0, loss_scale=256, train_wall=35, gb_free=9.5, wall=14408
2024-10-30 08:35:42 - progress_bar.py[line:274] - INFO: epoch 020:     42 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.715, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.13, wps=425.4, ups=0.08, wpb=5118.2, bsz=1280, num_updates=1030, lr=4.79235e-05, gnorm=0.005, clip=0, loss_scale=512, train_wall=35, gb_free=9.5, wall=14528
2024-10-30 08:37:39 - progress_bar.py[line:274] - INFO: epoch 020:     52 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.728, ntokens=4880, nsentences=1220.6, sample_size=1220.6, sample_size_v1=0, sample_size_v2=0, ppl=13.25, wps=420.2, ups=0.09, wpb=4880, bsz=1220.6, num_updates=1040, lr=4.78723e-05, gnorm=0.007, clip=0, loss_scale=512, train_wall=49, gb_free=9.5, wall=14644
slice_id 2 seek offset 2075
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038
slice_id 6 seek offset 6223
slice_id 4 seek offset 4149
slice_id 7 seek offset 7260
slice_id 5 seek offset 5186
2024-10-30 08:37:39 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 5 seek offset 5186
slice_id 1 seek offset 1038
slice_id 2 seek offset 2075slice_id 7 seek offset 7260

slice_id 6 seek offset 6223
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
2024-10-30 08:39:00 - progress_bar.py[line:282] - INFO: epoch 020 | valid on 'valid' subset | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.808 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 14 | score 0.0962 | wps 414.5 | wpb 638 | bsz 159.6 | num_updates 1040 | best_score 0.1493
2024-10-30 08:39:00 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 20 @ 1040 updates
2024-10-30 08:39:00 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 4 seek offset 32984
slice_id 7 seek offset 57721
slice_id 6 seek offset 49476
slice_id 5 seek offset 41230
slice_id 1 seek offset 8246
slice_id 3 seek offset 24738
slice_id 2 seek offset 16492
2024-10-30 08:39:11 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:39:11 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 20 @ 1040 updates, score 0.0962) (writing took 11.084878977388144 seconds)
2024-10-30 08:39:11 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 20 @ 1040 updates
2024-10-30 08:39:11 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:39:22 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:39:22 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 20 @ 1040 updates, score 0) (writing took 10.9573660120368 seconds)
2024-10-30 08:39:22 - train.py[line:336] - INFO: end of epoch 20 (average epoch stats below)
2024-10-30 08:39:22 - progress_bar.py[line:282] - INFO: epoch 020 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.715 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.14 | wps 323.4 | ups 0.06 | wpb 5072.4 | bsz 1268.6 | num_updates 1040 | lr 4.78723e-05 | gnorm 0.006 | clip 0 | loss_scale 512 | train_wall 197 | gb_free 9.5 | wall 14748
2024-10-30 08:39:22 - trainer.py[line:639] - INFO: loading train data for epoch 21
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 08:39:25 - trainer.py[line:703] - INFO: begin training epoch 21
2024-10-30 08:39:25 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 08:41:28 - progress_bar.py[line:274] - INFO: epoch 021:     10 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.73, ntokens=5118.4, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.27, wps=222.9, ups=0.04, wpb=5118.4, bsz=1280, num_updates=1050, lr=4.78212e-05, gnorm=0.009, clip=0, loss_scale=512, train_wall=35, gb_free=9.5, wall=14874
2024-10-30 08:43:27 - progress_bar.py[line:274] - INFO: epoch 021:     20 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.788, ntokens=5072.4, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=13.81, wps=426.3, ups=0.08, wpb=5072.4, bsz=1268.6, num_updates=1060, lr=4.777e-05, gnorm=0.008, clip=0, loss_scale=512, train_wall=35, gb_free=9.6, wall=14993
2024-10-30 08:45:27 - progress_bar.py[line:274] - INFO: epoch 021:     30 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.788, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.81, wps=426.3, ups=0.08, wpb=5118.3, bsz=1280, num_updates=1070, lr=4.77189e-05, gnorm=0.008, clip=0, loss_scale=512, train_wall=35, gb_free=9.5, wall=15113
2024-10-30 08:47:27 - progress_bar.py[line:274] - INFO: epoch 021:     40 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.771, ntokens=5117.8, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.65, wps=425.8, ups=0.08, wpb=5117.8, bsz=1280, num_updates=1080, lr=4.76678e-05, gnorm=0.007, clip=0, loss_scale=512, train_wall=35, gb_free=9.6, wall=15233
2024-10-30 08:49:29 - progress_bar.py[line:274] - INFO: epoch 021:     50 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.768, ntokens=5117.7, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.62, wps=421.5, ups=0.08, wpb=5117.7, bsz=1280, num_updates=1090, lr=4.76166e-05, gnorm=0.005, clip=0, loss_scale=512, train_wall=35, gb_free=9.6, wall=15355
slice_id 6 seek offset 6223slice_id 1 seek offset 1038slice_id 7 seek offset 7260
slice_id 2 seek offset 2075
slice_id 4 seek offset 4149
slice_id 3 seek offset 3112
slice_id 5 seek offset 5186
2024-10-30 08:49:47 - train.py[line:449] - INFO: begin validation on "valid" subset


slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 2 seek offset 2075
slice_id 6 seek offset 6223
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 7 seek offset 7260
slice_id 3 seek offset 3112
slice_id 4 seek offset 4149
2024-10-30 08:51:09 - progress_bar.py[line:282] - INFO: epoch 021 | valid on 'valid' subset | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.786 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.79 | score 0.1188 | wps 416.3 | wpb 638 | bsz 159.6 | num_updates 1092 | best_score 0.1493
2024-10-30 08:51:09 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 21 @ 1092 updates
2024-10-30 08:51:09 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 7 seek offset 57721
slice_id 5 seek offset 41230
slice_id 6 seek offset 49476
slice_id 3 seek offset 24738
slice_id 4 seek offset 32984
slice_id 1 seek offset 8246
slice_id 2 seek offset 16492
2024-10-30 08:51:20 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:51:20 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 21 @ 1092 updates, score 0.1188) (writing took 11.107299350202084 seconds)
2024-10-30 08:51:20 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 21 @ 1092 updates
2024-10-30 08:51:20 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:51:31 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 08:51:31 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 21 @ 1092 updates, score 0) (writing took 11.107245575636625 seconds)
2024-10-30 08:51:31 - train.py[line:336] - INFO: end of epoch 21 (average epoch stats below)
2024-10-30 08:51:31 - progress_bar.py[line:282] - INFO: epoch 021 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.768 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.62 | wps 362.1 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 1092 | lr 4.76064e-05 | gnorm 0.007 | clip 0 | loss_scale 512 | train_wall 182 | gb_free 9.5 | wall 15477
2024-10-30 08:51:31 - trainer.py[line:639] - INFO: loading train data for epoch 22
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 08:51:33 - trainer.py[line:703] - INFO: begin training epoch 22
2024-10-30 08:51:33 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 08:53:13 - progress_bar.py[line:274] - INFO: epoch 022:      8 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.761, ntokens=4926.4, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=13.56, wps=219.8, ups=0.04, wpb=4926.4, bsz=1232, num_updates=1100, lr=4.75655e-05, gnorm=0.008, clip=0, loss_scale=512, train_wall=34, gb_free=9.5, wall=15579
2024-10-30 08:55:11 - progress_bar.py[line:274] - INFO: epoch 022:     18 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.798, ntokens=5072.4, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=13.91, wps=429.1, ups=0.08, wpb=5072.4, bsz=1268.6, num_updates=1110, lr=4.75143e-05, gnorm=0.009, clip=0, loss_scale=512, train_wall=35, gb_free=9.5, wall=15697
2024-10-30 08:57:12 - progress_bar.py[line:274] - INFO: epoch 022:     28 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.813, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.06, wps=423.9, ups=0.08, wpb=5118.2, bsz=1280, num_updates=1120, lr=4.74632e-05, gnorm=0.005, clip=0, loss_scale=512, train_wall=35, gb_free=9.5, wall=15818
2024-10-30 08:59:11 - progress_bar.py[line:274] - INFO: epoch 022:     38 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.806, ntokens=5118.2, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.99, wps=428.5, ups=0.08, wpb=5118.2, bsz=1280, num_updates=1130, lr=4.7412e-05, gnorm=0.006, clip=0, loss_scale=512, train_wall=35, gb_free=9.5, wall=15937
2024-10-30 09:01:15 - progress_bar.py[line:274] - INFO: epoch 022:     48 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.764, ntokens=5117.6, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.58, wps=412.8, ups=0.08, wpb=5117.6, bsz=1280, num_updates=1140, lr=4.73609e-05, gnorm=0.006, clip=0, loss_scale=512, train_wall=50, gb_free=9.5, wall=16061
slice_id 4 seek offset 4149slice_id 2 seek offset 2075
slice_id 7 seek offset 7260
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038slice_id 6 seek offset 6223
slice_id 5 seek offset 5186

2024-10-30 09:01:59 - train.py[line:449] - INFO: begin validation on "valid" subset

slice_id 0 seek offset 0
slice_id 0 seek offset 0
slice_id 6 seek offset 6223
slice_id 5 seek offset 5186
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
slice_id 3 seek offset 3112
slice_id 1 seek offset 1038
slice_id 2 seek offset 2075
2024-10-30 09:03:21 - progress_bar.py[line:282] - INFO: epoch 022 | valid on 'valid' subset | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.751 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.46 | score 0.1293 | wps 413.9 | wpb 638 | bsz 159.6 | num_updates 1144 | best_score 0.1493
2024-10-30 09:03:21 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 22 @ 1144 updates
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
2024-10-30 09:03:21 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966


file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966

local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
slice_id 2 seek offset 16492
slice_id 3 seek offset 24738
slice_id 5 seek offset 41230
slice_id 4 seek offset 32984
slice_id 1 seek offset 8246
slice_id 7 seek offset 57721
slice_id 6 seek offset 49476
2024-10-30 09:03:33 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 09:03:33 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 22 @ 1144 updates, score 0.1293) (writing took 11.764764852821827 seconds)
2024-10-30 09:03:33 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 22 @ 1144 updates
2024-10-30 09:03:33 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 09:03:44 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 09:03:44 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 22 @ 1144 updates, score 0) (writing took 10.989963568747044 seconds)
2024-10-30 09:03:44 - train.py[line:336] - INFO: end of epoch 22 (average epoch stats below)
2024-10-30 09:03:44 - progress_bar.py[line:282] - INFO: epoch 022 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.788 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.81 | wps 359.8 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 1144 | lr 4.73404e-05 | gnorm 0.007 | clip 0 | loss_scale 512 | train_wall 213 | gb_free 9.5 | wall 16210
2024-10-30 09:03:44 - trainer.py[line:639] - INFO: loading train data for epoch 23
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 09:03:46 - trainer.py[line:703] - INFO: begin training epoch 23
2024-10-30 09:03:46 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 09:05:01 - progress_bar.py[line:274] - INFO: epoch 023:      6 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.74, ntokens=4926.2, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=13.36, wps=218.4, ups=0.04, wpb=4926.2, bsz=1232, num_updates=1150, lr=4.73097e-05, gnorm=0.005, clip=0, loss_scale=512, train_wall=51, gb_free=9.5, wall=16287
2024-10-30 09:07:01 - progress_bar.py[line:274] - INFO: epoch 023:     16 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.79, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.83, wps=425, ups=0.08, wpb=5118, bsz=1280, num_updates=1160, lr=4.72586e-05, gnorm=0.006, clip=0, loss_scale=512, train_wall=35, gb_free=9.4, wall=16407
2024-10-30 09:09:01 - progress_bar.py[line:274] - INFO: epoch 023:     26 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.806, ntokens=5072.7, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=13.99, wps=423.2, ups=0.08, wpb=5072.7, bsz=1268.6, num_updates=1170, lr=4.72074e-05, gnorm=0.007, clip=0, loss_scale=512, train_wall=35, gb_free=9.4, wall=16527
2024-10-30 09:11:03 - progress_bar.py[line:274] - INFO: epoch 023:     36 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.819, ntokens=5118.3, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.12, wps=421.6, ups=0.08, wpb=5118.3, bsz=1280, num_updates=1180, lr=4.71563e-05, gnorm=0.007, clip=0, loss_scale=512, train_wall=35, gb_free=9.5, wall=16648
2024-10-30 09:13:03 - progress_bar.py[line:274] - INFO: epoch 023:     46 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.811, ntokens=5117.5, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.03, wps=425.8, ups=0.08, wpb=5117.5, bsz=1280, num_updates=1190, lr=4.71052e-05, gnorm=0.007, clip=0, loss_scale=512, train_wall=35, gb_free=9.4, wall=16769
slice_id 4 seek offset 4149
slice_id 7 seek offset 7260
slice_id 3 seek offset 3112slice_id 2 seek offset 2075
2024-10-30 09:14:11 - train.py[line:449] - INFO: begin validation on "valid" subset
slice_id 1 seek offset 1038
slice_id 5 seek offset 5186
slice_id 6 seek offset 6223slice_id 0 seek offset 0


slice_id 0 seek offset 0
slice_id 5 seek offset 5186
slice_id 2 seek offset 2075
slice_id 1 seek offset 1038
slice_id 3 seek offset 3112
slice_id 7 seek offset 7260
slice_id 4 seek offset 4149
slice_id 6 seek offset 6223
2024-10-30 09:15:32 - progress_bar.py[line:282] - INFO: epoch 023 | valid on 'valid' subset | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.837 | ntokens 637.981 | nsentences 159.558 | sample_size 159.558 | sample_size_v1 0 | sample_size_v2 0 | ppl 14.29 | score 0.1209 | wps 415.6 | wpb 638 | bsz 159.6 | num_updates 1196 | best_score 0.1493
2024-10-30 09:15:32 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 23 @ 1196 updates
2024-10-30 09:15:32 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappingfile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 6 row count 8245 total row count 65966

file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 1 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 5 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 2 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 4 row count 8246 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 7 row count 8245 total row count 65966
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 3 row count 8246 total row count 65966
slice_id 6 seek offset 49476
slice_id 2 seek offset 16492
slice_id 4 seek offset 32984
slice_id 1 seek offset 8246
slice_id 7 seek offset 57721
slice_id 3 seek offset 24738
slice_id 5 seek offset 41230
2024-10-30 09:15:44 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 09:15:45 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 23 @ 1196 updates, score 0.1209) (writing took 12.34077238664031 seconds)
2024-10-30 09:15:45 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 23 @ 1196 updates
2024-10-30 09:15:45 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 09:15:56 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt
2024-10-30 09:15:56 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_pretrain_aihub_indoor_checkpoints/200_5e-5_512/checkpoint_last.pt (epoch 23 @ 1196 updates, score 0) (writing took 11.265210971236229 seconds)
2024-10-30 09:15:56 - train.py[line:336] - INFO: end of epoch 23 (average epoch stats below)
2024-10-30 09:15:56 - progress_bar.py[line:282] - INFO: epoch 023 | loss 0.008 | loss_v1 0 | loss_v2 0 | nll_loss 3.797 | ntokens 5072.36 | nsentences 1268.58 | sample_size 1268.58 | sample_size_v1 0 | sample_size_v2 0 | ppl 13.9 | wps 360.3 | ups 0.07 | wpb 5072.4 | bsz 1268.6 | num_updates 1196 | lr 4.70745e-05 | gnorm 0.006 | clip 0 | loss_scale 512 | train_wall 188 | gb_free 9.5 | wall 16942
2024-10-30 09:15:56 - trainer.py[line:639] - INFO: loading train data for epoch 24
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/pretrain/train_aihub_indoor.tsv slice_id 0 row count 8246 total row count 65966
slice_id 0 seek offset 0
2024-10-30 09:15:58 - trainer.py[line:703] - INFO: begin training epoch 24
2024-10-30 09:15:58 - train.py[line:297] - INFO: Start iterating over samples
2024-10-30 09:16:49 - progress_bar.py[line:274] - INFO: epoch 024:      4 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.798, ntokens=4926.1, nsentences=1232, sample_size=1232, sample_size_v1=0, sample_size_v2=0, ppl=13.91, wps=217.6, ups=0.04, wpb=4926.1, bsz=1232, num_updates=1200, lr=4.7054e-05, gnorm=0.006, clip=0, loss_scale=512, train_wall=40, gb_free=9.4, wall=16995
2024-10-30 09:18:50 - progress_bar.py[line:274] - INFO: epoch 024:     14 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.812, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=14.05, wps=424.9, ups=0.08, wpb=5118, bsz=1280, num_updates=1210, lr=4.70029e-05, gnorm=0.005, clip=0, loss_scale=512, train_wall=35, gb_free=9.6, wall=17115
2024-10-30 09:20:50 - progress_bar.py[line:274] - INFO: epoch 024:     24 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.834, ntokens=5072.9, nsentences=1268.6, sample_size=1268.6, sample_size_v1=0, sample_size_v2=0, ppl=14.26, wps=421.7, ups=0.08, wpb=5072.9, bsz=1268.6, num_updates=1220, lr=4.69517e-05, gnorm=0.006, clip=0, loss_scale=512, train_wall=35, gb_free=9.5, wall=17236
2024-10-30 09:22:50 - progress_bar.py[line:274] - INFO: epoch 024:     34 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.794, ntokens=5117.9, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.87, wps=427.4, ups=0.08, wpb=5117.9, bsz=1280, num_updates=1230, lr=4.69006e-05, gnorm=0.008, clip=0, loss_scale=512, train_wall=35, gb_free=9.6, wall=17355
2024-10-30 09:24:51 - progress_bar.py[line:274] - INFO: epoch 024:     44 / 52 loss=0.008, loss_v1=0, loss_v2=0, nll_loss=3.762, ntokens=5118, nsentences=1280, sample_size=1280, sample_size_v1=0, sample_size_v2=0, ppl=13.56, wps=421.6, ups=0.08, wpb=5118, bsz=1280, num_updates=1240, lr=4.68494e-05, gnorm=0.008, clip=0, loss_scale=512, train_wall=36, gb_free=9.5, wall=17477
