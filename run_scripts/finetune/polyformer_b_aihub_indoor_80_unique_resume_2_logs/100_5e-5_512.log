/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2024-11-21 05:18:59 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-11-21 05:18:59 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-11-21 05:18:59 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-11-21 05:18:59 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-11-21 05:18:59 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-11-21 05:18:59 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-11-21 05:18:59 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-11-21 05:18:59 - file_utils.py[line:39] - INFO: PyTorch version 1.13.1+cu117 available.
2024-11-21 05:19:01 - utils.py[line:258] - INFO: distributed init (rank 2): env://
2024-11-21 05:19:01 - utils.py[line:261] - INFO: Start init
2024-11-21 05:19:01 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2024-11-21 05:19:01 - utils.py[line:258] - INFO: distributed init (rank 4): env://
2024-11-21 05:19:01 - utils.py[line:261] - INFO: Start init
2024-11-21 05:19:01 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 4
2024-11-21 05:19:01 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2024-11-21 05:19:01 - utils.py[line:261] - INFO: Start init
2024-11-21 05:19:01 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2024-11-21 05:19:01 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2024-11-21 05:19:01 - utils.py[line:261] - INFO: Start init
2024-11-21 05:19:01 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2024-11-21 05:19:01 - utils.py[line:258] - INFO: distributed init (rank 5): env://
2024-11-21 05:19:01 - utils.py[line:261] - INFO: Start init
2024-11-21 05:19:01 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 5
2024-11-21 05:19:01 - utils.py[line:258] - INFO: distributed init (rank 6): env://
2024-11-21 05:19:01 - utils.py[line:261] - INFO: Start init
2024-11-21 05:19:01 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 6
2024-11-21 05:19:01 - utils.py[line:258] - INFO: distributed init (rank 3): env://
2024-11-21 05:19:01 - utils.py[line:258] - INFO: distributed init (rank 7): env://
2024-11-21 05:19:01 - utils.py[line:261] - INFO: Start init
2024-11-21 05:19:01 - utils.py[line:261] - INFO: Start init
2024-11-21 05:19:01 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 7
2024-11-21 05:19:01 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:1 to store for rank: 3
2024-11-21 05:19:01 - distributed_c10d.py[line:354] - INFO: Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-11-21 05:19:01 - distributed_c10d.py[line:354] - INFO: Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-11-21 05:19:01 - utils.py[line:274] - INFO: initialized host ailab-server-bengio as rank 7
single-machine distributed training is initialized.
2024-11-21 05:19:01 - utils.py[line:274] - INFO: initialized host ailab-server-bengio as rank 3
single-machine distributed training is initialized.
2024-11-21 05:19:01 - distributed_c10d.py[line:354] - INFO: Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-11-21 05:19:01 - distributed_c10d.py[line:354] - INFO: Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-11-21 05:19:01 - utils.py[line:274] - INFO: initialized host ailab-server-bengio as rank 2
single-machine distributed training is initialized.
2024-11-21 05:19:01 - utils.py[line:274] - INFO: initialized host ailab-server-bengio as rank 4
single-machine distributed training is initialized.
2024-11-21 05:19:01 - distributed_c10d.py[line:354] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-11-21 05:19:01 - utils.py[line:274] - INFO: initialized host ailab-server-bengio as rank 0
single-machine distributed training is initialized.
2024-11-21 05:19:01 - distributed_c10d.py[line:354] - INFO: Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-11-21 05:19:01 - distributed_c10d.py[line:354] - INFO: Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-11-21 05:19:01 - utils.py[line:274] - INFO: initialized host ailab-server-bengio as rank 6
2024-11-21 05:19:01 - utils.py[line:274] - INFO: initialized host ailab-server-bengio as rank 5
single-machine distributed training is initialized.
single-machine distributed training is initialized.
2024-11-21 05:19:01 - distributed_c10d.py[line:354] - INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2024-11-21 05:19:01 - utils.py[line:274] - INFO: initialized host ailab-server-bengio as rank 1
single-machine distributed training is initialized.
2024-11-21 05:19:03 - train.py[line:77] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../polyformer_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 8, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 500, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [8], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512', 'restore_file': '../finetune/polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': 1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'score', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='polyformer_b', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='polyformer_b', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=8, batch_size_valid=8, best_checkpoint_metric='score', bf16=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, cls_weight=0.0005, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv,../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, det_weight=0.1, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_acc=True, eval_args='{"beam":5,"min_len":2,"max_len_a":0,"max_len_b":2}', eval_print_samples=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=False, freeze_encoder_embedding=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=100, max_image_size=512, max_source_positions=1024, max_src_length=80, max_target_positions=1024, max_tgt_length=420, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=8, num_bins=64, num_shards=1, num_workers=0, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', out_index=3, pad=1, patch_image_size=512, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, restore_file='../finetune/polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt', sample_patch_num=196, save_dir='./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512', save_interval=1, save_interval_updates=500, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', scst=False, scst_args='{}', seed=1, selected_cols='0,5,6,2,4,3,7', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, sync_bn=False, task='refcoco', tensorboard_logdir=None, threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[8], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../polyformer_module', uses_ema=False, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=500, vis_encoder_type='swin-base', wandb_project=None, warmup_ratio=0.06, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'refcoco', 'data': '../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv,../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv', 'selected_cols': '0,5,6,2,4,3,7', 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 80, 'max_tgt_length': 420, 'code_dict_size': 8192, 'patch_image_size': 512, 'num_bins': 64, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'eval_acc': True, 'eval_args': '{"beam":5,"min_len":2,"max_len_a":0,"max_len_b":2}', 'uses_ema': False, 'eval_print_samples': False, 'max_image_size': 512, 'scst': False, 'scst_args': '{}'}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'det_weight': 0.1, 'cls_weight': 0.0005, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.06, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2024-11-21 05:19:03 - base_task.py[line:187] - INFO: source dictionary: 4100 types
2024-11-21 05:19:03 - base_task.py[line:188] - INFO: target dictionary: 4100 types
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
local datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
2024-11-21 05:19:14 - train.py[line:101] - INFO: PolyFormerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(4100, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): SwinTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=128, out_features=128, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): Identity()
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=128, out_features=512, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=512, out_features=128, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=128, out_features=384, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=128, out_features=128, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.009)
              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=128, out_features=512, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=512, out_features=128, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=512, out_features=256, bias=False)
            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.017)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=256, out_features=768, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=256, out_features=256, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.026)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=256, out_features=1024, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1024, out_features=256, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=1024, out_features=512, bias=False)
            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.035)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.043)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (2): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.052)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (3): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.061)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (4): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.070)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (5): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.078)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (6): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.087)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (7): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.096)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (8): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.104)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (9): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.113)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (10): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.122)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (11): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.130)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (12): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.139)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (13): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.148)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (14): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.157)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (15): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.165)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (16): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.174)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (17): SwinTransformerBlock(
              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=512, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=512, out_features=512, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.183)
              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=2048, out_features=1024, bias=False)
            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.191)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=1024, out_features=1024, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.200)
              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
    (bert): XLMRobertaForMaskedLM(
      (roberta): XLMRobertaModel(
        (embeddings): XLMRobertaEmbeddings(
          (word_embeddings): Embedding(250002, 768, padding_idx=1)
          (position_embeddings): Embedding(514, 768, padding_idx=1)
          (token_type_embeddings): Embedding(1, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): XLMRobertaEncoder(
          (layer): ModuleList(
            (0): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): XLMRobertaLayer(
              (attention): XLMRobertaAttention(
                (self): XLMRobertaSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): XLMRobertaSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): XLMRobertaIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
                (intermediate_act_fn): GELUActivation()
              )
              (output): XLMRobertaOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
      (lm_head): XLMRobertaLMHead(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (decoder): Linear(in_features=768, out_features=250002, bias=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(4100, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (reg_head): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=768, out_features=768, bias=True)
        (1): Linear(in_features=768, out_features=768, bias=True)
        (2): Linear(in_features=768, out_features=2, bias=True)
      )
    )
    (cls_head): Linear(in_features=768, out_features=3, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2024-11-21 05:19:14 - train.py[line:102] - INFO: task: RefcocoTask
2024-11-21 05:19:14 - train.py[line:103] - INFO: model: PolyFormerModel
2024-11-21 05:19:14 - train.py[line:104] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2024-11-21 05:19:14 - train.py[line:108] - INFO: num. shared model params: 478,547,732 (num. trained: 478,547,732)
local datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
2024-11-21 05:19:14 - train.py[line:115] - INFO: num. expert model params: 0 (num. trained: 0)
local datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 6 row count 6250 total row count 50000file ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 5 row count 6250 total row count 50000

local datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 3 row count 6250 total row count 50000
file ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 4 row count 6250 total row count 50000file ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 0 row count 6250 total row count 50000

local datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 7 row count 6250 total row count 50000file ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 1 row count 6250 total row count 50000

local datafile ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/finetune/aihub_indoor_test_1120/aihub_indoor_test.tsv slice_id 2 row count 6250 total row count 50000
2024-11-21 05:19:35 - distributed_c10d.py[line:319] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2024-11-21 05:19:36 - distributed_c10d.py[line:354] - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2024-11-21 05:19:36 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2024-11-21 05:19:36 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.layers.0.downsample.reduction.bias <- encoder.embed_images.layers.1.downsample.reduction.bias
2024-11-21 05:19:36 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.layers.0.downsample.reduction.bias <- encoder.embed_images.layers.2.downsample.reduction.bias
2024-11-21 05:19:36 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.layers.0.downsample.reduction.bias <- decoder.cls_head.bias
2024-11-21 05:19:36 - trainer.py[line:124] - INFO: detected shared parameter: encoder.bert.roberta.embeddings.word_embeddings.weight <- encoder.bert.lm_head.decoder.weight
2024-11-21 05:19:36 - trainer.py[line:124] - INFO: detected shared parameter: encoder.bert.lm_head.bias <- encoder.bert.lm_head.decoder.bias
2024-11-21 05:19:36 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 8 workers***********************
2024-11-21 05:19:36 - utils.py[line:765] - INFO: rank   0: capabilities =  8.6  ; total memory = 23.684 GB ; name = NVIDIA GeForce RTX 3090                 
2024-11-21 05:19:36 - utils.py[line:765] - INFO: rank   1: capabilities =  8.6  ; total memory = 23.684 GB ; name = NVIDIA GeForce RTX 3090                 
2024-11-21 05:19:36 - utils.py[line:765] - INFO: rank   2: capabilities =  8.6  ; total memory = 23.684 GB ; name = NVIDIA GeForce RTX 3090                 
2024-11-21 05:19:36 - utils.py[line:765] - INFO: rank   3: capabilities =  8.6  ; total memory = 23.684 GB ; name = NVIDIA GeForce RTX 3090                 
2024-11-21 05:19:36 - utils.py[line:765] - INFO: rank   4: capabilities =  8.6  ; total memory = 23.684 GB ; name = NVIDIA GeForce RTX 3090                 
2024-11-21 05:19:36 - utils.py[line:765] - INFO: rank   5: capabilities =  8.6  ; total memory = 23.684 GB ; name = NVIDIA GeForce RTX 3090                 
2024-11-21 05:19:36 - utils.py[line:765] - INFO: rank   6: capabilities =  8.6  ; total memory = 23.684 GB ; name = NVIDIA GeForce RTX 3090                 
2024-11-21 05:19:36 - utils.py[line:765] - INFO: rank   7: capabilities =  8.6  ; total memory = 23.684 GB ; name = NVIDIA GeForce RTX 3090                 
2024-11-21 05:19:36 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 8 workers***********************
2024-11-21 05:19:36 - train.py[line:145] - INFO: training on 8 devices (GPUs/TPUs)
2024-11-21 05:19:36 - train.py[line:151] - INFO: max tokens per device = None and max sentences per device = 8
2024-11-21 05:19:36 - trainer.py[line:458] - INFO: Preparing to load checkpoint ../finetune/polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
2024-11-21 05:19:48 - trainer.py[line:619] - INFO: Loaded checkpoint ../finetune/polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt (epoch 2 @ 0 updates)
2024-11-21 05:19:48 - trainer.py[line:639] - INFO: loading train data for epoch 1
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 row count 9850 total row count 78799
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping






file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 row count 9849 total row count 78799file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 row count 9850 total row count 78799

file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 row count 9850 total row count 78799
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 row count 9850 total row count 78799file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 row count 9850 total row count 78799file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 row count 9850 total row count 78799
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 row count 9850 total row count 78799


/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
slice_id 2 seek offset 19700
Total steps 15400, warmup steps 924, warmup_factor 0.0010822510822510823
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torchvision/transforms/functional.py:443: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  "Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. "
slice_id 6 seek offset 59100
Total steps 15400, warmup steps 924, warmup_factor 0.0010822510822510823
slice_id 3 seek offset 29550
slice_id 1 seek offset 9850
slice_id 0 seek offset 0
slice_id 7 seek offset 68950
Total steps 15400, warmup steps 924, warmup_factor 0.0010822510822510823
Total steps 15400, warmup steps 924, warmup_factor 0.0010822510822510823
2024-11-21 05:20:21 - trainer.py[line:703] - INFO: begin training epoch 1
2024-11-21 05:20:21 - train.py[line:297] - INFO: Start iterating over samples
Total steps 15400, warmup steps 924, warmup_factor 0.0010822510822510823
Total steps 15400, warmup steps 924, warmup_factor 0.0010822510822510823
slice_id 4 seek offset 39400
slice_id 5 seek offset 49250
Total steps 15400, warmup steps 924, warmup_factor 0.0010822510822510823
Total steps 15400, warmup steps 924, warmup_factor 0.0010822510822510823
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
loss_reg: 0.004291919322476169 loss_cls: 0.004080073442310095
loss_reg: 0.005516024837220798 loss_cls: 0.004720999859273434
loss_reg: 0.0075289417459123925 loss_cls: 0.003523848718032241
loss_reg: 0.006352836612982819 loss_cls: 0.004525905009359121
loss_reg: 0.005398920744316938 loss_cls: 0.003776875790208578
loss_reg: 0.005075113370539469 loss_cls: 0.005286491475999355
loss_reg: 0.007834268779851007 loss_cls: 0.0035572287160903215
loss_reg: 0.0050731344545379715 loss_cls: 0.004012922756373882
loss_reg: 0.011603149705739566 loss_cls: 0.003336384892463684
loss_reg: 0.005640371017942628 loss_cls: 0.004061291925609112
loss_reg: 0.0063162677885641545 loss_cls: 0.002834687475115061
loss_reg: 0.007100345482528235 loss_cls: 0.0038471308071166277
loss_reg: 0.005823177576135896 loss_cls: 0.0039499253034591675
loss_reg: 0.004295392762975511 loss_cls: 0.004500237293541431
loss_reg: 0.005835019941326388 loss_cls: 0.0026313187554478645
loss_reg: 0.005401974491694905 loss_cls: 0.002980930497869849
loss_reg: 0.004267223046677674 loss_cls: 0.004831431433558464
loss_reg: 0.0029238978588204856 loss_cls: 0.00415422348305583
loss_reg: 0.004270780545680362 loss_cls: 0.005051920190453529
loss_reg: 0.004977943023580091 loss_cls: 0.003961444366723299
loss_reg: 0.004082188061986779 loss_cls: 0.0035759471356868744
loss_reg: 0.005200729944832893 loss_cls: 0.0038320308085530996
loss_reg: 0.004286789810742656 loss_cls: 0.0035310364328324795
loss_reg: 0.004759671385401732 loss_cls: 0.0033685353118926287
loss_reg: 0.005318214309344565 loss_cls: 0.0035991498734802008
loss_reg: 0.007238693056598871 loss_cls: 0.0041604493744671345
loss_reg: 0.007570164885645242 loss_cls: 0.003856166498735547
loss_reg: 0.005053926388716645 loss_cls: 0.004150404594838619loss_reg: 0.00816868452466293 loss_cls: 0.0037959585897624493

loss_reg: 0.00816189099719027 loss_cls: 0.002865604357793927
loss_reg: 0.006114871872921761 loss_cls: 0.004188251681625843
loss_reg: 0.005834758428431257 loss_cls: 0.0034588619600981474
loss_reg: 0.006546143074137353 loss_cls: 0.0037127863615751266
loss_reg: 0.004013229361804937 loss_cls: 0.004832519683986902
loss_reg: 0.00492997843948856 loss_cls: 0.00358332647010684
loss_reg: 0.004267977427891268 loss_cls: 0.003455640049651265
loss_reg: 0.004488967174778955 loss_cls: 0.0047791870310902596
loss_reg: 0.005091549850427107 loss_cls: 0.004894406534731388
loss_reg: 0.005870878903344798 loss_cls: 0.004468180239200592
loss_reg: 0.0053495325793457855 loss_cls: 0.0036347710993140936
loss_reg: 0.004762795784393752 loss_cls: 0.0029983497224748135
loss_reg: 0.004950300517373133 loss_cls: 0.004095084965229034
loss_reg: 0.0058313362447095255 loss_cls: 0.004831369034945965
loss_reg: 0.005617164201085993 loss_cls: 0.0033276076428592205
loss_reg: 0.004877628668575875 loss_cls: 0.003964837174862623
loss_reg: 0.0050420202271224054 loss_cls: 0.0039836023934185505
loss_reg: 0.004430820955416552 loss_cls: 0.0032959943637251854
loss_reg: 0.004514164374309548 loss_cls: 0.004228526260703802
loss_reg: 0.004465189076818028 loss_cls: 0.006203717086464167
loss_reg: 0.00818589401220559 loss_cls: 0.004162621684372425
loss_reg: 0.005765721178425162 loss_cls: 0.004393476527184248
loss_reg: 0.003799343090955757 loss_cls: 0.005548710003495216
loss_reg: 0.004891758291813859 loss_cls: 0.004988245666027069
loss_reg: 0.004214552810692535 loss_cls: 0.004263815935701132
loss_reg: 0.0038593719236450505 loss_cls: 0.004733293782919645
loss_reg: 0.004460907926731779 loss_cls: 0.004832284990698099
loss_reg: 0.003703749044598369 loss_cls: 0.004607858136296272
loss_reg: 0.004571323435870222 loss_cls: 0.0032736281864345074
loss_reg: 0.007817163376193809 loss_cls: 0.004574383143335581
loss_reg: 0.005259608442819477 loss_cls: 0.0045875804498791695
loss_reg: 0.004102251879737494 loss_cls: 0.003904476296156645
loss_reg: 0.006128469165292001 loss_cls: 0.005335252732038498
loss_reg: 0.005120885548661693 loss_cls: 0.004844381473958492
loss_reg: 0.005086301900772397 loss_cls: 0.003467881353572011
2024-11-21 05:20:56 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.786, ntokens=9102.4, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.9, wps=3101, ups=0.34, wpb=9102.4, bsz=512, num_updates=10, lr=5.41126e-07, gnorm=0.008, clip=0, loss_scale=128, train_wall=32, gb_free=5.8, wall=79
2024-11-21 05:21:25 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.772, ntokens=9013.1, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.83, wps=3110.8, ups=0.35, wpb=9013.1, bsz=512, num_updates=20, lr=1.08225e-06, gnorm=0.006, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=108
2024-11-21 05:21:53 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.687, ntokens=8749, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.44, wps=3030.5, ups=0.35, wpb=8749, bsz=512, num_updates=30, lr=1.62338e-06, gnorm=0.005, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=137
2024-11-21 05:22:22 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.83, ntokens=9240.9, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.11, wps=3208.8, ups=0.35, wpb=9240.9, bsz=512, num_updates=40, lr=2.1645e-06, gnorm=0.005, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=166
2024-11-21 05:22:51 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.778, ntokens=9068.7, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.86, wps=3127.7, ups=0.34, wpb=9068.7, bsz=512, num_updates=50, lr=2.70563e-06, gnorm=0.004, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=195
2024-11-21 05:23:20 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.736, ntokens=8870.7, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.66, wps=3050.5, ups=0.34, wpb=8870.7, bsz=512, num_updates=60, lr=3.24675e-06, gnorm=0.004, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=224
2024-11-21 05:23:49 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.714, ntokens=8910.9, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.56, wps=3079.6, ups=0.35, wpb=8910.9, bsz=512, num_updates=70, lr=3.78788e-06, gnorm=0.005, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=253
2024-11-21 05:24:18 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.7, ntokens=8807.7, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.5, wps=3029.6, ups=0.34, wpb=8807.7, bsz=512, num_updates=80, lr=4.329e-06, gnorm=0.004, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=282
2024-11-21 05:24:47 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.773, ntokens=8995.2, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.84, wps=3105.7, ups=0.35, wpb=8995.2, bsz=512, num_updates=90, lr=4.87013e-06, gnorm=0.005, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=311
2024-11-21 05:25:16 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.808, ntokens=9169.4, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7, wps=3143.9, ups=0.34, wpb=9169.4, bsz=512, num_updates=100, lr=5.41126e-06, gnorm=0.005, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=340
2024-11-21 05:25:46 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.806, ntokens=9039.7, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.99, wps=3075.9, ups=0.34, wpb=9039.7, bsz=512, num_updates=110, lr=5.95238e-06, gnorm=0.005, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=370
2024-11-21 05:26:15 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.787, ntokens=9096.6, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.9, wps=3074.2, ups=0.34, wpb=9096.6, bsz=512, num_updates=120, lr=6.49351e-06, gnorm=0.006, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=399
2024-11-21 05:26:45 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.801, ntokens=9026.2, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.97, wps=3064.8, ups=0.34, wpb=9026.2, bsz=512, num_updates=130, lr=7.03463e-06, gnorm=0.007, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=429
2024-11-21 05:27:14 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.803, ntokens=9069.2, nsentences=507.1, sample_size=507.1, sample_size_v1=0, sample_size_v2=0, ppl=6.98, wps=3146.9, ups=0.35, wpb=9069.2, bsz=507.1, num_updates=140, lr=7.57576e-06, gnorm=0.007, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=458
2024-11-21 05:27:43 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.716, ntokens=8820.1, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.57, wps=3031.4, ups=0.34, wpb=8820.1, bsz=512, num_updates=150, lr=8.11688e-06, gnorm=0.005, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=487
reach the end of datafile, start a new reader
reach the end of datafile, start a new reader
2024-11-21 05:27:54 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 154 updates
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_1.ptcp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_1.ptcp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_1.pt


cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_1.ptcp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_1.pt
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_1.pt

cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_1.pt
2024-11-21 05:27:54 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
2024-11-21 05:29:45 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 row count 9850 total row count 78799
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 row count 9850 total row count 78799
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 row count 9850 total row count 78799
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 row count 9850 total row count 78799
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 row count 9850 total row count 78799
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 row count 9849 total row count 78799
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 row count 9850 total row count 78799
slice_id 3 seek offset 29550
slice_id 4 seek offset 39400
slice_id 5 seek offset 49250
slice_id 1 seek offset 9850
slice_id 7 seek offset 68950
slice_id 2 seek offset 19700
slice_id 6 seek offset 59100
2024-11-21 05:31:40 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_best.pt (epoch 1 @ 154 updates, score 0) (writing took 226.14520340412855 seconds)
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_1.pt
2024-11-21 05:32:38 - train.py[line:336] - INFO: end of epoch 1 (average epoch stats below)
2024-11-21 05:32:38 - progress_bar.py[line:282] - INFO: epoch 001 | loss 0.001 | loss_v1 0 | loss_v2 0 | nll_loss 2.768 | ntokens 9003.17 | nsentences 511.682 | sample_size 511.682 | sample_size_v1 0 | sample_size_v2 0 | ppl 6.81 | wps 1890.4 | ups 0.21 | wpb 9003.2 | bsz 511.7 | num_updates 154 | lr 8.33333e-06 | gnorm 0.005 | clip 0 | loss_scale 128 | train_wall 449 | gb_free 5.7 | wall 782
2024-11-21 05:32:38 - trainer.py[line:639] - INFO: loading train data for epoch 2
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 row count 9850 total row count 78799
slice_id 0 seek offset 0
2024-11-21 05:33:09 - trainer.py[line:703] - INFO: begin training epoch 2
2024-11-21 05:33:09 - train.py[line:297] - INFO: Start iterating over samples
2024-11-21 05:33:28 - progress_bar.py[line:274] - INFO: epoch 002:      6 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.824, ntokens=9236.9, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.08, wps=267.2, ups=0.03, wpb=9236.9, bsz=512, num_updates=160, lr=8.65801e-06, gnorm=0.005, clip=0, loss_scale=128, train_wall=28, gb_free=5.7, wall=832
2024-11-21 05:33:57 - progress_bar.py[line:274] - INFO: epoch 002:     16 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.782, ntokens=9024.2, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.88, wps=3136.1, ups=0.35, wpb=9024.2, bsz=512, num_updates=170, lr=9.19913e-06, gnorm=0.007, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=861
2024-11-21 05:34:26 - progress_bar.py[line:274] - INFO: epoch 002:     26 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.794, ntokens=9068.4, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.94, wps=3127.5, ups=0.34, wpb=9068.4, bsz=512, num_updates=180, lr=9.74026e-06, gnorm=0.009, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=890
2024-11-21 05:34:55 - progress_bar.py[line:274] - INFO: epoch 002:     36 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.712, ntokens=8782, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.55, wps=3041.9, ups=0.35, wpb=8782, bsz=512, num_updates=190, lr=1.02814e-05, gnorm=0.008, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=919
2024-11-21 05:35:24 - progress_bar.py[line:274] - INFO: epoch 002:     46 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.787, ntokens=9037.3, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.9, wps=3132.9, ups=0.35, wpb=9037.3, bsz=512, num_updates=200, lr=1.08225e-05, gnorm=0.008, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=948
2024-11-21 05:35:53 - progress_bar.py[line:274] - INFO: epoch 002:     56 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.825, ntokens=9142, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.09, wps=3148, ups=0.34, wpb=9142, bsz=512, num_updates=210, lr=1.13636e-05, gnorm=0.007, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=977
2024-11-21 05:36:22 - progress_bar.py[line:274] - INFO: epoch 002:     66 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.787, ntokens=9062.7, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.9, wps=3118.6, ups=0.34, wpb=9062.7, bsz=512, num_updates=220, lr=1.19048e-05, gnorm=0.007, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=1006
2024-11-21 05:36:51 - progress_bar.py[line:274] - INFO: epoch 002:     76 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.778, ntokens=9015.9, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.86, wps=3081, ups=0.34, wpb=9015.9, bsz=512, num_updates=230, lr=1.24459e-05, gnorm=0.008, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=1035
2024-11-21 05:37:20 - progress_bar.py[line:274] - INFO: epoch 002:     86 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.724, ntokens=8799.9, nsentences=507.1, sample_size=507.1, sample_size_v1=0, sample_size_v2=0, ppl=6.61, wps=3022.7, ups=0.34, wpb=8799.9, bsz=507.1, num_updates=240, lr=1.2987e-05, gnorm=0.011, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=1064
2024-11-21 05:37:50 - progress_bar.py[line:274] - INFO: epoch 002:     96 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.771, ntokens=8969.1, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.83, wps=3070, ups=0.34, wpb=8969.1, bsz=512, num_updates=250, lr=1.35281e-05, gnorm=0.009, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=1093
2024-11-21 05:38:19 - progress_bar.py[line:274] - INFO: epoch 002:    106 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.838, ntokens=9213.2, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.15, wps=3138.5, ups=0.34, wpb=9213.2, bsz=512, num_updates=260, lr=1.40693e-05, gnorm=0.007, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=1123
2024-11-21 05:38:48 - progress_bar.py[line:274] - INFO: epoch 002:    116 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.776, ntokens=8955.2, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.85, wps=3049.7, ups=0.34, wpb=8955.2, bsz=512, num_updates=270, lr=1.46104e-05, gnorm=0.007, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=1152
2024-11-21 05:39:18 - progress_bar.py[line:274] - INFO: epoch 002:    126 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.781, ntokens=8943.6, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.87, wps=3065.8, ups=0.34, wpb=8943.6, bsz=512, num_updates=280, lr=1.51515e-05, gnorm=0.009, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=1181
2024-11-21 05:39:47 - progress_bar.py[line:274] - INFO: epoch 002:    136 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.781, ntokens=9017.1, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.87, wps=3099.9, ups=0.34, wpb=9017.1, bsz=512, num_updates=290, lr=1.56926e-05, gnorm=0.012, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=1210
2024-11-21 05:40:16 - progress_bar.py[line:274] - INFO: epoch 002:    146 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.749, ntokens=8943.6, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.72, wps=3077.7, ups=0.34, wpb=8943.6, bsz=512, num_updates=300, lr=1.62338e-05, gnorm=0.011, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=1239
reach the end of datafile, start a new reader
reach the end of datafile, start a new reader
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_2.ptcp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_2.pt

cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_2.pt2024-11-21 05:40:39 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 308 updates

cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_2.pt
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_2.ptcp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_2.pt

cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_2.pt
2024-11-21 05:40:39 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
2024-11-21 05:42:07 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping


file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 row count 9850 total row count 78799
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 row count 9850 total row count 78799file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 row count 9850 total row count 78799

local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping



file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 row count 9850 total row count 78799file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 row count 9850 total row count 78799
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 row count 9849 total row count 78799file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 row count 9850 total row count 78799


slice_id 3 seek offset 29550
slice_id 4 seek offset 39400
slice_id 5 seek offset 49250
slice_id 2 seek offset 19700
slice_id 1 seek offset 9850
slice_id 7 seek offset 68950
slice_id 6 seek offset 59100
2024-11-21 05:43:42 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_best.pt (epoch 2 @ 308 updates, score 0) (writing took 182.71238638390787 seconds)
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_2.pt
2024-11-21 05:44:30 - train.py[line:336] - INFO: end of epoch 2 (average epoch stats below)
2024-11-21 05:44:30 - progress_bar.py[line:282] - INFO: epoch 002 | loss 0.001 | loss_v1 0 | loss_v2 0 | nll_loss 2.783 | ntokens 9015.69 | nsentences 511.682 | sample_size 511.682 | sample_size_v1 0 | sample_size_v2 0 | ppl 6.88 | wps 1949.9 | ups 0.22 | wpb 9015.7 | bsz 511.7 | num_updates 308 | lr 1.66667e-05 | gnorm 0.009 | clip 0 | loss_scale 128 | train_wall 446 | gb_free 5.7 | wall 1494
2024-11-21 05:44:30 - trainer.py[line:639] - INFO: loading train data for epoch 3
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 row count 9850 total row count 78799
slice_id 0 seek offset 0
2024-11-21 05:45:01 - trainer.py[line:703] - INFO: begin training epoch 3
2024-11-21 05:45:01 - train.py[line:297] - INFO: Start iterating over samples
2024-11-21 05:45:09 - progress_bar.py[line:274] - INFO: epoch 003:      2 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.829, ntokens=9160.3, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.11, wps=312.1, ups=0.03, wpb=9160.3, bsz=512, num_updates=310, lr=1.67749e-05, gnorm=0.014, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=1533
2024-11-21 05:45:38 - progress_bar.py[line:274] - INFO: epoch 003:     12 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.758, ntokens=8949.9, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.76, wps=3092.6, ups=0.35, wpb=8949.9, bsz=512, num_updates=320, lr=1.7316e-05, gnorm=0.011, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=1562
2024-11-21 05:46:07 - progress_bar.py[line:274] - INFO: epoch 003:     22 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.806, ntokens=9077.1, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7, wps=3135.4, ups=0.35, wpb=9077.1, bsz=512, num_updates=330, lr=1.78571e-05, gnorm=0.008, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=1591
2024-11-21 05:46:36 - progress_bar.py[line:274] - INFO: epoch 003:     32 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.836, ntokens=9157.8, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.14, wps=3164.9, ups=0.35, wpb=9157.8, bsz=512, num_updates=340, lr=1.83983e-05, gnorm=0.008, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=1620
2024-11-21 05:47:05 - progress_bar.py[line:274] - INFO: epoch 003:     42 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.798, ntokens=8991.3, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.96, wps=3069.1, ups=0.34, wpb=8991.3, bsz=512, num_updates=350, lr=1.89394e-05, gnorm=0.007, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=1649
2024-11-21 05:47:34 - progress_bar.py[line:274] - INFO: epoch 003:     52 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.777, ntokens=8999.3, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.85, wps=3105.1, ups=0.35, wpb=8999.3, bsz=512, num_updates=360, lr=1.94805e-05, gnorm=0.008, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=1678
2024-11-21 05:48:03 - progress_bar.py[line:274] - INFO: epoch 003:     62 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.804, ntokens=9033.3, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.98, wps=3096.7, ups=0.34, wpb=9033.3, bsz=512, num_updates=370, lr=2.00216e-05, gnorm=0.009, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=1707
2024-11-21 05:48:32 - progress_bar.py[line:274] - INFO: epoch 003:     72 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.782, ntokens=8999.9, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.88, wps=3101.6, ups=0.34, wpb=8999.9, bsz=512, num_updates=380, lr=2.05628e-05, gnorm=0.009, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=1736
2024-11-21 05:49:02 - progress_bar.py[line:274] - INFO: epoch 003:     82 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.773, ntokens=8944.7, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.84, wps=3062.7, ups=0.34, wpb=8944.7, bsz=512, num_updates=390, lr=2.11039e-05, gnorm=0.009, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=1765
2024-11-21 05:49:31 - progress_bar.py[line:274] - INFO: epoch 003:     92 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.805, ntokens=9008.6, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.99, wps=3099.1, ups=0.34, wpb=9008.6, bsz=512, num_updates=400, lr=2.1645e-05, gnorm=0.009, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=1795
2024-11-21 05:50:00 - progress_bar.py[line:274] - INFO: epoch 003:    102 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.818, ntokens=9083.2, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.05, wps=3113.5, ups=0.34, wpb=9083.2, bsz=512, num_updates=410, lr=2.21861e-05, gnorm=0.013, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=1824
2024-11-21 05:50:29 - progress_bar.py[line:274] - INFO: epoch 003:    112 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.806, ntokens=8978.8, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.99, wps=3075.1, ups=0.34, wpb=8978.8, bsz=512, num_updates=420, lr=2.27273e-05, gnorm=0.011, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=1853
2024-11-21 05:50:58 - progress_bar.py[line:274] - INFO: epoch 003:    122 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.863, ntokens=9203.2, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.28, wps=3162.1, ups=0.34, wpb=9203.2, bsz=512, num_updates=430, lr=2.32684e-05, gnorm=0.011, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=1882
2024-11-21 05:51:27 - progress_bar.py[line:274] - INFO: epoch 003:    132 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.798, ntokens=9030.8, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.96, wps=3120.6, ups=0.35, wpb=9030.8, bsz=512, num_updates=440, lr=2.38095e-05, gnorm=0.009, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=1911
2024-11-21 05:51:56 - progress_bar.py[line:274] - INFO: epoch 003:    142 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.816, ntokens=8939.2, nsentences=507.1, sample_size=507.1, sample_size_v1=0, sample_size_v2=0, ppl=7.04, wps=3086.3, ups=0.35, wpb=8939.2, bsz=507.1, num_updates=450, lr=2.43506e-05, gnorm=0.012, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=1940
2024-11-21 05:52:25 - progress_bar.py[line:274] - INFO: epoch 003:    152 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.855, ntokens=9230.7, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.24, wps=3160.1, ups=0.34, wpb=9230.7, bsz=512, num_updates=460, lr=2.48918e-05, gnorm=0.012, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=1969
reach the end of datafile, start a new reader
reach the end of datafile, start a new reader
2024-11-21 05:52:31 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 3 @ 462 updates
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_3.pt
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_3.pt
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_3.pt
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_3.ptcp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_3.ptcp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_3.pt


2024-11-21 05:52:31 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_best.pt
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_3.pt
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
2024-11-21 05:53:59 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping


file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 row count 9850 total row count 78799
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 row count 9850 total row count 78799file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 row count 9850 total row count 78799

local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping



file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 row count 9850 total row count 78799
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 row count 9849 total row count 78799file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 row count 9850 total row count 78799

file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 row count 9850 total row count 78799
slice_id 4 seek offset 39400
slice_id 5 seek offset 49250
slice_id 3 seek offset 29550
slice_id 6 seek offset 59100
slice_id 1 seek offset 9850
slice_id 2 seek offset 19700
slice_id 7 seek offset 68950
2024-11-21 05:55:33 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_best.pt (epoch 3 @ 462 updates, score 0) (writing took 181.51817399193533 seconds)
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_3.pt
2024-11-21 05:56:21 - train.py[line:336] - INFO: end of epoch 3 (average epoch stats below)
2024-11-21 05:56:21 - progress_bar.py[line:282] - INFO: epoch 003 | loss 0.001 | loss_v1 0 | loss_v2 0 | nll_loss 2.81 | ntokens 9052.66 | nsentences 511.682 | sample_size 511.682 | sample_size_v1 0 | sample_size_v2 0 | ppl 7.02 | wps 1959.3 | ups 0.22 | wpb 9052.7 | bsz 511.7 | num_updates 462 | lr 2.5e-05 | gnorm 0.01 | clip 0 | loss_scale 128 | train_wall 446 | gb_free 5.7 | wall 2205
2024-11-21 05:56:21 - trainer.py[line:639] - INFO: loading train data for epoch 4
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 row count 9850 total row count 78799
slice_id 0 seek offset 0
2024-11-21 05:56:53 - trainer.py[line:703] - INFO: begin training epoch 4
2024-11-21 05:56:53 - train.py[line:297] - INFO: Start iterating over samples
2024-11-21 05:57:18 - progress_bar.py[line:274] - INFO: epoch 004:      8 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.89, ntokens=9257.8, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.41, wps=316.2, ups=0.03, wpb=9257.8, bsz=512, num_updates=470, lr=2.54329e-05, gnorm=0.012, clip=0, loss_scale=128, train_wall=29, gb_free=5.8, wall=2262
2024-11-21 05:57:47 - progress_bar.py[line:274] - INFO: epoch 004:     18 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.853, ntokens=9217.1, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.22, wps=3188, ups=0.35, wpb=9217.1, bsz=512, num_updates=480, lr=2.5974e-05, gnorm=0.01, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=2291
2024-11-21 05:58:16 - progress_bar.py[line:274] - INFO: epoch 004:     28 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.812, ntokens=9051.9, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.02, wps=3107.8, ups=0.34, wpb=9051.9, bsz=512, num_updates=490, lr=2.65152e-05, gnorm=0.01, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=2320
2024-11-21 05:58:45 - progress_bar.py[line:274] - INFO: epoch 004:     38 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.779, ntokens=8824, nsentences=507.1, sample_size=507.1, sample_size_v1=0, sample_size_v2=0, ppl=6.87, wps=3048.8, ups=0.35, wpb=8824, bsz=507.1, num_updates=500, lr=2.70563e-05, gnorm=0.009, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=2349
2024-11-21 05:59:14 - progress_bar.py[line:274] - INFO: epoch 004:     48 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.777, ntokens=8980.8, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.85, wps=3092.8, ups=0.34, wpb=8980.8, bsz=512, num_updates=510, lr=2.75974e-05, gnorm=0.014, clip=0, loss_scale=128, train_wall=29, gb_free=5.7, wall=2378
2024-11-21 05:59:43 - progress_bar.py[line:274] - INFO: epoch 004:     58 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.866, ntokens=9218.5, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.29, wps=3189, ups=0.35, wpb=9218.5, bsz=512, num_updates=520, lr=2.81385e-05, gnorm=0.011, clip=0, loss_scale=256, train_wall=29, gb_free=5.8, wall=2407
2024-11-21 06:00:12 - progress_bar.py[line:274] - INFO: epoch 004:     68 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.729, ntokens=8783, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.63, wps=3020.5, ups=0.34, wpb=8783, bsz=512, num_updates=530, lr=2.86797e-05, gnorm=0.012, clip=0, loss_scale=256, train_wall=29, gb_free=5.6, wall=2436
2024-11-21 06:00:41 - progress_bar.py[line:274] - INFO: epoch 004:     78 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.743, ntokens=8813.8, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.7, wps=3023.8, ups=0.34, wpb=8813.8, bsz=512, num_updates=540, lr=2.92208e-05, gnorm=0.014, clip=0, loss_scale=256, train_wall=29, gb_free=5.7, wall=2465
2024-11-21 06:01:10 - progress_bar.py[line:274] - INFO: epoch 004:     88 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.781, ntokens=8902.5, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.87, wps=3067.2, ups=0.34, wpb=8902.5, bsz=512, num_updates=550, lr=2.97619e-05, gnorm=0.013, clip=0, loss_scale=256, train_wall=29, gb_free=5.8, wall=2494
2024-11-21 06:01:40 - progress_bar.py[line:274] - INFO: epoch 004:     98 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.85, ntokens=9137.6, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.21, wps=3121.3, ups=0.34, wpb=9137.6, bsz=512, num_updates=560, lr=3.0303e-05, gnorm=0.014, clip=0, loss_scale=256, train_wall=29, gb_free=5.7, wall=2523
2024-11-21 06:02:09 - progress_bar.py[line:274] - INFO: epoch 004:    108 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.856, ntokens=9167.2, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.24, wps=3125.9, ups=0.34, wpb=9167.2, bsz=512, num_updates=570, lr=3.08442e-05, gnorm=0.014, clip=0, loss_scale=256, train_wall=29, gb_free=5.7, wall=2553
2024-11-21 06:02:38 - progress_bar.py[line:274] - INFO: epoch 004:    118 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.814, ntokens=8992.1, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.03, wps=3069, ups=0.34, wpb=8992.1, bsz=512, num_updates=580, lr=3.13853e-05, gnorm=0.014, clip=0, loss_scale=256, train_wall=29, gb_free=5.7, wall=2582
2024-11-21 06:03:08 - progress_bar.py[line:274] - INFO: epoch 004:    128 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.838, ntokens=9057.8, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.15, wps=3095.1, ups=0.34, wpb=9057.8, bsz=512, num_updates=590, lr=3.19264e-05, gnorm=0.012, clip=0, loss_scale=256, train_wall=29, gb_free=5.7, wall=2611
2024-11-21 06:03:37 - progress_bar.py[line:274] - INFO: epoch 004:    138 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.817, ntokens=9002.4, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.05, wps=3085.4, ups=0.34, wpb=9002.4, bsz=512, num_updates=600, lr=3.24675e-05, gnorm=0.017, clip=0, loss_scale=256, train_wall=29, gb_free=5.7, wall=2641
2024-11-21 06:04:06 - progress_bar.py[line:274] - INFO: epoch 004:    148 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.852, ntokens=9187.9, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.22, wps=3144.3, ups=0.34, wpb=9187.9, bsz=512, num_updates=610, lr=3.30087e-05, gnorm=0.014, clip=0, loss_scale=256, train_wall=29, gb_free=5.7, wall=2670
reach the end of datafile, start a new reader
reach the end of datafile, start a new reader
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_4.pt2024-11-21 06:04:23 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 4 @ 616 updates

cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_4.pt
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_4.pt
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_4.pt
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_4.pt
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_4.ptcp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_4.pt

2024-11-21 06:04:23 - trainer.py[line:431] - INFO: Saving checkpoint to ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_best.pt
cp: cp: cannot create regular file './polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_4.pt'cannot create regular file './polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_4.pt': File exists: File exists

local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 3 row count 9850 total row count 78799
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 4 row count 9850 total row count 78799
slice_id 3 seek offset 29550
slice_id 4 seek offset 39400
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 begin to initialize row_count and line_idx-to-offset mapping
2024-11-21 06:05:55 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_best.pt
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 5 row count 9850 total row count 78799
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 finished initializing row_count and line_idx-to-offset mappinglocal datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 finished initializing row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 finished initializing row_count and line_idx-to-offset mapping

file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 7 row count 9849 total row count 78799
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 6 row count 9850 total row count 78799
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 2 row count 9850 total row count 78799
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 1 row count 9850 total row count 78799
slice_id 5 seek offset 49250
slice_id 7 seek offset 68950
slice_id 6 seek offset 59100
slice_id 1 seek offset 9850
slice_id 2 seek offset 19700
2024-11-21 06:07:30 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_best.pt (epoch 4 @ 616 updates, score 0) (writing took 186.7113582501188 seconds)
cp ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_last.pt ./polyformer_b_aihub_indoor_80_unique_resume_2_checkpoints/100_5e-5_512/checkpoint_epoch_4.pt
2024-11-21 06:08:18 - train.py[line:336] - INFO: end of epoch 4 (average epoch stats below)
2024-11-21 06:08:18 - progress_bar.py[line:282] - INFO: epoch 004 | loss 0.001 | loss_v1 0 | loss_v2 0 | nll_loss 2.82 | ntokens 9044.64 | nsentences 511.682 | sample_size 511.682 | sample_size_v1 0 | sample_size_v2 0 | ppl 7.06 | wps 1944.4 | ups 0.21 | wpb 9044.6 | bsz 511.7 | num_updates 616 | lr 3.33333e-05 | gnorm 0.013 | clip 0 | loss_scale 256 | train_wall 446 | gb_free 5.7 | wall 2921
2024-11-21 06:08:18 - trainer.py[line:639] - INFO: loading train data for epoch 5
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 begin to initialize row_count and line_idx-to-offset mapping
local datafile ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 finished initializing row_count and line_idx-to-offset mapping
file ../../datasets/finetune/aihub_indoor_train_1121/aihub_indoor_train.tsv slice_id 0 row count 9850 total row count 78799
slice_id 0 seek offset 0
2024-11-21 06:08:49 - trainer.py[line:703] - INFO: begin training epoch 5
2024-11-21 06:08:49 - train.py[line:297] - INFO: Start iterating over samples
2024-11-21 06:09:03 - progress_bar.py[line:274] - INFO: epoch 005:      4 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.925, ntokens=9344.3, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.59, wps=314.6, ups=0.03, wpb=9344.3, bsz=512, num_updates=620, lr=3.35498e-05, gnorm=0.012, clip=0, loss_scale=256, train_wall=29, gb_free=5.7, wall=2967
2024-11-21 06:09:32 - progress_bar.py[line:274] - INFO: epoch 005:     14 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.878, ntokens=9242.8, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.35, wps=3211.1, ups=0.35, wpb=9242.8, bsz=512, num_updates=630, lr=3.40909e-05, gnorm=0.012, clip=0, loss_scale=256, train_wall=29, gb_free=5.7, wall=2996
2024-11-21 06:10:01 - progress_bar.py[line:274] - INFO: epoch 005:     24 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.827, ntokens=9059.3, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.09, wps=3135.4, ups=0.35, wpb=9059.3, bsz=512, num_updates=640, lr=3.4632e-05, gnorm=0.013, clip=0, loss_scale=256, train_wall=29, gb_free=5.8, wall=3024
2024-11-21 06:10:30 - progress_bar.py[line:274] - INFO: epoch 005:     34 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.773, ntokens=8856.9, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.84, wps=3039.3, ups=0.34, wpb=8856.9, bsz=512, num_updates=650, lr=3.51732e-05, gnorm=0.014, clip=0, loss_scale=256, train_wall=29, gb_free=5.7, wall=3054
2024-11-21 06:10:59 - progress_bar.py[line:274] - INFO: epoch 005:     44 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.869, ntokens=9084.6, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.31, wps=3119.2, ups=0.34, wpb=9084.6, bsz=512, num_updates=660, lr=3.57143e-05, gnorm=0.014, clip=0, loss_scale=256, train_wall=29, gb_free=5.8, wall=3083
2024-11-21 06:11:28 - progress_bar.py[line:274] - INFO: epoch 005:     54 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.833, ntokens=9053, nsentences=507.1, sample_size=507.1, sample_size_v1=0, sample_size_v2=0, ppl=7.13, wps=3131.4, ups=0.35, wpb=9053, bsz=507.1, num_updates=670, lr=3.62554e-05, gnorm=0.014, clip=0, loss_scale=256, train_wall=29, gb_free=5.6, wall=3112
2024-11-21 06:11:57 - progress_bar.py[line:274] - INFO: epoch 005:     64 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.86, ntokens=9086.3, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.26, wps=3136.4, ups=0.35, wpb=9086.3, bsz=512, num_updates=680, lr=3.67965e-05, gnorm=0.017, clip=0, loss_scale=256, train_wall=29, gb_free=5.8, wall=3141
2024-11-21 06:12:26 - progress_bar.py[line:274] - INFO: epoch 005:     74 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.801, ntokens=8987.6, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.97, wps=3079.6, ups=0.34, wpb=8987.6, bsz=512, num_updates=690, lr=3.73377e-05, gnorm=0.014, clip=0, loss_scale=256, train_wall=29, gb_free=5.7, wall=3170
2024-11-21 06:12:55 - progress_bar.py[line:274] - INFO: epoch 005:     84 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.795, ntokens=8966, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=6.94, wps=3037.9, ups=0.34, wpb=8966, bsz=512, num_updates=700, lr=3.78788e-05, gnorm=0.01, clip=0, loss_scale=256, train_wall=29, gb_free=5.8, wall=3199
2024-11-21 06:13:25 - progress_bar.py[line:274] - INFO: epoch 005:     94 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.864, ntokens=9169.3, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.28, wps=3148.1, ups=0.34, wpb=9169.3, bsz=512, num_updates=710, lr=3.84199e-05, gnorm=0.01, clip=0, loss_scale=256, train_wall=29, gb_free=5.8, wall=3228
2024-11-21 06:13:54 - progress_bar.py[line:274] - INFO: epoch 005:    104 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.89, ntokens=9161.5, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.41, wps=3157.4, ups=0.34, wpb=9161.5, bsz=512, num_updates=720, lr=3.8961e-05, gnorm=0.013, clip=0, loss_scale=256, train_wall=29, gb_free=5.8, wall=3257
2024-11-21 06:14:23 - progress_bar.py[line:274] - INFO: epoch 005:    114 / 154 loss=0.001, loss_v1=0, loss_v2=0, nll_loss=2.886, ntokens=9212.6, nsentences=512, sample_size=512, sample_size_v1=0, sample_size_v2=0, ppl=7.39, wps=3157.6, ups=0.34, wpb=9212.6, bsz=512, num_updates=730, lr=3.95022e-05, gnorm=0.02, clip=0, loss_scale=256, train_wall=29, gb_free=5.7, wall=3287
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "../../train.py", line 543, in <module>
  File "../../train.py", line 543, in <module>
Traceback (most recent call last):
WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers
  File "../../train.py", line 543, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180548 closing signal SIGINT
  File "../../train.py", line 543, in <module>
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180549 closing signal SIGINT
    cli_main()
  File "../../train.py", line 536, in cli_main
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180550 closing signal SIGINT
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180551 closing signal SIGINT
      File "../../train.py", line 536, in cli_main
cli_main()WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180552 closing signal SIGINT


WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180553 closing signal SIGINT

WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180554 closing signal SIGINT
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180555 closing signal SIGINT
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/src/fairseq/fairseq/fairseq/distributed/utils.py", line 374, in call_main
  File "../../train.py", line 536, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/src/fairseq/fairseq/fairseq/distributed/utils.py", line 374, in call_main
    
distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/src/fairseq/fairseq/fairseq/distributed/utils.py", line 348, in distributed_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/src/fairseq/fairseq/fairseq/distributed/utils.py", line 348, in distributed_main
    distributed_utils.call_main(cfg, main)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/src/fairseq/fairseq/fairseq/distributed/utils.py", line 374, in call_main
    main(cfg, **kwargs)    
main(cfg, **kwargs)  File "../../train.py", line 190, in main

      File "../../train.py", line 190, in main
distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/src/fairseq/fairseq/fairseq/distributed/utils.py", line 348, in distributed_main
        valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
    
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/contextlib.py", line 74, in inner
main(cfg, **kwargs)  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/contextlib.py", line 74, in inner

  File "../../train.py", line 190, in main
        return func(*args, **kwds)return func(*args, **kwds)    

valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)  File "../../train.py", line 302, in train
  File "../../train.py", line 302, in train

  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "../../train.py", line 302, in train
    log_output = trainer.train_step(samples)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/trainer.py", line 780, in train_step
        log_output = trainer.train_step(samples)log_output = trainer.train_step(samples)

  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/contextlib.py", line 74, in inner
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/contextlib.py", line 74, in inner
        return func(*args, **kwds)return func(*args, **kwds)

  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/trainer.py", line 780, in train_step
      File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/trainer.py", line 780, in train_step
**extra_kwargs,
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/tasks/base_task.py", line 275, in train_step
    loss, sample_size, logging_output = criterion(model, sample, update_num=update_num)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    **extra_kwargs,
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/tasks/base_task.py", line 275, in train_step
    **extra_kwargs,
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/tasks/base_task.py", line 275, in train_step
    loss, sample_size, logging_output = criterion(model, sample, update_num=update_num)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
        loss, sample_size, logging_output = criterion(model, sample, update_num=update_num)return forward_call(*input, **kwargs)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl

  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/criterions/label_smoothed_cross_entropy.py", line 220, in forward
    return forward_call(*input, **kwargs)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/criterions/label_smoothed_cross_entropy.py", line 220, in forward
    net_output = model(**sample["net_input"])
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    net_output = model(**sample["net_input"])
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/criterions/label_smoothed_cross_entropy.py", line 220, in forward
    return forward_call(*input, **kwargs)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/src/fairseq/fairseq/fairseq/distributed/module_proxy_wrapper.py", line 55, in forward
    return forward_call(*input, **kwargs)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/src/fairseq/fairseq/fairseq/distributed/module_proxy_wrapper.py", line 55, in forward
    net_output = model(**sample["net_input"])    
    return self.module(*args, **kwargs)  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
return self.module(*args, **kwargs)

  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    return forward_call(*input, **kwargs)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/src/fairseq/fairseq/fairseq/distributed/module_proxy_wrapper.py", line 55, in forward
    return self.module(*args, **kwargs)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
        output = self._run_ddp_forward(*inputs, **kwargs)output = self._run_ddp_forward(*inputs, **kwargs)

  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return forward_call(*input, **kwargs)    
    return module_to_run(*inputs[0], **kwargs[0])  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
return module_to_run(*inputs[0], **kwargs[0])

  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl

        return forward_call(*input, **kwargs)return forward_call(*input, **kwargs)

  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/polyformer.py", line 108, in forward
      File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/polyformer.py", line 125, in forward
output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    sample_patch_num=sample_patch_num
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return_all_hiddens=return_all_hiddens,
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
      File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_transformer.py", line 692, in forward
    return module_to_run(*inputs[0], **kwargs[0])return forward_call(*input, **kwargs)

  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_transformer.py", line 1177, in forward
    sample_patch_num)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_transformer.py", line 748, in forward_scriptable
    alignment_heads=alignment_heads,
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_transformer.py", line 1216, in extract_features
    pos_embed, image_pos_embed
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_transformer.py", line 625, in forward_embedding
    return forward_call(*input, **kwargs)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/polyformer.py", line 125, in forward
    x += self.type_embedding(src_tokens.new_zeros(x.size()[:2]))
    return_all_hiddens=return_all_hiddens,
KeyboardInterrupt  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl

    alignment_heads,
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_transformer.py", line 1377, in extract_features_scriptable
        cross_attn_bias=cross_abs_pos_biasreturn forward_call(*input, **kwargs)

  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_transformer.py", line 1177, in forward
    return forward_call(*input, **kwargs)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_transformer_layer.py", line 483, in forward
    alignment_heads=alignment_heads,
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_transformer.py", line 1216, in extract_features
    x = self.residual_connection(x, residual)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_transformer_layer.py", line 366, in residual_connection
    return residual + self.drop_path(x)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    alignment_heads,
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_transformer.py", line 1377, in extract_features_scriptable
    return forward_call(*input, **kwargs)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_transformer_layer.py", line 51, in forward
    return drop_path(x, self.drop_prob, self.training)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_transformer_layer.py", line 37, in drop_path
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
KeyboardInterrupt
    cross_attn_bias=cross_abs_pos_bias
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_transformer_layer.py", line 478, in forward
    attn_bias=cross_attn_bias
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/models/polyformer/unify_multihead_attention.py", line 223, in forward
    q = self.q_proj(query)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Traceback (most recent call last):
  File "../../train.py", line 543, in <module>
    cli_main()
  File "../../train.py", line 536, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/src/fairseq/fairseq/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/src/fairseq/fairseq/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "../../train.py", line 302, in train
    log_output = trainer.train_step(samples)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/trainer.py", line 780, in train_step
terminate called without an active exception
    **extra_kwargs,
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/tasks/base_task.py", line 279, in train_step
    optimizer.backward(loss)
  File "/SSDe/sangbeom_lee/AIHub-polygon-transformer/src/fairseq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180548 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180549 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180550 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180551 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180552 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180553 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180554 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1180555 closing signal SIGTERM
Traceback (most recent call last):
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1180282 got signal: 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/elastic/agent/server/api.py", line 716, in run
    self._shutdown(e.sigval)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 289, in _shutdown
    self._pcontext.close(death_sig)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 332, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 709, in _close
    handler.proc.wait(time_to_wait)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/subprocess.py", line 1019, in wait
    return self._wait(timeout=timeout)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/subprocess.py", line 1647, in _wait
    time.sleep(delay)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1180282 got signal: 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/run.py", line 756, in run
    )(*cmd_args)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 237, in launch_agent
    result = agent.run()
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/elastic/metrics/api.py", line 129, in wrapper
    result = f(*args, **kwargs)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/elastic/agent/server/api.py", line 721, in run
    self._shutdown()
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 289, in _shutdown
    self._pcontext.close(death_sig)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 332, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 709, in _close
    handler.proc.wait(time_to_wait)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/subprocess.py", line 1019, in wait
    return self._wait(timeout=timeout)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/subprocess.py", line 1647, in _wait
    time.sleep(delay)
  File "/home/sangbeom_lee/anaconda3/envs/polyformer/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1180282 got signal: 2
